---
title: "HDAT9600 Survival Analysis Modelling Assignment"
subtitle: "Submission deadline - 22/04/2024"
author: "Zhenyu Zhang"
date: "22/04/2024"
output: html_document
---

```{r setup, include=FALSE}
# leave this code here, but feel free to adjust the options or add some more
# see the knitr documentation for details
knitr::opts_chunk$set(echo = TRUE, fig.width=5, fig.height=5)
```


## Instructions

This file is the R Markdown document in which you need to complete your HDAT9600 Survival Modelling assignment. This assignment is assessed and will count for 25% of the total course marks.

You should now complete the tasks described below, in the spaces provided. Don't hesitate to ask the course instructor for help (use OpenLearning environment to do that) --- but remember that these are **individual** assignments and thus what you submit should be your own work, and you need to both understand and be able to explain what you did in your solutions. The course convenor is happy to point you in the right direction and to make suggestions, but they won't, of course, complete your assignments for you!

Each task below attracts the indicated number of marks (out of a total of 25 marks).


## Introduction

For the Survival Modelling assignment, we are going to analyse colon cancer survival data, available as a built-in dataset in the _survival_ package. These data include information from a clinical trial on the effectiveness of two different types of chemotherapy (levamisole and levamisole+5-fluorouracil) compared to controls (i.e. no chemotherapy treatment) on survival from stage B/C colon cancer. There are two rows per person in the dataset, one for cancer recurrence and one for death, indicated by the event type (`etype`) variable (`etype==1` corresponds to recurrence and `etype==2` to death). In the tasks below, you will focus on analysing death as an outcome.

As the _survival_ package is one of the core R packages, you don’t need to separately install it. To get started, you just need to load it into the workspace in R:

```{r data-setup, warning=FALSE}
library(survival)

## required packages for examples 
libs <- c("eha", "stringi", "bshazard", "survminer")
missing <- !libs %in% installed.packages()
if (any(missing)) {
  install.packages(libs[missing],repos="https://cloud.r-project.org")
}

library(eha)
library(stringi)
library(bshazard)
library(survminer)
library(ggplot2)
library(pch)
library(survival)
library(flexsurv)
library(muhaz)
```
We are then going to select a subset of the data. We will subset the dataset to only include deaths (not recurrence). We will also focus on just five of the potential predictors: `age` , `sex` , `rx` ,  `extent` , `node4` as below:

```{r subset}
#Choose just deaths and only the specified predictors
colondeath <- colon[colon$etype==2,
                   c("id","time","status","rx","age","sex","extent","node4")]

```

These selected variables are described in the package vignette as: 

* `id`: id
* `time`: days until event or censoring
* `status`: censoring status
* `rx`: Treatment: Obs(ervation), Lev(amisole), Lev(amisole)+5-FU
* `sex`: (1=male, 0=female)
* `age`: in years
* `extent`: Extent of local spread (1=submucosa, 2=muscle, 3=serosa, 4=contiguous structures)
* `node4`: more than 4 positive lymph nodes (1=yes, 0=no)

<br>
```{r subset2}
library(knitr)  # Load the knitr package

# Prepare descriptive labels
variable_descriptions <- c(
  Status = "Censoring status (0: Censored, 1: Event occurred)",
  Treatment = "Treatment: Obs (Observation), Lev (Levamisole), Lev+5FU (Levamisole + 5-Fluorouracil)",
  Sex = "Sex (1: Male, 0: Female)",
  Age = "Age in years",
  Extent = "Extent of local spread (1: Submucosa, 2: Muscle, 3: Serosa, 4: Contiguous structures)",
  Node4 = "More than 4 positive lymph nodes (1: Yes, 0: No)",
  Time = "Days until event or censoring"
)

category_maps <- list(
  Status = c("0" = "Censored", "1" = "Event occurred"),
  Treatment = c("Obs" = "Observation", "Lev" = "Levamisole", "Lev+5FU" = "Levamisole + 5-Fluorouracil"),
  Sex = c("0" = "Female", "1" = "Male"),
  Extent = c("1" = "Submucosa", "2" = "Muscle", "3" = "Serosa", "4" = "Contiguous structures"),
  Node4 = c("0" = "No", "1" = "Yes")
)

```
Familiarise yourself with this dataset and complete the following tasks by writing R code and commentary text where appropriate. All quantitative results must be generated by R code which you have included in this file. Results calculated by other means or by code not included in this file will not receive any marks.


## Task 1 (3 marks)
(EDA: 3 marks)

Conduct a brief EDA making sure you address the following points:

* Describe the dataset in terms of number of people, and number/type of variables
* Briefly describe the frequency distributions for the variables - include frequencies and proportions for the categorical variables, and median and interquartile range (i.e. first quartile & third quartile) for any continuous variables.
* What was the median follow-up time until death? 

```{r task-1.0}
# Describe the dataset
length(unique(colondeath$id))  # Number of unique patients
str(colondeath)                # Structure of the dataset

# Histogram of follow-up times until death with density line
ggplot(colondeath, aes(x = time)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 100, fill = "skyblue", color = "black", alpha = 0.7) +
  geom_density(color = "blue") +
  labs(title = "Distribution of Follow-up Time Until Death",
       x = "Follow-up Time",
       y = "Density") +
  theme_minimal()

# Histogram of ages at death with density line
ggplot(colondeath, aes(x = age)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 5, fill = "pink", color = "black", alpha = 0.7) +
  geom_density(color = "red") +
  labs(title = "Distribution of Age at Death",
       x = "Age",
       y = "Density") +
  theme_minimal()
```
```{r task-1.1}
# Custom function to print frequency and proportion tables with descriptions
print_categorical <- function(data, varname) {
  descriptions <- variable_descriptions[varname]
  category_map <- category_maps[[varname]]  # Get the category descriptions for the variable

  # Create the frequency and proportion tables
  freq <- table(data)
  prop <- prop.table(freq) * 100  # Convert proportions to percentages

  # Map the data levels to more descriptive names
  rownames_map <- if (!is.null(category_map)) {
    category_map[as.character(names(freq))]
  } else {
    names(freq)
  }

  # Create a data frame
  combined_table <- data.frame(
    Data = rownames_map,
    Frequency = as.integer(freq),
    Proportion = sprintf("%.2f%%", prop)
  )

  # Set row names for the table
  rownames(combined_table) <- rownames_map

  # Print the table with kable
  print(kable(combined_table, caption = descriptions, row.names = FALSE))
}



# Use the function on each categorical variable
print_categorical(colondeath$status, "Status")
print_categorical(colondeath$rx, "Treatment")
print_categorical(colondeath$sex, "Sex")
print_categorical(colondeath$extent, "Extent")
print_categorical(colondeath$node4, "Node4")
```

```{r task-1.2}
#median(colondeath$time, na.rm = TRUE)  # Median follow-up time

# Custom function to print summaries for continuous variables
print_summary <- function(data, varname) {
  description <- variable_descriptions[varname]
  summary_stats <- summary(data)
  median_value <- median(data, na.rm = TRUE)
  iqr_value <- IQR(data, na.rm = TRUE)
  summary_df <- data.frame(
    Statistic = c("Minimum", "First Quartile (Q1)", "Median", "Mean", "Third Quartile (Q3)", "Maximum", "Interquartile Range"),
    Value = c(summary_stats[1], summary_stats[2], median_value, summary_stats[4], summary_stats[5], summary_stats[6], iqr_value)
  )
  cat(description, "\n")
  kable(summary_df, caption = description)  # Ensure it's using a proper data frame
}

# Apply the summary function to continuous variables
print_summary(colondeath$age, "Age")
print_summary(colondeath$time, "Time")

```
_Dataset Description_

The dataset contains data from 929 unique patients and 8 variables. This figure represents the number of distinct individuals who have been observed in terms of death (since `etype==2` is selected).

1. **Censoring Status (`status`)**
   - **Censored (0)**: Represents patients who were censored, which means they left the study or the study ended before their death was observed. About 51.35% of observations are censored.
   - **Event Occurred (1)**: Represents patients who died during the study. This group comprises 48.65% of the data, indicating a nearly balanced occurrence between censored and event cases.

2. **Treatment (`rx`)**
   - **Observation (Obs)**: 33.91% of patients were in the observation group, receiving no active treatment.
   - **Levamisole (Lev)**: 33.37% received Levamisole only.
   - **Levamisole + 5-Fluorouracil (Lev+5FU)**: 32.72% were treated with a combination of Levamisole and 5-Fluorouracil. 
   - The treatment groups are very evenly distributed, which is ideal for comparative analysis.

3. **Sex (`sex`)**
   - **Female (0)**: Represents 47.90% of the patients.
   - **Male (1)**: Comprises 52.10% of the patients. This slight imbalance should be considered in analyses to check for any sex-specific influences on survival.

4. **Extent of Local Spread (`extent`)**
   - The data shows a significant majority (81.70%) with serosa involvement (level 3), indicating advanced local spread.
   - Few patients have submucosa (level 1) or contiguous structures (level 4) involvement, which are less common stages.

5. **Lymph Nodes (`node4`)**
   - **No (0)**: 72.55% of patients had four or fewer positive lymph nodes, indicating less extensive disease.
   - **Yes (1)**: 27.45% had more than four positive lymph nodes, a marker for more aggressive or advanced disease.

6. **Age in years**:
   - **Range**: Ages range from 18 to 85 years.
   - **Median Age**: The median age is 61 years, indicating that half of the patients are younger than 61 and the other half older.
   - **IQR**: The interquartile range is 16 years, suggesting that the middle 50% of the data is spread across 16 years from the 25th to the 75th percentile.
  
7. **Days until event or censoring**:
   - **Range**: Follow-up time ranges from 23 days to 3329 days.
   - **Median Follow-up Time**: The median follow-up time is 1976 days, which is a good indicator of the follow-up period before an event or censoring occurs.
   - **IQR**: The interquartile range is 1558 days, showing a substantial spread in the middle 50% of the data, indicating a large variability in different patients followed up time.

## Task 2 (2 marks)
(EDA/Presentation: 2 marks)


#### Task 2.a

Plot the overall Kaplan-Meier survival curve for the `colondeath` data.

```{r task-2-a, fig.keep='all', fig.show='asis'}

# Creating the survival object
surv_obj <- Surv(time = colondeath$time, event = colondeath$status)

# Fitting the Kaplan-Meier survival model
surv_fit <- survfit(surv_obj ~ 1)  # `~ 1` for an overall curve
surv_summary <- summary(surv_fit)

# Create a concise summary
concise_summary <- data.frame(
  time = surv_summary$time, 
  n.risk = surv_summary$n.risk,
  n.event = surv_summary$n.event,
  n.censor = surv_summary$n.censor,
  surv = surv_summary$surv,
  upper = surv_summary$upper,
  lower = surv_summary$lower
)

# Display only the first few rows of the concise summary
# Calculate indices for the 25th, 50th (median), and 75th percentiles
quantile_indices <- quantile(1:nrow(concise_summary), probs = c(0.25, 0.5, 0.75), na.rm = TRUE)

# Subset rows for first, 25%, median, 75%, and last
selected_rows <- concise_summary[c(1, 
                                   quantile_indices['25%'], 
                                   quantile_indices['50%'], 
                                   quantile_indices['75%'], 
                                   nrow(concise_summary)), ]

# Display the selected rows
selected_rows

#determining median survival time
median_surv_time <- surv_summary$time[surv_summary$surv <= 0.5][1]
median_surv_time

# Plotting the survival curve using ggsurvplot from survminer
ggsurvplot(surv_fit, data = colondeath,
           conf.int = TRUE,  # Include confidence intervals
           risk.table = TRUE,  # Show number of subjects at risk over time
           xlab = "Days", ylab = "Survival Probability",
           ggtheme = theme_minimal(),  # A cleaner theme for the plot
           title = "Overall Kaplan-Meier Survival Curve")
```


#### Task 2.b

Plot the Nelson-Aalen cumulative hazard function for the `colondeath` data.

```{r task-2-b}
# Plotting the Nelson-Aalen cumulative hazard curve
ggsurvplot(surv_fit, data = colondeath, fun = "cumhaz",
           xlab = "Days", ylab = "Cumulative Hazard",
           ggtheme = theme_minimal(),  # A cleaner theme for the plot
           title = "Nelson-Aalen Cumulative Hazard Function")
```



## Task 3 (4 marks) 
(EDA/Modelling/Presentation: 3 marks; Interpretation: 1 mark)

#### Task 3.a

Plot the Kaplan-Meier survival curves, by group, for each of the categorical variables in the data. 
```{r task-3-a}
# Create a copy of the original colondeath dataset
colondeath_analysis <- colondeath

# Convert the 'sex' 'extent' and 'node4' columns to factors with the correct labels in the new dataset
colondeath_analysis$sex <- factor(colondeath_analysis$sex, levels = c(0, 1), labels = c("Female", "Male"))
colondeath_analysis$extent <- factor(colondeath_analysis$extent, levels = 1:4, labels = c("Submucosa", "Muscle", "Serosa", "Contiguous structures"))
colondeath_analysis$node4 <- factor(colondeath_analysis$node4, levels = c(0, 1), labels = c("No", "Yes"))

# Fit Kaplan-Meier models for each categorical variable
km_fit_by_rx <- survfit(Surv(time, status) ~ rx, data = colondeath_analysis)
km_fit_by_sex <- survfit(Surv(time, status) ~ sex, data = colondeath_analysis)
km_fit_by_extent <- survfit(Surv(time, status) ~ extent, data = colondeath_analysis)
km_fit_by_node4 <- survfit(Surv(time, status) ~ node4, data = colondeath_analysis)

# Define a common plotting function
plot_km_curve <- function(km_fit, variable_name) {
  plot <- ggsurvplot(km_fit, data = colondeath_analysis,
             palette = "Dark2",  # Use distinct colors for each group
             pval = TRUE,  # Include p-value of log-rank test between groups
             conf.int = TRUE,  # Include confidence intervals
             xlab = "Days", ylab = "Survival Probability",
             ggtheme = theme_minimal(),  # Use a minimal theme for the plot
             legend.title = variable_name,  # Label for the legend
             title = paste("Kaplan-Meier Curves by", variable_name))
  # Add a horizontal line at y = 0.5 to indicate median survival
  plot$plot <- plot$plot + geom_hline(yintercept = 0.5, linetype = "dashed", color = "red")
  print(plot)
}

# Plot Kaplan-Meier curves for each categorical variable
plot_km_curve(km_fit_by_rx, "Treatment")
plot_km_curve(km_fit_by_sex, "Sex")
plot_km_curve(km_fit_by_extent, "Extent of Spread")
plot_km_curve(km_fit_by_node4, "More than 4 Positive Lymph Nodes")
```

#### Task 3.b

Report the number of deaths and the median survival times by categories for all categorical variables in the data (for example in the format of a table).

```{r task-3-b}
print (km_fit_by_rx)
print (km_fit_by_sex)
print (km_fit_by_extent)
print (km_fit_by_node4)

# Manually create the summary table
final_summary_table <- data.frame(
  Category = c(rep("Treatment", 3), rep("Sex", 2), rep("Extent of Spread", 4), rep("More than 4 Positive Lymph Nodes", 2)),
  Variable = c("Obs", "Lev", "Lev+5FU", "Female", "Male", "Submucosa", "Muscle", "Serosa", "Contiguous structures", "No", "Yes"),
  Number_of_Deaths = c(168, 161, 123, 215, 237, 4, 36, 383, 29, 271, 181),
  Median_Survival = as.character(c(2083, 2152, NA, NA, 2542, NA, NA, 2318, 1276, NA, 944)) # NA values as string to keep consistent format
)

# Print the table
kable(final_summary_table)
```

#### Task 3.c

Carry out the log-rank tests for differences in survival experience by the categorical variables and interpret the results.

```{r task-3-c}
# Perform log-rank tests
log_rank_test_rx <- survdiff(Surv(time, status) ~ rx, data = colondeath_analysis)
log_rank_test_sex <- survdiff(Surv(time, status) ~ sex, data = colondeath_analysis)
log_rank_test_extent <- survdiff(Surv(time, status) ~ extent, data = colondeath_analysis)
log_rank_test_node4 <- survdiff(Surv(time, status) ~ node4, data = colondeath_analysis)

# Print the results of log-rank tests
print(log_rank_test_rx)
print(log_rank_test_sex)
print(log_rank_test_extent)
print(log_rank_test_node4)


```

_commentary for task 3.c_
The p-values obtained from the `survdiff` function and from the Kaplan-Meier plots (`ggsurvplot`) with `pval = TRUE` are both valid results of log-rank tests, but there might be slight differences due to the way the p-values are displayed or the precision with which they are reported.The output of the `survdiff()` function gives the p-value and provides the observed and expected number of events, as well as test statistics that can be used to interpret the results of the log-rank tests. 

1. **Observed (O):** This is the actual number of events (deaths) observed in each group.

2. **Expected (E):** This is the number of events we would expect to observe in each group if there were no true difference in survival times between the groups.

3. **(O-E)^2/E:** This is the component of the chi-squared statistic for each group. It measures how much the observed count deviates from the expected count. The larger the value, the greater the discrepancy.

4. **(O-E)^2/V:** This is the chi-squared statistic adjusted for the variance, which takes into account the variability of the event times. It is used in calculating the overall chi-squared statistic for the test.

**Treatment (rx):**<br>
- The discrepancy between observed and expected deaths, particularly in the 'Lev+5FU' group, is notable, indicating that the treatment had an effect on survival.<br>
- The log-rank test for treatment groups shows a Chi-squared statistic of 11.7 with 2 degrees of freedom, resulting in a p-value of 0.003. This indicates that there are statistically significant differences in survival between the three treatment groups (Observation, Levamisole, and Levamisole + 5-Fluorouracil).<br>

**Sex:**<br>
- The observed and expected numbers of deaths (215 observed vs. 217 expected for females, and 237 observed vs. 235 expected for males) are very close, indicating that there's no strong evidence of a difference in survival between males and females.<br>
- For the sex groups, the log-rank test produces a p-value of 0.9 with 1 degree of freedom, which suggests that there is no statistically significant difference in survival between male and female patients within this study.<br>

**Extent of Spread (extent):**<br>
- The largest discrepancies between observed and expected deaths are in the 'Muscle' and 'Contiguous structures' groups, with fewer observed than expected in 'Muscle' and more in 'Contiguous structures'.<br>
- The 'Serosa' group shown smaller discrepancies between the observed and expected numbers.<br>
- The Chi-squared statistic for the extent of spread is 26.9 with 3 degrees of freedom and a p-value of 6e-06, indicating that the extent of the spread significantly affects survival.<br>

**More than 4 Positive Lymph Nodes (node4):**<br>
- For patients with no more than 4 positive lymph nodes ('No'), there were significantly less deaths than expected (271 observed vs. 358.7 expected), while for those with more than 4 positive lymph nodes ('Yes'), there were far more deaths observed than expected (181 observed vs. 93.3 expected).<br>
- The log-rank test for the presence of more than four positive lymph nodes gives a very high Chi-squared value of 105 with 1 degree of freedom, leading to an extremely small p-value (<2e-16). This suggests an extremely statistically significant difference in survival between patients with no more than four positive lymph nodes and those with more than four.<br>


## Task 4 (8 marks)
(Modelling: 4 marks; Interpretation: 4 marks)

#### Task 4.a

Fit univariable Cox models separately for all five predictor variables in the data. Interpret the results. 

```{r task-4-a}
# Fit univariable Cox models for each predictor variable
cox_model_rx <- coxph(Surv(time, status) ~ rx, data = colondeath_analysis)
cox_model_sex <- coxph(Surv(time, status) ~ sex, data = colondeath_analysis)
cox_model_age <- coxph(Surv(time, status) ~ age, data = colondeath_analysis)
cox_model_extent <- coxph(Surv(time, status) ~ extent, data = colondeath_analysis)
cox_model_node4 <- coxph(Surv(time, status) ~ node4, data = colondeath_analysis)

# Print the summary of each model
print(summary(cox_model_rx))
print(summary(cox_model_sex))
print(summary(cox_model_age))
print(summary(cox_model_extent))
print(summary(cox_model_node4))
```

_commentary for task 4.a_

**Treatment (`rx`):**<br>
- Coefficients: Significant negative coefficient for `Lev+5FU` (-0.37171) suggests a reduced hazard compared to the baseline (`Obs`), meaning this treatment potentially lowers the risk of the event (death).<br>
- Hazard Ratios (HR): `Lev+5FU` has a HR of 0.6896, significantly below 1, indicating it's protective against the event.<br>
- P-value: The p-value for `Lev+5FU` is 0.00175, which is statistically significant, suggesting that the treatment type affects survival.<br>

**Sex:**<br>
- Coefficient: The coefficient for males is 0.01332, indicating a slightly higher hazard compared to females, but this is not significant.<br>
- HR: The HR for males is 1.01341, almost neutral, showing no substantial difference in hazard between males and females.<br>
- P-value: High p-value (0.888), indicating no significant difference in survival between sexes.<br>

**Age:**<br>
- Coefficient: Small positive coefficient (0.001954) suggests a very slight increase in hazard with each additional year of age.<br>
- HR: HR is close to 1 (1.001956), showing that age has a minimal impact on the hazard.<br>
- P-value: High p-value (0.628) indicates that age is not a significant predictor of survival in this model.<br>

**Extent of Local Spread (`extent`):**<br>
- Coefficients: Increasing coefficients for more severe extent categories (`Muscle`, `Serosa`, `Contiguous structures`) suggest increasing hazards with more advanced extent of local spread.<br>
- HRs: All HRs are greater than 1, with `Contiguous structures` showing a HR of 5.6313, indicating a significantly higher risk compared to the baseline (`Submucosa`).<br>
- P-value:** `Serosa` and `Contiguous structures` show significant p-values (0.0149 and 0.0012 respectively), indicating that the extent of spread is a significant predictor of survival, particularly at more advanced stages.<br>

**More than 4 Positive Lymph Nodes (`node4`):**<br>
- Coefficient: High positive coefficient (0.9539) for having more than 4 positive nodes.<br>
- HR: HR of 2.5960 for `Yes` indicates approximately 2.6 times risk of death compared to `No`, which is a substantial increase.<br>
- P-value: Extremely small p-value (<2e-16), confirming that this variable is a very strong and significant predictor of survival.<br>

**Summary:**<br>
- [Treatment] and [Extent of Local Spread] show variations in risk associated with different levels, with `Lev+5FU` and `Contiguous structures` spread stages being particularly noteworthy.<br>
- [Sex] and [Age] do not appear to significantly impact survival.<br>
- [More than 4 Positive Lymph Nodes] is a strong predictor, with a significant increase in hazard when more than four lymph nodes are involved.<br>

#### Task 4.b

Fit a full multivariable Cox model with all five predictor variables. Interpret the results. Are all the variables that were significant in the univariable models still significant in the multivariable model?

```{r task-4-b}
# Fit a multivariable Cox model with all predictor variables
full_cox_model <- coxph(Surv(time, status) ~ rx + sex + age + extent + node4, data = colondeath_analysis)

# Print the summary of the model
summary(full_cox_model)
```

_commentary for task 4.b_<br>
**Model Summary:**<br>
- Number of Participants: 929<br>
- Number of Events (deaths): 452<br>
- Concordance: (a measure used in survival analysis to evaluate the predictive accuracy of a Cox model. This statistic, close to 1, indicates a good fit of the model to the data.)<br>
    _ In this case, the concordance index of 0.659 means that the model correctly predicts the survival order 65.9% of the time. <br>
    _ Standard Error (se): The value, 0.013 in multivariable Cox model, quantifies the variability or uncertainty in the estimation of the concordance index. A smaller standard error indicates a more precise estimate.<br>

- three alternative tests for overall significance of the model. These methods are asymptotically equivalent, meaning that with large N they will provide similar results. For small N, the LRT is generally preferred.<br>
  - Likelihood Ratio Test: compares the fit of full model against a null model that has no covariates. It examines whether adding predictors (covariates) to the model improves the fit significantly.<br>
    _ In this case, 125.4 indicates the test statistic calculated from the ratio of the likelihoods (likelihood of the full model divided by the likelihood of the null model).<br>
    _ 8 df: implies the test involves 8 parameters – the number of covariates or predictor variables in the model.<br>
    _ p < 2e-16: (extremely small) suggests that the model fits significantly better than a model without predictors, validating the inclusion of these variables.<br>
  - Wald Test: evaluates the significance of individual coefficients in the model.<br>
    _ 129.1: reflects the cumulative Wald statistics for all coefficients, indicating how far the estimated coefficients collectively deviate from zero under the null hypothesis.<br>
    _ p < 2e-16: Similar to the LRT, it confirms that at least one of the predictors is significantly associated with the outcome, justifying their use in the model.<br>
  - Score (Logrank) Test: evaluates the overall significance of the model like the Wald and LRT but is based on the score vector (the derivative of the log-likelihood function).<br>
    _ 138.9: assessing the contribution of each covariate under the null hypothesis that it equals zero.<br>
    _ p < 2e-16: A very small p-value indicates that the model with predictors provides a significantly better fit than the null model.<br>

**key statistics**<br>
- `Coef` (Coefficient): The estimated coefficient for each covariate in the Cox model. It represents the effect of a one-unit increase in the predictor variable on the hazard, expressed in log terms. <br>
  - A positive coefficient indicates that as the predictor increases, the hazard of the event also increases.<br>
  - A negative coefficient indicates that as the predictor increases, the hazard of the event decreases. <br>

- `Exp(Coef)` (Hazard Ratio): This is the exponential of the coefficient and is known as the hazard ratio (HR). It quantifies the effect of a one-unit increase in the predictor on the hazard, or risk, of the event occurring.<br>
  - The hazard ratio can be obtained by exponentiating the coefficient (`exp(coef)`). <br>
  - [> 1] indicates an increased hazard of the event with an increase in the predictor. If a coefficient is 0.5, the hazard ratio is `exp(0.5) ≈ 1.65`, means the hazard increases by 65% for each one-unit increase in the predictor.<br>
  - [< 1] indicates a decreased hazard of the event with an increase in the predictor. if a coefficient is -0.5, the hazard ratio is `exp(-0.5) ≈ 0.61`, indicates a 39% decrease in the hazard for each one-unit increase in the predictor.<br>
  - [= 1] implies no effect of the predictor on the hazard.<br>
  
- `SE(Coef)` (Standard Error of the Coefficient): an indication of the precision of the estimated effect of the predictor. Lower standard errors correspond to more precise estimates.

- `z`: the test statistic for testing the null hypothesis that the coefficient (log hazard ratio) is zero (no effect). It is calculated as `z = coef / se(coef)`.

- `Pr(>|z|)`: the p-value associated with the z-test. It tests the hypothesis that the predictor has no effect on the hazard rate. A small p-value (< 0.05) indicates the rejection of the null hypothesis and consider the predictor is statistically significant on the outcome.

- `exp(-coef)`: provides the inverse of the hazard ratio. For `rxLev+5FU`, `exp(coef)` = exp(-0.383574) = 0.681421 means that taking the treatment `Lev+5FU` is associated with 68.14% (a 31.86% decrease) in the hazard relative to the baseline (`Obs`).`exp(-coef)` = exp(0.383574) = 1.4675 indicates that the hazard of the reference group (`Obs`) is about 46.75% higher than that for the `Lev+5FU` group. 

- `lower .95` & `upper .95`**: These represent the lower and upper bounds of the 95% confidence interval for the hazard ratio. If this interval does not include 1, the effect is considered statistically significant at the 5% level.

**statistically significant covariates**<br>
- `rxLev+5FU` `extent Contiguous structures` and `node4 Yes` are predictors that demonstrated statistical significance in the Cox proportional hazards model.<br>
  - Each of these covariates has a p-value less than 0.05<br>
  - The 95% confidence intervals for the hazard ratios of these variables do not include 1.
  

#### Task 4.c

Fit a reduced multivariable Cox model retaining only the variables that were significant in the full model (4b).  What are the AICs for the full and reduced  models (show how you calculated them) and how are they interpreted?

```{r task-4-c}
# Fit the reduced Cox model
reduced_cox_model <- coxph(Surv(time, status) ~ rx + extent + node4, data = colondeath_analysis)

# Print the summary of the reduced model
summary(reduced_cox_model)

# Calculate AIC for the full model if not already done
aic_full <- AIC(full_cox_model)
print(paste("AIC for the Full Model:", aic_full))

# Calculate AIC for the reduced model with corrected names
aic_reduced <- AIC(reduced_cox_model)
print(paste("AIC for the Reduced Model:", aic_reduced))

```

_commentary for task 4.c_<br>
1. Full Model: Includes all predictors (rx, sex, age, extent, node4).       AIC: 5751.03<br>
2. Reduced Model: Excludes sex and age, retaining rx, extent, and node4.    AIC: 5749.93<br>

The Akaike Information Criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.Lower AIC values generally suggest a model that better balances complexity (number of parameters) with fit (how well model describes data).<br>
  _ AIC = -2(log-likelihood) + 2k, where k is the number of estimated parameters in the model.<br>
  _ The reduced model has a lower AIC value compared to the full model. This is indicative of a better model fit relative to the number of parameters used.<br>

Between two competing models, the one with the lower AIC is normally preferred. In this case, the reduced model not only simplifies the analysis (by focusing on fewer variables) but also potentially enhances the interpretability and generalizability of the model without compromising the model's accuracy.<br>


#### Task 4.d

Compare the full and reduced models using the likelihood ratio test. Interpret the results. 

```{r task-4-d}
print(anova(full_cox_model, reduced_cox_model))

# 'full_cox_model' is already fitted with all predictors
print(drop1(full_cox_model, test = "Chisq"))

```

_commentary for task 4.d_<br>
**LRT test**: anova.coxph() function in the survival package be used to carry out the LRT test between Full Model and Reduced Model.<br>
  _ The likelihood ratio test comparing these two models gives a chi-squared value of 2.9064 with 2 degrees of freedom and a p-value of 0.2338 (not significant). This result suggests that removing sex and age from the full model does not significantly worsen the fit, thereby the simpler model (reduced model) might be adequate.<br>

**Drop1 Function**: available in the eha package, also based on the likelihood ratio test<br>
  _ Akaike Information Criterion (AIC): Lower AIC values indicate a model with a better balance between goodness of fit and complexity.<br> 
  _ LRT: Likelihood ratio statistic for dropping the term. The larger the LRT value, the better the model fits compared to its simpler counterpart.<br>
  _ Pr(>Chi): P-value for each test. Significant values suggest that dropping that variable significantly worsens the model fit, indicating the variable is important.<br>

The overall analysis suggests that the reduced model might be sufficiently capturing the necessary information without the need for additional predictors included in the full model. The predictors of `sex` and `age` can potentially be excluded without significantly compromising the model's predictive power. This approach simplifies the model without a significant loss in predictive accuracy or fit.<br>


## Task 5 (3 marks)
(Modelling: 2 marks; Interpretation 1 mark)

#### Task 5.a

Check whether the proportional hazards assumptions hold for the variables included in the reduced model. Interpret the results.

```{r task-5}
# drawn plots of log(-log(S(t))) for an easier check
# For rx
ggsurvplot(km_fit_by_rx, fun = "cloglog", xlab = "Time", ylab = "log(-log(S(t)))", title = "log(-log(S(t))) by Treatment")

# For sex
ggsurvplot(km_fit_by_sex, fun = "cloglog", xlab = "Time", ylab = "log(-log(S(t)))", title = "log(-log(S(t))) by Sex")

# For extent
ggsurvplot(km_fit_by_extent, fun = "cloglog", xlab = "Time", ylab = "log(-log(S(t)))", title = "log(-log(S(t))) by Extent of Spread")

# For node4
ggsurvplot(km_fit_by_node4, fun = "cloglog", xlab = "Time", ylab = "log(-log(S(t)))", title = "log(-log(S(t))) by More than 4 Positive Lymph Nodes")



```


```{r task-5-a}
# Check the proportional hazards assumption
ph_test <- cox.zph(reduced_cox_model)
print(ph_test)

# Plotting the Schoenfeld residuals to visually inspect any trends over time
plot(ph_test)

# the scaled Schoenfeld residuals along with a smooth curve
ggcoxzph(ph_test)
```

_commentary for task 5.a_<br>
**proportional hazards assumption**<br>
The key assumption in the Cox proportional hazards model is that of proportional hazards (PH).This assumption means that the hazards for the groups being compared should be proportional and thus should not cross. 
Based on Kaplan-Meier curves already get in Task 3.a and the plots of log(-log(S(t))), the curves shown crossing and the PH assumption may be violated. <br>

the proportional hazard assumption is supported by a non-significant relationship between residuals and time, while a significant relationship favours the alternative of non-constant hazards. 
The function cox.zph() in the survival package provides a convenient solution for testing the PH assumption for each covariate included in the Cox model.A low p-value (typically less than 0.05) suggests that the proportional hazards assumption does not hold for that variable. <br>

The output from the proportional hazards test shows that:<br>
- `rx`: With a p-value of 0.369, suggesting that there is not enough evidence to conclude that the proportional hazards assumption has been violated. Would not reject the null hypothesis of proportional hazards for this variable.<br>
- `extent`: The p-value of 0.065 is marginally above the typical alpha level of 0.05, suggesting that a possible violation of the proportional hazards assumption, although it is not definitive and could be subject to further scrutiny.<br>
- `node4`: The p-value is 0.011, which is less than 0.05. This indicates that the assumption of proportional hazards is likely violated.<br>
- `GLOBAL`: The global p-value is 0.022, which indicates that at least one of the covariates is violating the proportional hazards assumption when considering the model as a whole.<br>

**Schoenfeld residual plots**<br>
The Schoenfeld residual plots generated for each covariate and the global test are graphical diagnostics used to assess the proportional hazards assumption in Cox model. Each plot displays the Schoenfeld residuals against time. Under the proportional hazards assumption, if the proportional hazards assumption holds would expect to see the residuals scattered randomly around the zero line without any discernible trend / systematic pattern. When residuals cluster or show a pattern as time increases, this may indicate a violation of the proportional hazards assumption. However, a visual assessment can be subjective, and it's important to also consider the results of statistical tests.<br>
the solid line is a smoothing spline fit to the plot, with the dashed lines representing a +/- 2-standard-error band around the fit.<br>
- `rx`: doesn't appear to indicate a strong trend or systematic pattern over time.<br>
- `extent`: seems there is a bit of a downward trend as time increases, which might suggest a potential violation of the proportional hazards assumption for the `extent` variable.<br>
- `node4`: there appears to be a distinct trend in the residuals over time, suggesting that the proportional hazards assumption does not hold for the `node4` variable.<br>

**Global schoenfeld test**<br>
In the graphs of the scaled Schoenfeld residuals, the solid line is a smoothing spline fit to the plot, with the dashed lines representing ± 2 standard errors.Systematic departures from a horizontal line are indicative of non-proportional hazards, since proportional hazards assumes that estimates do not vary much over time. The graphical inspection does not show pattern with time.<br>


#### Task 5.b

You would like to obtain hazard ratios for all of the variables in the reduced model, and thus you choose to address the violations of the proportional hazards assumption by including time x covariate interaction(s) in the model for the variable(s) that violated the proportional hazards assumption.

Take a look at the survival curves stratified by the variable(s) that violated the proportional hazards assumption [Note: you do not need to plot these again - have a look at the survival curves in task 3.a]. Choose a follow-up time point around which to split the follow-up time to fit two Cox models to attempt to resolve the violation, and split the dataset into two time intervals around your chosen time point.

```{r task-5-b}
# Assuming 'colondeath_analysis' is your dataset and you've decided to split at X days
# Split the dataset at X days
colondeath_split <- survSplit(Surv(time, status) ~ ., data = colondeath_analysis, cut = (500), episode = "time_group", id = "id2")
head(colondeath_split)

```

## Task 6 (3 marks)
(Modelling: 2 marks; Interpretation 1 mark)


Fit the multivariable Cox model with time x covariate interaction(s) for the variable(s) that violated the proportional hazards assumption using the dataset you created in task 5.b in this assignment. Re-check the proportional hazards assumptions. Interpret the results [Hint: if you have chosen the follow-up cut-off point wisely, you should see significant associations in at least one time interval].

```{r task-6-a}
# Fit the Cox model with time-dependent covariates
# Adjust the model formula if necessary to include the interaction terms
cox_model_time_dep <- coxph(Surv(time, status) ~ rx + extent + node4 * strata(time_group), data = colondeath_split)
summary(cox_model_time_dep)

# Check the proportional hazards assumption for the new model
coxph_assumption_check <- cox.zph(cox_model_time_dep, terms=FALSE)

# Output the summary of the new model and the test for proportional hazards
summary(cox_model_time_dep)
print(coxph_assumption_check)
```

_commentary for task 6_<br>
The model output suggests that the interaction between node4 and time_group is statistically significant (node4Yes:time_group p=0.01388), This suggests that there's a significant difference in the hazard ratio for `node4Yes` before and after the chosen time point of 500 days.<br>

- Before 500 days, the `node4Yes` variable has a coefficient that translates to an increased hazard ratio [ exp(coef) significantly greater than 1 ], indicating that individuals with more than 4 positive lymph nodes have a higher risk of the event occurring.<br>
- After 500 days, the `node4Yes:time_group` interaction term modifies this hazard ratio. Since this interaction term is significant, it suggests that the effect of `node4Yes` on the hazard changes after 1000 days. The coefficient for `node4Yes:time_group` is negative, which indicates that the hazard ratio for `node4Yes` decreases after 500 days compared to before.<br>

The `GLOBAL` test has a p-value of 0.162, which is not significant, suggesting that the overall model does not violate the proportional hazards assumption when the interaction with time is included.<br>

## Task 7 (2 marks)
(Modelling/Interpretation: 2 marks)

Is there a significant survival difference between treatment with Levamisole compared to Levamisole + 5-fluorouracil (5-FU)? Describe which model would provide the best answer to this question and run any new analyses if you need to. Make sure you justify your conclusions. 

```{r task-7-a}
library(eha)  # for the pchreg function

# Create survival object
colondeath_analysis$surv_obj <- Surv(colondeath_analysis$time, colondeath_analysis$status == 1)
#colondeath_analysis$surv_obj

# Fit Weibull model
weibull_model <- phreg(surv_obj ~ rx + age + sex + extent + node4, data = colondeath_analysis, dist = "weibull")
print(weibull_model)

# Fit Gompertz model
gompertz_model <- phreg(surv_obj ~ rx + age + sex + extent + node4, data = colondeath_analysis, dist = "gompertz")
print(gompertz_model)

# Fit Piecewise Constant Hazards model
# Define time intervals for PCH model
#time_breaks <- quantile(colondeath_analysis$time, probs = seq(0, 1, by = 0.25), na.rm = TRUE)
time_cuts <- c(1, 500, 1000, 1500, 2000, 2500, 3000, max(colondeath_analysis$time))
# Fit the Piecewise Constant Hazards model using pchreg
pch_model <- eha::pchreg(surv_obj ~ rx + age + sex + extent + node4,
                         data = colondeath_analysis,
                         cuts = time_cuts)
print(pch_model)

```
_commentary for task 7a_<br>
The maximized log likelihoods for each model:<br>
  1. **Weibull Model**: Maximized log likelihood = -4064.9<br>
  2. **Gompertz Model**: Maximized log likelihood = -4065.8<br>
  3. **Piecewise Constant Hazards Model**: Maximized log likelihood = -4044.2<br>
The higher log likelihood value (closer to zero, since log likelihoods are typically negative) indicates a model that better explains the variability in the data. The **Piecewise Constant Hazards Model** shows the highest (least negative) log likelihood value of -4044.2, indicating a better fit to the data compared to the other two models.<br>

```{r task-7-b}
# Fit a Cox model for baseline hazard comparison
cox_model <- coxreg(surv_obj ~ rx + age + sex + extent + node4, data = colondeath_analysis)

check.dist(cox_model, weibull_model)
check.dist(cox_model, gompertz_model)
check.dist(cox_model, pch_model)

# outputting coefficients from different models
cat("Cox Model Coefficients:\n", paste(names(cox_model$coefficients), cox_model$coefficients, sep = ": ", collapse = "\n "), "\n\n")
cat("Weibull Model Coefficients:\n", paste(names(weibull_model$coefficients), weibull_model$coefficients, sep = ": ", collapse = "\n "), "\n\n")
cat("Gompertz Model Coefficients:\n", paste(names(gompertz_model$coefficients), gompertz_model$coefficients, sep = ": ", collapse = "\n "), "\n\n")
cat("PCH Model Coefficients:\n", paste(names(pch_model$coefficients), pch_model$coefficients, sep = ": ", collapse = "\n "), "\n\n")

```
_commentary for task 7b_<br>
using a function `check.dist()` to compare the distributions<br>
- **Weibull Model**: The cumulative hazard curve is smooth and doesn't show intervals but rather a continuous function. Its closeness to the Cox model seems reasonable as it appears to diverge at durations after 500.<br>
- **Gompertz Model**: Similar to the Weibull (a bit better), the Gompertz model also provides a smooth cumulative hazard function. It follows the Cox model's cumulative hazard, much like the Weibull model.<br>
-**Piecewise Constant Plot**: The PCH model's follows the non-parametric curve closely, it cumulative hazard appears to fit the non-parametric estimate very well, capturing the steps and potential changes in the hazard rate at different time points.From the visual comparison, while the Weibull and Gompertz models provide a smooth estimation, the PCH model shows distinct steps which would represent the narrowest time intervals since it allows for changes at specific points in time.<br>
Therefore, **if the criteria are the narrowness of time intervals and closeness to the Cox model's estimate**, the PCH model is likely the best choice.<br>

```{r task-7-c}
# Visualizing the survival curves might also help, Omitting the confidence intervals from these plots can enhance clarity, making it easier to focus on the differences between groups.
library(RColorBrewer)
colors_dark2 <- brewer.pal(n = 3, name = "Dark2")  
plot(survfit(surv_obj ~ rx, data = colondeath), col = colors_dark2, main = "Survival Curves", xlab = "Days", ylab = "Survival Probability")
legend("bottomleft", legend = c("Observation", "Lev", "Lev+5FU"), col = colors_dark2, lty = 1)

# Akaike Information Criterion (AIC)
weibull_aic <- AIC(weibull_model)
gompertz_aic <- AIC(gompertz_model)

# Display the AIC values
cat("AIC for Weibull Model:", weibull_aic, "\n")
cat("AIC for Gompertz Model:", gompertz_aic, "\n")

# logLik() does not work for 'pchreg', 'tpchreg'
#pch_log_likelihood <- logLik(pch_model)  # Replaced as logLik doesn't work with the correct value from the model summary
#pch_n_params <- length(pch_model$coefficients)  # Number of estimated parameters
#pch_aic <- -2 * pch_log_likelihood + 2 * pch_n_params

```


## Save, knit and submit

**Reminder**: don't forget to save this file, to knit it to check that everything works, and then submit via the drop box in Openlearning.

## Submit your assignment

When you have finished, and are satisfied with your assignment solutions, and this file knits without errors and the output looks the way you want, then you should submit via the drop box in Openlearning.

### Problems?

If you encounter problems with any part of the process described above, please contact the course instructor via OpenLearning as soon as possible so that the issues can be resolved in good time, and well before the assignment is due.
