{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2MYApyud30w"
   },
   "source": [
    "### Rationale part 1: Dataset Preparation for Multi-Class Classification\n",
    "- **Reason**: I did this to perform classification, via build a dataset that includes labeled images from three classes: **normal**, **bacterial pneumonia**, and **viral pneumonia**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOTUSYdUejyf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Step 1: Prepare Data for Multi-Class Classification\n",
    "# Separate paths for normal, bacterial and viral pneumonia paths\n",
    "normal_image_paths = [path for path in image_paths if 'normal' in path.lower()]\n",
    "bacterial_image_paths = [path for path in pneumonia_image_paths if 'bacteria' in path.lower()]\n",
    "viral_image_paths = [path for path in pneumonia_image_paths if 'virus' in path.lower()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htCNmgW2r32_"
   },
   "source": [
    "\n",
    "Here I divide the images into three categories based on their labels:\n",
    "- 'normal': Images that do not show signs of pneumonia.\n",
    "- 'bacteria': Images with bacterial pneumonia.\n",
    "- 'virus': Images with viral pneumonia.\n",
    "\n",
    "This separation is necessary for building a multi-class classification model, and each category represents a unique label for classification.\n",
    "By filtering the image paths based on keywords, separate datasets for each class can be prepared, which will help in assigning distinct labels for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JjEEEc4r4tP"
   },
   "outputs": [],
   "source": [
    "# Step 2: Sampling the Data for Balanced Dataset\n",
    "# - Sample 40% from each category to reduce dataset size, which helps manage computational cost.\n",
    "sample_size_normal = int(0.4 * len(normal_image_paths))\n",
    "sample_size_bacterial = int(0.4 * len(bacterial_image_paths))\n",
    "sample_size_viral = int(0.4 * len(viral_image_paths))\n",
    "\n",
    "# Use `random.sample` to get a random subset from each category.\n",
    "sampled_normal_image_paths = random.sample(normal_image_paths, sample_size_normal)\n",
    "sampled_bacterial_image_paths = random.sample(bacterial_image_paths, sample_size_bacterial)\n",
    "sampled_viral_image_paths = random.sample(viral_image_paths, sample_size_viral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HivRc_wBr3q_"
   },
   "source": [
    "- As dataset is relatively large via using colab, which makes model training computationally intensive.\n",
    "\n",
    "- To make training more feasible in terms of resources, I chose to sample 40% of images from each category.\n",
    "\n",
    "- Random sampling helps ensure that the selected subset is representative of the original data while reducing computational requirements.\n",
    "\n",
    "- random.sample() is used to randomly select a specific portion of the dataset to prevent sampling bias and to make sure the model generalizes well during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgXPdceLr4-w"
   },
   "outputs": [],
   "source": [
    "# Step 3: Combine the Sampled Image Paths\n",
    "# - Combine the sampled image paths for normal, bacterial, and viral pneumonia images.\n",
    "multi_class_image_paths = sampled_normal_image_paths + sampled_bacterial_image_paths + sampled_viral_image_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S185VsbdtnXR"
   },
   "source": [
    "- After obtaining the sampled images for each class, combine them into a single list called multi_class_image_paths.\n",
    "\n",
    "- This combination creates a unified dataset containing all three categories, which is essential for training the multi-class classifier.\n",
    "\n",
    "- This step allows to work with a combined dataset that can be further processed and fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8EKXgqwtumn"
   },
   "outputs": [],
   "source": [
    "# Step 4: Assign Labels to the Images\n",
    "# - Assign numerical labels to each category.\n",
    "# - 0: NORMAL, 1: BACTERIAL PNEUMONIA, 2: VIRAL PNEUMONIA\n",
    "normal_labels = [0] * len(sampled_normal_image_paths)\n",
    "bacterial_labels = [1] * len(sampled_bacterial_image_paths)\n",
    "viral_labels = [2] * len(sampled_viral_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-_c40AcqWgm"
   },
   "source": [
    "- Each image class needs to be assigned a numerical label for training purposes.\n",
    "\n",
    "- Assigning numerical labels allows the model to understand which category each image belongs to, which is essential for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3t4px3IuMQM"
   },
   "outputs": [],
   "source": [
    "# Step 5: Combine the Labels\n",
    "# - Combine labels for normal, bacterial, and viral pneumonia images.\n",
    "multi_class_labels = normal_labels + bacterial_labels + viral_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He8c2nniuauF"
   },
   "source": [
    "- After assigning labels to each category, combine them into a single list called multi_class_labels.\n",
    "\n",
    "- This combined list ensures that each image in multi_class_image_paths has a corresponding label in multi_class_labels.\n",
    "\n",
    "- Maintaining this one-to-one correspondence between images and labels allows the model to learn the relationships between input images and their respective categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO6WMvKBewei"
   },
   "source": [
    "### Rationale Part 2: Preprocessing and Augmentation\n",
    "- **Reason**: Image preprocessing, such as resizing and normalizing, ensures all images are in a consistent format for training. Augmentation helps increase the robustness of the model by introducing slight variations that it may encounter in real scenarios, reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOJjlcUsfFO6"
   },
   "outputs": [],
   "source": [
    "# Define a preprocessing and augmentation function to resize and normalize the images\n",
    "def preprocess_and_augment_image2(image_path):\n",
    "\n",
    "    # Load and decode the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=1)  # Convert to grayscale\n",
    "\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, [64, 64])  # Resize to 64x64 pixels\n",
    "\n",
    "    # Data augmentation\n",
    "    image = tf.image.random_flip_left_right(image)  # Randomly flip horizontally\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)  # Randomly adjust brightness\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)  # Randomly adjust contrast\n",
    "\n",
    "    # Normalize the image\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dt4hCPFvGSq"
   },
   "source": [
    "- **Loading and Decoding the Image**:\n",
    "  - `tf.io.read_file(image_path)` reads the image file from the given path.\n",
    "  - `tf.image.decode_jpeg(image, channels=1)` decodes the JPEG image into a format suitable for TensorFlow processing. Here, `channels=1` to convert the image to **grayscale** (single channel).\n",
    "  - Grayscale: Chest X-ray images are typically grayscale, so converting them to one channel helps reduce the model's complexity, as there's no need to process color information.\n",
    "\n",
    "- **Resizing the Image**:\n",
    "  - `tf.image.resize(image, [input_height, input_width])` resizes the image to a standard dimension (`input_height` x `input_width`), ensuring consistency across all images.\n",
    "  - During training, all images need to have the **same dimensions** so that the model can process them in batches.\n",
    "  -Makes sure that the model input layer always receives a fixed size.\n",
    "\n",
    "- **Random Augmentation**:\n",
    "  - `tf.image.random_flip_left_right(image)` randomly flips the image horizontally.\n",
    "  - `tf.image.random_brightness(image, max_delta=0.1)` randomly adjusts the brightness of the image within a specified range (`max_delta=0.1`).\n",
    "  - `f.image.random_contrast(image, lower=0.8, upper=1.2)` randomly adjusts the contrast of the image within range 0.8 to 1.2\n",
    "  - Image augmentation introduces variability to help the model generalize better and reduce overfitting.\n",
    "\n",
    "- **Normalization**:\n",
    "  - `tf.cast(image, tf.float32) / 255.0` converts pixel values from `uint8` (0 to 255) to `float32` and scales them to a range between **0 and 1**.\n",
    "  - Normalization makes it easier for the model to learn and converge during training.\n",
    "  - It ensures that all input values are in a consistent range, which helps in reducing bias during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2nng4Z7fNVH"
   },
   "source": [
    "### Rationale Part 3: Data Pipeline Optimization and Splitting the Dataset\n",
    "- **Reason**: To ensure a well-structured training process, the dataset is split into training and validation sets. Proper batching and optimization of the data pipeline improve training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar_NGymQ1s_4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEbVigUEfMkX"
   },
   "outputs": [],
   "source": [
    "# Step 1: Split the Dataset and Shuffle\n",
    "# Split the Dataset into training and validation sets\n",
    "dataset_size = len(multi_class_image_paths)\n",
    "train_size = int(0.8 * dataset_size)  # 80% for training\n",
    "val_size = dataset_size - train_size  # 20% for validation\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "train_dataset = multi_class_dataset_filtered.take(train_size)\n",
    "val_dataset = multi_class_dataset_filtered.skip(train_size)\n",
    "\n",
    "# # Batch the dataset\n",
    "# batch_size = 128\n",
    "# train_dataset_batched = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "# val_dataset_batched = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siFTSEpo2EYn"
   },
   "source": [
    " - determine the size of the entire dataset and then split it into 80% for training (`train_size`) and 20% for validation (`val_size`).\n",
    " - use `.take(train_size)` and `.skip(train_size)` to create the training and validation datasets.\n",
    " - As I have a better batching operation in step 3, this batch step here results in repeating the batching operation, which does not cause error but it is redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNi50uB2yQmA"
   },
   "outputs": [],
   "source": [
    "# Step 2: Calculate Steps per Epoch Based on Dataset Size and Batch Size\n",
    "# Total sampled images for training\n",
    "total_sampled_images = len(sampled_normal_image_paths) + len(sampled_bacterial_image_paths) + len(sampled_viral_image_paths)\n",
    "\n",
    "# Steps per epoch calculation\n",
    "steps_per_epoch = total_sampled_images // batch_size\n",
    "validation_steps = val_size // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0ZsDAkVzN-X"
   },
   "source": [
    "- Steps per Epoch:\n",
    "  - `steps_per_epoch` represents the number of batches that the model will process in one complete pass through the training dataset.\n",
    "  - It is calculated as the total number of sampled images divided by the batch size (`total_sampled_images // batch_size`).\n",
    "- Validation Steps:\n",
    "  - Similarly, `validation_steps` is calculated to determine how many validation batches are processed per epoch.\n",
    "- Reason of Calculation:\n",
    "  - Knowing the number of steps per epoch is crucial for controlling the length of each training epoch.\n",
    "  - This ensures the model iterates through the entire dataset during each epoch, enabling the training to progress consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s91NjEXTyPiu"
   },
   "outputs": [],
   "source": [
    "# Step 3: Batch the Dataset and Repeat to Avoid Running Out of Data\n",
    "# Batch the dataset and repeat to avoid running out of data\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "val_dataset = val_dataset.batch(batch_size, drop_remainder=True).repeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF6kUw720O4Y"
   },
   "source": [
    "- Batching:\n",
    "  - `train_dataset.batch(batch_size, drop_remainder=True)` splits the dataset into smaller **batches** of a specified size (`batch_size`). This allows the model to process multiple images at once, improving training speed and computational efficiency.\n",
    "  - The `drop_remainder=True` parameter ensures that only full batches are used, preventing issues if the total number of images is not divisible by the batch size.\n",
    "\n",
    "- Repeat:\n",
    "  - `.repeat()` repeats the dataset infinitely. It allows the model to continue accessing the training and validation data without running out of data after one epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGz9PJbufHqa"
   },
   "outputs": [],
   "source": [
    "# Step 4: Data Pipeline Optimization with Prefetching\n",
    "# Data Pipeline Optimization\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7nhWyAJ0OXW"
   },
   "source": [
    "- Prefetching:\n",
    "  - `.prefetch(tf.data.experimental.AUTOTUNE)` is used to improve the efficiency of data input by overlapping the data preparation and model training steps.\n",
    "  - `tf.data.experimental.AUTOTUNE` automatically tunes the prefetch buffer size to optimize performance.\n",
    "  - Prefetching ensures that while the model is training on the current batch, the data for the next batch is already being prepared.\n",
    "  - This helps in reducing idle time for the GPU/CPU, thereby making training faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukzoHm_ygLS8"
   },
   "source": [
    "### Rationale Part 4: Multi-Class VAE and Classifier\n",
    "- **Reason**: For this task, a **Variational Autoencoder (VAE)** was used to learn a latent representation of the input images. This latent space can then be used for reconstruction as well as classification. The **classifier** model is built on top of the latent vectors produced by the VAE to distinguish between the three classes (normal, bacterial pneumonia, viral pneumonia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nq0SDbFN5LLB"
   },
   "outputs": [],
   "source": [
    "# Define activation choice with updated parameter\n",
    "activation_choice = 'LeakyReLU'  # Options: 'LeakyReLU' or 'ReLU'\n",
    "\n",
    "# Set activation function based on choice\n",
    "if activation_choice == 'LeakyReLU':\n",
    "    activation = layers.LeakyReLU(negative_slope=0.01)\n",
    "else:\n",
    "    activation = layers.ReLU()\n",
    "\n",
    "# Adjusted latent dimension for better feature capture\n",
    "latent_dim2 = 128  # Increased to capture more complex features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY22Dxn29RUN"
   },
   "source": [
    "- Using  `LeakyReLU` as activation function instead of `ReLU`, which helps to mitigate issues related to neurons \"dying\" during training (i.e., neurons getting stuck with zero output). This choice can improve the model’s ability to learn complex relationships, especially with medical image data where subtle features may be crucial.\n",
    "\n",
    "- Using `latent_dim2 = 128` instead of `latent_dim = 64`. This increasing the latent dimension allows the model to capture more complex features from the data, which is helpful when dealing with images that have a high degree of variability, such as medical images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2--5Mc_AghW4"
   },
   "outputs": [],
   "source": [
    "# VAE Encoder\n",
    "# Step 2.1: Define the encoder with Dropout and Batch Normalization\n",
    "def build_encoder2(input_shape=(64, 64, 1), latent_dim=latent_dim2):\n",
    "    encoder_inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), strides=2, padding='same')(encoder_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder2\")\n",
    "    return encoder\n",
    "\n",
    "encoder2 = build_encoder2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBgiCRoc-VqU"
   },
   "source": [
    "- Input Layer and Convolution:\n",
    "  - The input shape is `(64, 64, 1)`, representing 64x64 pixel grayscale images.\n",
    "  - Convolutional layers are used to extract relevant features from the images, while `BatchNormalization()` helps to stabilize and speed up the training process.\n",
    "- LeakyReLU Activation:\n",
    "  - The `LeakyReLU` activation function helps prevent the vanishing gradient problem by allowing small negative values instead of setting them to zero (as in `ReLU`). This is especially useful for deep networks.\n",
    "- Dropout:\n",
    "  - Dropout (`0.4`) is applied after each convolution and dense layer to prevent **overfitting**, ensuring the model generalizes better to unseen data.\n",
    "- Latent Space:\n",
    "  - `z_mean` and `z_log_var` are learned to model the latent distribution for the input images.\n",
    "  - The Sampling layer generates a latent vector (`z`) using `z_mean` and `z_log_var`, ensuring variability in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1UIT_tdghMj"
   },
   "outputs": [],
   "source": [
    "# VAE Decoder\n",
    "# Step 2.2: Define the decoder\n",
    "def build_decoder2(latent_dim=latent_dim2):\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(16 * 16 * 64)(latent_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Reshape((16, 16, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    decoder = models.Model(latent_inputs, decoder_outputs, name=\"decoder2\")\n",
    "    return decoder\n",
    "\n",
    "decoder2 = build_decoder2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8zHxw0Q-wJ9"
   },
   "source": [
    "- Latent Input and Dense Layers:\n",
    "  - The decoder starts with a dense layer that transforms the latent vector into a shape that can be reshaped into a feature map.\n",
    "- Upsampling and Convolutional Layers:\n",
    "  - The `Conv2DTranspose` layers, paired with `BatchNormalization` and `LeakyReLU`, are used to gradually restore the spatial dimensions.\n",
    "  - The output layer uses a `sigmoid` activation function to produce values between 0 and 1, representing reconstructed grayscale images.\n",
    "- Dropout:\n",
    "  - Applying `Dropout` in the decoder helps prevent overfitting by ensuring the model does not rely too heavily on specific neurons during reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnB3-m2p_NDN"
   },
   "outputs": [],
   "source": [
    "# Classifier for Latent Space\n",
    "# Step 2.3: Build a simple classifier model for multi-class classification using the latent vectors.\n",
    "def build_classifier(latent_dim=latent_dim2):\n",
    "    classifier_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(classifier_inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # The output shape should match the batch size (None) and number of classes (3)\n",
    "    classifier_outputs = layers.Dense(3, activation='softmax')(x)  # Ensure batch-wise output\n",
    "    classifier = models.Model(classifier_inputs, classifier_outputs, name=\"classifier\")\n",
    "    return classifier\n",
    "\n",
    "classifier = build_classifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIichb-O_URH"
   },
   "source": [
    "- Latent Input:\n",
    "  - The input to the classifier is the latent vector produced by the encoder. This vector is a low-dimensional representation of the original input image, which encodes important features for classification.\n",
    "  - The classifier takes the latent vector (`latent_dim2=128`) as input. This vector represents the compressed representation of the input image.\n",
    "- Fully Connected Layers:\n",
    "  - Two dense layers are used with 64 and 32 units respectively, to learn non-linear combinations of the features (extracted by the encoder) specific for classification.\n",
    "  - `ReLU` activation is used for non-linearity, helping the classifier to learn complex decision boundaries.\n",
    "- Dropout Layers:\n",
    "  - Dropout (0.3) is applied after each dense layer to prevent overfitting. Dropout randomly \"drops\" units during training, ensuring that the classifier does not become overly reliant on any specific features and learns generalized patterns.\n",
    "- Output Layer:\n",
    "  - The output layer has 3 units, corresponding to the three classes (normal, bacterial pneumonia, viral pneumonia).\n",
    "  - `Softmax` activation ensures the output represents class probabilities, which is suitable for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciJLeeXc_7Oj"
   },
   "outputs": [],
   "source": [
    "# Combining Encoder, Decoder, and Classifier\n",
    "# Define input layer\n",
    "inputs = tf.keras.Input(shape=(64, 64, 1))\n",
    "\n",
    "# Get the outputs from the encoder, decoder, and classifier\n",
    "z_mean, z_log_var, z = encoder2(inputs)\n",
    "reconstruction_output = decoder2(z)\n",
    "classification_output = classifier(z)\n",
    "\n",
    "# Rename the outputs explicitly using Keras' Lambda layer\n",
    "reconstruction_output = tf.keras.layers.Lambda(lambda x: x, name='reconstruction_output')(reconstruction_output)\n",
    "classification_output = tf.keras.layers.Lambda(lambda x: x, name='classification_output')(classification_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dIwseds__yF"
   },
   "source": [
    "- Combining Components:\n",
    "  - The encoder, decoder, and classifier are combined into a single model that takes an image as input and outputs:\n",
    "    - Reconstructed Image (via decoder).\n",
    "    - Class Probabilities (via classifier).\n",
    "- Input Layer:\n",
    "  - The input layer takes an image with dimensions (64, 64, 1), representing a grayscale image of size 64x64 pixels.\n",
    "- Encoder Output:\n",
    "  - The input image is passed through the encoder, which outputs the latent vector (z) along with the mean (z_mean) and log variance (z_log_var) used for the sampling.\n",
    "  - The latent vector (z) represents the key features extracted by the encoder.\n",
    "- Linking Latent Vectors to Classifier:\n",
    "  - The latent vector (z) is then passed to the classifier, which processes it through its fully connected layers and outputs a probability distribution over the three classes.\n",
    "  - This linkage between the encoder and classifier is what allows the model to take a raw input image and classify it after encoding it into a latent representation.\n",
    "- Renaming Outputs:\n",
    "  - The `Lambda` layer is used to explicitly rename the outputs for easier reference and checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOkdwMERgo2c"
   },
   "outputs": [],
   "source": [
    "# Create the model with the renamed outputs\n",
    "vae_with_classifier_model = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=[reconstruction_output, classification_output]\n",
    ")\n",
    "\n",
    "# Compile the model with correct output names in the loss function\n",
    "vae_with_classifier_model.compile(\n",
    "    optimizer=AdamW(learning_rate=0.001),\n",
    "    loss={\n",
    "        'classification_output': 'categorical_crossentropy',  # Classification loss\n",
    "        'reconstruction_output': 'mse'  # Reconstruction loss\n",
    "    },\n",
    "    metrics={'classification_output': 'accuracy'}  # Track accuracy for classification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvph-kX5BFc7"
   },
   "source": [
    "- Model Creation:\n",
    "  - The VAE with Classifier model (vae_with_classifier_model) takes an input image and outputs both:\n",
    "    1. Reconstructed Image (reconstruction_output), generated by the decoder.\n",
    "    2. Class Probabilities (classification_output), generated by the classifier.\n",
    "  - This structure allows the model to learn both tasks: reconstructing the input image and classifying it into one of the three categories.\n",
    "\n",
    "- Loss Functions:\n",
    "  - The model is trained with two loss functions:\n",
    "    1. `categorical_crossentropy` is used for training the classifier to measure how well the model assigns the correct label to each input. The model is penalized based on the difference between the predicted class probabilities and the actual class label (one-hot encoded). This helps the model learn to classify images correctly.\n",
    "    2. Mean Squared Error (`mse`) is used for training the VAE decoder, which measures how well the decoder is able to reconstruct the original input image from the latent vector.\n",
    "\n",
    "- AdamW Optimizer:\n",
    "  - `AdamW` is used for optimization, which is a variant of Adam that includes weight decay. It helps in **regularizing** the model, thereby reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ndlyxgkgpIv"
   },
   "source": [
    "### Rationale Part 5: Building the Classifier and Incorporating Multi-Class Outputs\n",
    "- Reason: The latent vectors from the encoder are then used to perform classification. A separate classifier model is trained using the latent vectors, which helps in distinguishing among normal, bacterial pneumonia, and viral pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eeqjtjxhg69L"
   },
   "outputs": [],
   "source": [
    "# Create the model with the renamed outputs\n",
    "vae_with_classifier_model = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=[reconstruction_output, classification_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSSAU7jKo3PD"
   },
   "outputs": [],
   "source": [
    "# Compile the model with correct output names in the loss function\n",
    "vae_with_classifier_model.compile(\n",
    "    optimizer=AdamW(learning_rate=0.001),\n",
    "    loss={\n",
    "        'classification_output': 'categorical_crossentropy',  # Classification loss\n",
    "        'reconstruction_output': 'mse'  # Reconstruction loss\n",
    "    },\n",
    "    metrics={'classification_output': 'accuracy'}  # Track accuracy for classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnZGtFSRClwq"
   },
   "source": [
    "### Overall Explanation of Classification Process\n",
    "In this question, I employed a VAE, followed by a classifier to classify chest X-ray images into three categories:\n",
    "1. Normal\n",
    "2. Bacterial Pneumonia\n",
    "3. Viral Pneumonia\n",
    "\n",
    "This approach can be seen in two major components:\n",
    "- VAE Encoder: Used to convert images into a latent vector representation.\n",
    "- Classifier: The classifier that takes the latent vector as input and outputs the probability of each class.\n",
    "\n",
    "Here is a detailed explanation of the entire process:\n",
    "\n",
    "### 1. Linking the Latent Vectors to Labels for Classification\n",
    "#### 1.1 Encoder and Latent Space Representation\n",
    "The encoder is the first part of the VAE. It compresses an input image into a latent vector in a lower-dimensional latent space. Here's how it works:\n",
    "- Each input image (`64x64x1`) is passed through several convolutional layers, batch normalization, and activation functions (LeakyReLU here).\n",
    "- The output of the convolutional layers is then flattened and transformed into two vectors: **`z_mean`** and **`z_log_var`**.\n",
    "  - `z_mean`: Represents the mean of the latent distribution.\n",
    "  - `z_log_var`: Represents the log variance of the latent distribution.\n",
    "\n",
    "The purpose of the latent vector is to encode important features of the image in a compact form. The latent vector is computed by sampling from a distribution defined by `z_mean` and `z_log_var`. This sampled vector (`z`) is use to represent the input image in a compressed form. This is essentially a feature representation of the original image.\n",
    "\n",
    "\n",
    "#### 1.2 Classifier for Latent Vectors\n",
    "##### 1.2.1 Fedding latent vector into the classifier model:\n",
    "- The classifier is a simple feedforward neural network (with dense layers) that takes the latent vector (`z`) as input.\n",
    "- The classifier then passes the latent vector through a few dense layers (fully connected layers) with `ReLU` activation and `Dropout` to learn non-linear relationships.\n",
    "- Then, the output layer uses a `softmax` activation function to produce a probability distribution over the three classes.\n",
    "\n",
    "##### 1.2.2 The output from the classifier:\n",
    "- A vector with three values each representing the probability that the given input image belongs to one of the three classes (normal, bacterial pneumonia viral pneumonia).\n",
    "\n",
    "\n",
    "### 2. How the Model Learns to Distinguish Between the Three Classes\n",
    "The training process involves teaching the model to learn the distinguishing features of each class, which happens in the following way:\n",
    "\n",
    "\n",
    "#### 2.1 One-Hot Encoding of Labels\n",
    "- The labels are one-hot encoded before feeding into the model.\n",
    "  - Normal: `[1, 0, 0]`\n",
    "  - Bacterial Pneumonia: `[0, 1, 0]`\n",
    "  - Viral Pneumonia: `[0, 0, 1]`\n",
    "  \n",
    "This one-hot encoding allows the model to learn which vector corresponds to which class, providing a target for the classification part of the network.\n",
    "\n",
    "\n",
    "#### 2.2 Convolutional Feature Extraction by the Encoder\n",
    "- Convolutional Layers in the encoder are responsible for feature extraction.\n",
    "  - Filters in these layers detect specific patterns such as edges, textures, shapes, etc.\n",
    "  - By passing the images through several convolutional layers, the encoder extracts the most important features that are required to identify the differences between the classes.\n",
    "  \n",
    "- For example:\n",
    "  - For normal chest X-rays, the model might learn to detect features that indicate clear lung fields.\n",
    "  - For bacterial pneumonia, it might learn features like dense, localized opacity, which are common in bacterial infections.\n",
    "  - For viral pneumonia, the features might include diffuse opacity patterns that are distinct from bacterial infections.\n",
    "\n",
    "\n",
    "#### 2.3 Latent Space Representation\n",
    "- The latent vector (`z`) is a compressed representation of the image. It captures the key features extracted by the encoder.\n",
    "- The encoder compresses images into a latent space in such a way that similar images have similar latent representations.\n",
    "  - Images of normal lungs, for instance, would have latent vectors close to each other.\n",
    "  - Similarly, images of bacterial pneumonia and viral pneumonia would be clustered in different areas of the latent space, but they would still maintain some/certain similarities based on shared pneumonia features.\n",
    "\n",
    "\n",
    "#### 2.4 Classifier to Distinguish Classes\n",
    "- The classifier receives the latent vector (`z`) and passes it through dense layers to map the latent features to class probabilities.\n",
    "- The goal of classifier is to find **decision boundaries** in the latent space that separate the three classes.\n",
    "  - During training, the **loss function** (categorical cross-entropy) penalizes incorrect classifications.\n",
    "  - For every image, the model tries to predict the class by minimizing the distance between its prediction and the true label.\n",
    "  - Over many epochs, the classifier learns most important features (encoded in the latent vector) for such as patterns, textures, and opacities, which are key indicators of the image's class identifying.\n",
    "    - For example, features like opacity patterns might be indicative of pneumonia, while their distribution (localized vs. diffuse) could help distinguish between bacterial and viral pneumonia.\n",
    "\n",
    "\n",
    "#### 2.5 Softmax Output for Classification\n",
    "- The `softmax` function at the final output layer converts the output into a probability distribution over the three classes.\n",
    "- The highest probability indicates the class the model thinks the image belongs to.\n",
    "  - For example, for a **normal** image, the output might look like `[0.98, 0.01, 0.01]`, indicating a **98% probability** that the image is normal.\n",
    "  - An another example, if the classifier produce an output like `[0.05, 0.85, 0.10]`, indicating that the model predicts an 85% probability that the image shows bacterial pneumonia.\n",
    "\n",
    "I hope this explanation clarifies how the CNN classifier implemented and how the labels are linked to the latent vectors to differentiate between the three classes.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPRN8miPwvIPxMcJrW3Usko",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
