{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cx8YUZuzoI35ArpI1Nbo_-4QHN83fxZc","timestamp":1730952769891}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# The Importance of Understanding Antiretroviral Therapy (ART) for HIV\n","\n","Antiretroviral Therapy (ART) plays a critical role in managing HIV infection. By suppressing the viral load and boosting the immune system, ART improves the quality of life for patients, helps control drug resistance, and tailors treatment strategies to individual needs. In this way, ART has been instrumental in turning HIV from a life-threatening condition into a manageable chronic illness for millions of people worldwide.\n","\n","However, optimising ART regimens is a complex challenge. Each patient’s response to treatment can vary depending on numerous factors, such as their baseline viral load, CD4 count, and the combination of medications they receive. This is where data science plays a crucial role: by analysing patient data, we can uncover insights that improve treatment outcomes, drive innovations in therapy design, and support public health policies.\n","\n","In this assignment, students are expected to harness the power of **machine learning algorithms** to analyse and predict patient outcomes based on their ART regimens and health indicators. By doing so, you'll not only engage with real-world clinical data but also learn how to apply advanced predictive modeling techniques that can ultimately enhance the way we understand and manage HIV.\n","\n","As we work through this dataset, you'll apply various machine learning algorithms to identify patterns and trends that influence treatment success. By the end of this assignment, you'll have a stronger grasp of how machine learning can be used to make data-driven decisions in clinical settings, particularly in managing chronic diseases like HIV.\n"],"metadata":{"id":"Kdd5vJjWJ5PA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EJ-FT4tJy-A"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"QSYM7PdaKAvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Bhgg9RNVKAxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Pre-processing Code: Important Instructions\n","\n","In this assignment, you are provided with a pre-processing script that will prepare the dataset for analysis. This code is essential for ensuring that the data is cleaned and formatted correctly.\n","\n","You will need to run the code provided below to ensure the data is correctly processed before starting your analysis.\n","\n","**IMPORTANT: You SHOULD NOT modify the code.** It is crucial that this script remains unchanged, as any modifications could result in errors or inconsistencies in the dataset that could affect your subsequent analysis.\n","\n","Please follow these instructions carefully:\n","\n","1. Copy the code as it is.\n","2. Run the code before you begin your machine learning analysis.\n","3. Do not attempt to change or adjust the code in any way.\n","\n","**CAUTION: DO NOT MODIFY, DO NOT MODIFY, DO NOT MODIFY.**\n"],"metadata":{"id":"wP1yw54tKBFl"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Call the seed function at the beginning of your code\n","set_seed(42)\n","\n","# Load dataset\n","All_Data = pd.read_csv(\"https://figshare.com/ndownloader/files/40584980\")\n","\n","# Drop unnecessary columns\n","All_Data.drop(['VL (M)', 'CD4 (M)'], axis=1, inplace=True)\n","\n","# Replace categorical codes with meaningful labels\n","replace_dict = {\n","    \"Gender\": {1: \"Male\", 2: \"Female\"},\n","    \"Ethnic\": {1: \"Asian\", 2: \"African\", 3: \"Caucasian\", 4: \"Other\"},\n","    \"Base Drug Combo\": {0: \"FTC + TDF\", 1: \"3TC + ABC\", 2: \"FTC + TAF\", 3: \"DRV + FTC + TDF\", 4: \"FTC + RTVB + TDF\", 5: \"Other\"},\n","    \"Comp. INI\": {0: \"DTG\", 1: \"RAL\", 2: \"EVG\", 3: \"Not Applied\"},\n","    \"Comp. NNRTI\": {0: \"NVP\", 1: \"EFV\", 2: \"RPV\", 3: \"Not Applied\"},\n","    \"Extra PI\": {0: \"DRV\", 1: \"RTVB\", 2: \"LPV\", 3: \"RTV\", 4: \"ATV\", 5: \"Not Applied\"},\n","    \"Extra pk-En\": {0: \"False\", 1: \"True\"}\n","}\n","All_Data.replace(replace_dict, inplace=True)\n","\n","# Set drug-related columns to NaN where 'Drug (M)' equals 0\n","Drug_Combo = ['Base Drug Combo', 'Comp. INI', 'Comp. NNRTI', 'Extra PI', 'Extra pk-En']\n","All_Data.loc[All_Data[\"Drug (M)\"] == 0, Drug_Combo] = np.nan\n","\n","# Drop 'Drug (M)' column\n","All_Data.drop(['Drug (M)'], axis=1, inplace=True)\n","\n","# Display first few rows\n","All_Data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Aq91J4QRKEke","executionInfo":{"status":"ok","timestamp":1731358618827,"user_tz":-660,"elapsed":26964,"user":{"displayName":"Peter Zhang","userId":"11053071034181927577"}},"outputId":"5e577d50-40c3-4c00-ac5e-a7503e23914f"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          VL        CD4    Rel CD4 Gender     Ethnic Base Drug Combo  \\\n","0  29.944271  793.45830  30.834505   Male  Caucasian       FTC + TDF   \n","1  29.241980  467.41890  30.355980   Male  Caucasian       FTC + TDF   \n","2  28.748991  465.12485  30.405320   Male  Caucasian       FTC + TDF   \n","3  28.101835  692.00690  30.248816   Male  Caucasian       FTC + TDF   \n","4  28.813837  641.75714  29.944712   Male  Caucasian       FTC + TDF   \n","\n","  Comp. INI  Comp. NNRTI     Extra PI Extra pk-En  PatientID  Timestep  \n","0       DTG  Not Applied  Not Applied       False          0         0  \n","1       DTG  Not Applied  Not Applied       False          0         1  \n","2       DTG  Not Applied  Not Applied       False          0         2  \n","3       DTG  Not Applied  Not Applied       False          0         3  \n","4       DTG  Not Applied  Not Applied       False          0         4  "],"text/html":["\n","  <div id=\"df-39a078a2-cf46-452a-a58f-72d902514ee4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>VL</th>\n","      <th>CD4</th>\n","      <th>Rel CD4</th>\n","      <th>Gender</th>\n","      <th>Ethnic</th>\n","      <th>Base Drug Combo</th>\n","      <th>Comp. INI</th>\n","      <th>Comp. NNRTI</th>\n","      <th>Extra PI</th>\n","      <th>Extra pk-En</th>\n","      <th>PatientID</th>\n","      <th>Timestep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.944271</td>\n","      <td>793.45830</td>\n","      <td>30.834505</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>FTC + TDF</td>\n","      <td>DTG</td>\n","      <td>Not Applied</td>\n","      <td>Not Applied</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.241980</td>\n","      <td>467.41890</td>\n","      <td>30.355980</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>FTC + TDF</td>\n","      <td>DTG</td>\n","      <td>Not Applied</td>\n","      <td>Not Applied</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>28.748991</td>\n","      <td>465.12485</td>\n","      <td>30.405320</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>FTC + TDF</td>\n","      <td>DTG</td>\n","      <td>Not Applied</td>\n","      <td>Not Applied</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>28.101835</td>\n","      <td>692.00690</td>\n","      <td>30.248816</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>FTC + TDF</td>\n","      <td>DTG</td>\n","      <td>Not Applied</td>\n","      <td>Not Applied</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28.813837</td>\n","      <td>641.75714</td>\n","      <td>29.944712</td>\n","      <td>Male</td>\n","      <td>Caucasian</td>\n","      <td>FTC + TDF</td>\n","      <td>DTG</td>\n","      <td>Not Applied</td>\n","      <td>Not Applied</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39a078a2-cf46-452a-a58f-72d902514ee4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-39a078a2-cf46-452a-a58f-72d902514ee4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-39a078a2-cf46-452a-a58f-72d902514ee4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-878cdb0d-e711-48fa-86f1-778c1f6cda50\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-878cdb0d-e711-48fa-86f1-778c1f6cda50')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-878cdb0d-e711-48fa-86f1-778c1f6cda50 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"All_Data"}},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["## Simplifying Categorical Data\n","\n","To simplify the dataset and make it easier to work with, we will convert all categorical variables into numeric levels. This step is necessary because many machine learning algorithms require numeric input rather than categorical strings.\n","\n","We will be using a **LabelEncoder** to transform the following categorical columns:\n","\n","- Gender\n","- Ethnic group\n","- Base Drug Combo\n","- Companion INI\n","- Companion NNRTI\n","- Extra PI\n","- Extra pk-En\n","\n","The **LabelEncoder** is used here to convert each unique category in these columns into a numeric value. For example, if the `Gender` column contains values like `['Male', 'Female', 'Other']`, the LabelEncoder might convert them into `[0, 1, 2]`. Each unique string is mapped to a unique integer, allowing us to replace the categorical values with numerical ones.\n","\n","**Note**: This is *not* one-hot encoding. Each category is represented by a single integer rather than creating multiple binary columns, which helps to simplify the representation of categorical data without expanding the dataset's dimensionality.\n","\n","This transformation will allow us to proceed with the machine learning algorithms, as they will now be able to process the dataset efficiently.\n","\n","**IMPORTANT: You SHOULD NOT modify the code.** It is crucial that this script remains unchanged, as any modifications could result in errors or inconsistencies in the dataset that could affect your subsequent analysis.\n","\n","Please follow these instructions carefully:\n","\n","1. Copy the code as it is.\n","2. Run the code before you begin your machine learning analysis.\n","3. Do not attempt to change or adjust the code in any way.\n","\n","**CAUTION: DO NOT MODIFY, DO NOT MODIFY, DO NOT MODIFY.**\n"],"metadata":{"id":"QSZiL-bvLwog"}},{"cell_type":"code","source":["set_seed(42)\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Convert all categorical levels back into numeric levels using LabelEncoder\n","label_cols = ['Gender', 'Ethnic', 'Base Drug Combo', 'Comp. INI', 'Comp. NNRTI', 'Extra PI', 'Extra pk-En']\n","le = LabelEncoder()\n","\n","# Apply LabelEncoder to each categorical column\n","for col in label_cols:\n","    All_Data[col] = le.fit_transform(All_Data[col].astype(str))\n","\n","# Display first few rows\n","All_Data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"9peLv-5nLzm9","executionInfo":{"status":"ok","timestamp":1731359716674,"user_tz":-660,"elapsed":1197,"user":{"displayName":"Peter Zhang","userId":"11053071034181927577"}},"outputId":"c565d0b8-b04c-4687-92c7-5f47e49bc617"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          VL        CD4    Rel CD4  Gender  Ethnic  Base Drug Combo  \\\n","0  29.944271  793.45830  30.834505       1       2                4   \n","1  29.241980  467.41890  30.355980       1       2                4   \n","2  28.748991  465.12485  30.405320       1       2                4   \n","3  28.101835  692.00690  30.248816       1       2                4   \n","4  28.813837  641.75714  29.944712       1       2                4   \n","\n","   Comp. INI  Comp. NNRTI  Extra PI  Extra pk-En  PatientID  Timestep  \n","0          0            2         3            0          0         0  \n","1          0            2         3            0          0         1  \n","2          0            2         3            0          0         2  \n","3          0            2         3            0          0         3  \n","4          0            2         3            0          0         4  "],"text/html":["\n","  <div id=\"df-670894b7-3a35-40aa-ab44-1decd973c134\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>VL</th>\n","      <th>CD4</th>\n","      <th>Rel CD4</th>\n","      <th>Gender</th>\n","      <th>Ethnic</th>\n","      <th>Base Drug Combo</th>\n","      <th>Comp. INI</th>\n","      <th>Comp. NNRTI</th>\n","      <th>Extra PI</th>\n","      <th>Extra pk-En</th>\n","      <th>PatientID</th>\n","      <th>Timestep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.944271</td>\n","      <td>793.45830</td>\n","      <td>30.834505</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.241980</td>\n","      <td>467.41890</td>\n","      <td>30.355980</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>28.748991</td>\n","      <td>465.12485</td>\n","      <td>30.405320</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>28.101835</td>\n","      <td>692.00690</td>\n","      <td>30.248816</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28.813837</td>\n","      <td>641.75714</td>\n","      <td>29.944712</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-670894b7-3a35-40aa-ab44-1decd973c134')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-670894b7-3a35-40aa-ab44-1decd973c134 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-670894b7-3a35-40aa-ab44-1decd973c134');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f8b0412c-b953-433f-8ce9-4b678d340a58\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8b0412c-b953-433f-8ce9-4b678d340a58')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f8b0412c-b953-433f-8ce9-4b678d340a58 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"All_Data"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":[],"metadata":{"id":"bflI_WgSOqra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vmjGfw-ROqtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-EQSRcIcOq5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Assignment: Code Reading Comprehension\n","\n","## Predicting Patient's Future State for Illness Condition Management\n","\n","In managing chronic illnesses like HIV, predicting a patient's future condition is essential for optimising treatment strategies and ensuring timely intervention. For **Antiretroviral Therapy (ART)** in HIV management, a critical question is whether, by analysing a patient's early information (data from the first 3 timesteps), we can accurately predict their health status at a future point (i.e., at the 59th timestep).\n","\n","In this assignment, we aim to predict whether the viral load (VL) at the 59th timestep is below 200 using the early information of a patient. Viral load ≤ 200 is a critical clinical indicator of **viral suppression**, a key goal of ART in HIV treatment. Knowing whether viral load will be suppressed in the future helps inform treatment adjustments and patient care.\n","\n","To achieve this, we will use a **3-layer Multi-Layer Perceptron (MLP)**. The input to the MLP will include all features across the initial 3 timesteps (0, 1, 2), and the target will be predicting whether the viral load at the 59th timestep is less than or equal to 200.\n","\n","## Instructions\n","\n","For this question, we do **not** expect you to code everything from scratch. Instead, we have provided a **working example** of the required steps. Your task is to read and understand the code.\n","\n","Whenever you see the following:\n","\n","```python\n","#-----------------------------------------\n","# [Your answer here:]\n","# (WRITE HERE)\n","#-----------------------------------------\n","```\n","We expect you to write your understanding or explanation in the space marked as **(WRITE HERE)**. Make sure to comment your answers with a `#` to ensure the code still runs correctly.\n","\n","Your comprehension will be tested on the following topics:\n","\n","1. **Data Preprocessing**: Understanding how the input data is prepared, reshaped, and standardised.\n","2. **MLP Backbone Network Definition**: Comprehending how the MLP layers are defined and what their roles are.\n","3. **Training the Model**: Recognising the steps in the training loop, including forward pass, loss calculation, and backpropagation.\n","\n","## Why This is Important\n","\n","Predicting future viral load is crucial in managing HIV. Clinicians rely on viral suppression (VL ≤ 200) to gauge the effectiveness of ART. Being able to predict this based on early patient information helps adjust treatments proactively, improving patient outcomes.\n","\n","Now, proceed to the provided code example and fill in the explanation wherever prompted. Understanding each step in this process will enhance your ability to read, understand, and eventually build complex models for clinical data prediction tasks.\n","\n","Good luck!\n"],"metadata":{"id":"2x7NvY7wOr0K"}},{"cell_type":"markdown","source":["## Assignment Marks Breakdown (Total: 10 Marks)\n","\n","This question is worth **10 marks** and is divided into the following sections:\n","\n","1. **Data Preprocessing**: Understanding how the input data is prepared, reshaped, and standardised.\n","   - There are **6 specific places** where students are expected to comment on the code.\n","   - Each valid and correct comment is awarded **1 mark**, resulting in a total of **6 marks** for this section.\n","\n","2. **MLP Backbone Network Definition**: Comprehending how the MLP layers are defined and what their roles are.\n","   - There are **2 specific places** where students are expected to comment on the code.\n","   - Each valid and correct comment is awarded **1 mark**, resulting in a total of **2 marks** for this section.\n","\n","3. **Training the Model**: Recognising the steps in the training loop, including forward pass, loss calculation, and backpropagation.\n","   - There are **2 specific places** where students are expected to comment on the code.\n","   - Each valid and correct comment is awarded **1 mark**, resulting in a total of **2 marks** for this section.\n","\n","Thus, the total marks breakdown is as follows:  \n","**6 marks** for Data Preprocessing + **2 marks** for MLP Backbone Network Definition + **2 marks** for Training the Model = **10 Marks** in total.\n","\n","### Important Notice:\n","\n","It is expected that some of the required comments may not be immediately straightforward or directly obvious to the students. However, it should be noted that most of the answers can either be determined by **searching for relevant information online** or through **practical test-and-trial** methods to understand the underlying meaning behind the code.\n","\n","Students are **strongly encouraged** to take their time in reviewing the code and applying appropriate methods to arrive at the correct explanations. This assignment has been designed not only to test comprehension but also to develop problem-solving skills.\n","\n","This is a **strict requirement** and no partial marks will be awarded unless the comments accurately reflect an understanding of the relevant section of the code.\n"],"metadata":{"id":"ZMu2CgwcO2yl"}},{"cell_type":"code","source":["# -----------------------------------------\n","# Setting a random seed (set_seed(42)) ensures reproducibility (that any random operations produce the same results each time the code is run)\n","# Libraries provide tools for data handling (NumPy, Pandas)\n","#                                      data preprocessing (LabelEncoder, StandardScaler)\n","#                                      machine learning/model building (PyTorch)\n","# -----------------------------------------\n","set_seed(42)\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# -----------------------------------------\n","# Selects rows where 'Timestep' is 0, 1, or 2 means only the earliest timesteps are included.\n","# Using .pivot() reshapes the data, setting PatientID as the index and each timestep as a separate column for each feature\n","# Each patient has a single row with columns for each feature across timesteps\n","# Renaming the columns with suffixes like _T0, _T1, etc., ensures each feature-timestep pair is uniquely identifiable, simplifying later analysis\n","# Applying .reset_index() method to data_reshaped to convert PatientID back from an index to a regular column, giving the DataFrame a new integer index\n","# -----------------------------------------\n","training_data = All_Data[All_Data['Timestep'].isin([0, 1, 2])]\n","data_reshaped = training_data.pivot(index='PatientID', columns='Timestep')\n","data_reshaped.columns = [f'{col[0]}_T{col[1]}' for col in data_reshaped.columns]\n","data_reshaped = data_reshaped.reset_index()\n","\n","# -----------------------------------------\n","# Selects only the rows with 'Timestep' is 59 as the final point/step for each patient\n","# Creat a new column 'VL_below_200' to indicate if the viral load (VL) is below 200, with 1 representing below 200 and 0 for above\n","# This binary target variable allows classification (modeling)\n","# ------------------------------------------\n","target_data = All_Data[All_Data['Timestep'] == 59].reset_index(drop=True)\n","target_data['VL_below_200'] = (target_data['VL'] <= 200).astype(int)\n","\n","# -----------------------------------------\n","# 'X_data' is created by dropping the PatientID column from data_reshaped, leaving only the features for the model\n","# 'y_data' is created contains the target variable ('VL_below_200') for classification\n","# This separation ensures X_data is ready as input, while y_data serves as output during model training\n","# -----------------------------------------\n","X_data = data_reshaped.drop(columns=['PatientID'])\n","y_data = target_data['VL_below_200']\n","\n","# -----------------------------------------\n","# StandardScaler() removing the mean and scaling to unit variance to standardizes the features in 'X_data'\n","# This step ensures that features have similar scales which help to improve model convergence and performance\n","# .fit_transform() on scaler performs two operations:\n","#   Firstly, computes the mean and standard deviation of each feature in X_data (fitting)\n","#   Secondly, uses these statistics to standardize X_data by scaling each feature to have a mean of 0 and a standard deviation of 1 (transforming)\n","# -----------------------------------------\n","scaler = StandardScaler()\n","X_data = scaler.fit_transform(X_data)\n","\n","# -----------------------------------------\n","# train_test_split() splits X_data and y_data into training (80%) and testing (20%) sets\n","# random_state=42 achieves reproducibility\n","# stratify=y_data maintains class distribution in both sets\n","# This step ensures balanced representation of target classes in training and testing, which aids generalization\n","# -----------------------------------------\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",")\n","\n","# -----------------------------------------\n","# Converting X_train, y_train, X_test, and y_test to PyTorch tensors for following model training in PyTorch\n","# .view(-1, 1) reshapes y_train and y_test to ensure they have the correct dimensions for the model (target values is column vectors)\n","# -----------------------------------------\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n","\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n"],"metadata":{"id":"w2yFGQbTO0CU","executionInfo":{"status":"ok","timestamp":1731360427560,"user_tz":-660,"elapsed":305,"user":{"displayName":"Peter Zhang","userId":"11053071034181927577"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Check if Timestep == 59 is the final timestep for each patient\n","final_timesteps = All_Data.groupby('PatientID')['Timestep'].max()\n","if (final_timesteps == 59).all():\n","    print(\"Timestep 59 is the final timestep for all patients.\")\n","else:\n","    print(\"Not all patients have Timestep 59 as the final timestep.\")\n","    print(final_timesteps[final_timesteps != 59])  # Display which patients differ\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"174iZyv5VeuD","executionInfo":{"status":"ok","timestamp":1731363995674,"user_tz":-660,"elapsed":2,"user":{"displayName":"Peter Zhang","userId":"11053071034181927577"}},"outputId":"e413d597-d07f-4365-b3cc-798f31983713"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Timestep 59 is the final timestep for all patients.\n"]}]},{"cell_type":"code","source":["set_seed(42)\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","\n","        # -----------------------------------------\n","        # These lines define the layers of the MLP model\n","        #   self.layer1 is the input layer, which takes as input the number of features in X_train_tensor and outputs 64 units\n","        #   self.layer2 is a hidden layer with 64 input units and 32 output units, allowing for complex transformations\n","        #   self.layer3 is the output layer with 1 output unit, which will produce a single value suitable for binary classification\n","        # -----------------------------------------\n","        self.layer1 = nn.Linear(X_train_tensor.shape[1], 64)\n","\n","        self.layer2 = nn.Linear(64, 32)\n","\n","        self.layer3 = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.layer1(x))\n","\n","        x = torch.relu(self.layer2(x))\n","\n","        # -----------------------------------------\n","        # The sigmoid activation function is applied in self.layer3 to the output layer to produce a probability score between 0 and 1\n","        # The values  represents the model’s confidence in predicting the positive class\n","        # When the value close to 1 indicate a high likelihood of the positive class and when the values close to 0 indicate a high likelihood of the negative class\n","        # -----------------------------------------\n","        x = torch.sigmoid(self.layer3(x))\n","\n","        return x\n"],"metadata":{"id":"3l3QdaPbPCoS","executionInfo":{"status":"ok","timestamp":1731360440793,"user_tz":-660,"elapsed":449,"user":{"displayName":"Peter Zhang","userId":"11053071034181927577"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["set_seed(42)\n","\n","model = MLP()\n","\n","criterion = nn.BCELoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 500\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    outputs = model(X_train_tensor)\n","\n","    loss = criterion(outputs, y_train_tensor)\n","\n","    optimizer.zero_grad()\n","\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        test_outputs = model(X_test_tensor)\n","\n","        # -----------------------------------------\n","        # the loss on the test set is calculated using the model's predictions (test_outputs) and the true labels (y_test_tensor)\n","        # This is done within the torch.no_grad() context to prevent gradient calculations\n","        # This saves memory and computation since only evaluate the model, not update its weights\n","        # The result of 'test_loss' can help monitor model performance on unseen data during training\n","        # -----------------------------------------\n","        test_loss = criterion(test_outputs, y_test_tensor)\n","\n","    # -----------------------------------------\n","    # prints the training and test losses every 50 epochs\n","    # providing monitoring on the model's progress during training to identify if the model is learning effectively or it might be overfitting\n","    # when train loss decreases, but test (or validation) loss remains high or starts to increase, indicating the model might be overfitting\n","    # In this case, both the training loss and test loss decrease steadily throughout the training process, which suggests that the model is learning effectively without overfitting, the model could generalizing well to unseen data\n","    # -----------------------------------------\n","    if (epoch+1) % 50 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n","\n","print(\"Training and validation complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0W4HAwJPJUa","executionInfo":{"status":"ok","timestamp":1731360452185,"user_tz":-660,"elapsed":10363,"user":{"displayName":"Peter Zhang","userId":"11053071034181927577"}},"outputId":"aea54b50-e91e-4c52-fd31-85ad4c2e1b11"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [50/500], Train Loss: 0.1450, Test Loss: 0.1433\n","Epoch [100/500], Train Loss: 0.1192, Test Loss: 0.1205\n","Epoch [150/500], Train Loss: 0.1131, Test Loss: 0.1157\n","Epoch [200/500], Train Loss: 0.1097, Test Loss: 0.1126\n","Epoch [250/500], Train Loss: 0.1071, Test Loss: 0.1103\n","Epoch [300/500], Train Loss: 0.1047, Test Loss: 0.1084\n","Epoch [350/500], Train Loss: 0.1026, Test Loss: 0.1069\n","Epoch [400/500], Train Loss: 0.1004, Test Loss: 0.1056\n","Epoch [450/500], Train Loss: 0.0982, Test Loss: 0.1043\n","Epoch [500/500], Train Loss: 0.0960, Test Loss: 0.1030\n","Training and validation complete!\n"]}]},{"cell_type":"markdown","source":["### Outcome Interpretation\n","\n","Our three-layer MLP, utilising a **90% threshold**, achieved an overall accuracy of **93.72%** in predicting whether a patient’s viral load (VL) would remain below 200 at a future time point. The model demonstrated a **Type I error (false positive rate)** of **62.00%**, indicating that **some patients with a VL > 200 were incorrectly classified as having VL ≤ 200**. Conversely, the **Type II error (false negative rate)** was **4.67%**, reflecting the missed predictions for patients with VL ≤ 200. The model yielded an **F1 score** of **0.9672**, underscoring its effectiveness in identifying true positives.\n","\n","Notably, the high threshold contributed to the model accurately identifying **1,653 true positives**; further optimisation may improve its sensitivity.\n","\n","---\n","\n","**IMPORTANT NOTE**:\n","\n","You **DO NOT** need to modify anything in this section. This analysis is provided only to demonstrate what a complete outcome interpretation after modelling might look like. It is intended to give you an understanding of how a machine learning model’s performance can be assessed in practice.\n","\n","For your assignment, please do not alter anything below this point.\n","\n","**DO NOT MODIFY, DO NOT MODIFY, DO NOT MODIFY.**\n"],"metadata":{"id":"SV3My3URPpkc"}},{"cell_type":"code","source":["set_seed(42)\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(X_test_tensor)\n","    predictions = test_outputs >= 0.9\n","    predictions = predictions.numpy().astype(int).flatten()\n","    y_test_np = y_test_tensor.numpy().astype(int).flatten()\n","\n","accuracy = accuracy_score(y_test_np, predictions)\n","tn, fp, fn, tp = confusion_matrix(y_test_np, predictions).ravel()\n","\n","type_1_error = fp / (fp + tn)\n","type_2_error = fn / (fn + tp)\n","\n","f1 = f1_score(y_test_np, predictions)\n","\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n","print(f'Type I Error (False Positive Rate): {type_1_error:.4f}')\n","print(f'Type II Error (False Negative Rate): {type_2_error:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","\n","print(f'Confusion Matrix: \\nTN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceZkpZXQPnLk","executionInfo":{"status":"ok","timestamp":1728862672296,"user_tz":-660,"elapsed":331,"user":{"displayName":"Nic Kuo","userId":"00955284890436520281"}},"outputId":"bf605846-a26e-4379-dafa-fedeed4f7859"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 93.72%\n","Type I Error (False Positive Rate): 0.6200\n","Type II Error (False Negative Rate): 0.0467\n","F1 Score: 0.9672\n","Confusion Matrix: \n","TN: 19, FP: 31, FN: 81, TP: 1653\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gRh-boHrQQfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5T5IzD-tQQhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bg4w4Q2_QQj-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2: Transforming an MLP Setup into an RNN Setup (Total: 10 Marks)\n","\n","This question is worth **10 marks** and must be answered in **exactly 4 paragraphs**. For each paragraph, students are required to address specific aspects of transforming the MLP setup into an RNN (Recurrent Neural Network) setup. Any deviation from the required number of paragraphs will result in **mark deductions**:\n","\n","- **More than 4 paragraphs**: Students will have marks **deducted** for each additional paragraph.\n","- **Less than 4 paragraphs**: Students will have marks **deducted** for each missing paragraph.\n","\n","### Paragraph Requirements and Marks Breakdown:\n","\n","1. **Paragraph 1 (2 Marks)**:  \n","   Students must **philosophically explain** why using an RNN could be better than an MLP for this particular task. The explanation should focus on RNN's ability to handle sequential data and how it may capture temporal dependencies that an MLP may not be able to capture. This paragraph must be concise but insightful.\n","\n","2. **Paragraph 2 (3 Marks)**:  \n","   Students must explain why the data should be presented as **three dimensions** when using an RNN: **batch size, feature size, and the time dimension**. The answer should discuss the importance of organising data in this manner for RNNs to properly process sequences over time, and how it differs from MLPs that work with fixed-size inputs.\n","\n","3. **Paragraph 3 (3 Marks)**:  \n","   Students must provide an explanation of the **memory of an RNN**: how it is **initialised** and how the memory is **updated** during training. This includes describing the hidden state, how it carries information between time steps, and the role it plays in learning temporal patterns in sequential data.\n","\n","4. **Paragraph 4 (2 Marks)**:  \n","   Students must explain how the experimental setup would change if **5 time steps** were used instead of 3. They should discuss how the input data shape would change, how the RNN would handle the additional temporal information, and what changes would be necessary in the model architecture to accommodate more time steps.\n","\n","### Marks Breakdown:\n","\n","- **Paragraph 1**: 2 Marks\n","- **Paragraph 2**: 3 Marks\n","- **Paragraph 3**: 3 Marks\n","- **Paragraph 4**: 2 Marks\n","\n","**Total: 10 Marks**\n","\n","### Important Notice:\n","\n","This is **not an easy question** to answer. Students are expected to approach this task as if writing different subsections for a top-tier academic conference. This includes balancing **conciseness**, **clarity**, and **soundness of methodology**. Each paragraph should be well-structured and clearly address the specific point of the question.\n","\n","Failure to adhere to these instructions, including the paragraph limit, will result in penalties.\n"],"metadata":{"id":"9nvc4_BdQQzD"}},{"cell_type":"markdown","source":["# WRITE HERE\n","**Paragraph 1 (2 Marks):**  \n","RNNs are well-suited for tasks where temporal dependencies play a critical role. Unlike MLPs, which process inputs independently without considering the sequence order, RNNs are designed to retain information from previous time steps, allowing them to learn/capture temporal patterns and relationships/dependencies between consecutive data points. This ability is crucial for medical time-series data (where prior values may influence future outcomes), such as tracking patients' viral load changes/trends over time.\n","\n","---\n","\n","**Paragraph 2 (3 Marks):**  \n","For an RNN to process sequential data effectively, when feeding data into an RNN, the data must be arranged in three dimensions: batch size, time steps, and feature size.  This structure allows the RNN to process multiple sequences simultaneously within each batch, enabling efficient learning. The time dimension allows the RNN to interpret sequences in chronological order. The feature dimension represents the various attributes per timestep (e.g., viral load, CD4 count). Unlike MLPs, which take fixed-size inputs without sequential context, RNNs process each sequence step-by-step, updating the hidden state at each time step to capture evolving information across the sequence. This organization making RNNs preferable in working with time-dependent data, such as predicting future viral load trends based on historical values.\n","\n","---\n","\n","**Paragraph 3 (3 Marks):**  \n","RNNs use a hidden state serves as a form of short-term memory for the sequence to retain and carry information across time steps. At the start of each sequence, the hidden state is initialized (set to zero or a random value). As the model moves through each time step, the hidden state is updated based on the current input and the previous hidden state, effectively “remembering” relevant information from earlier steps. This dynamic/iterative updating allows the RNN to learn/capture temporal dependencies and accumulate information throughout the sequence.\n","\n","---\n","\n","**Paragraph 4 (2 Marks):**  \n","Increasing Time Steps from 3 to 5, the input shape would change to `[batch_size, 5, feature_size]`, means the RNN will now process two additional time steps which will extending the RNN's ability to consider a longer temporal context. This might improve performance on tasks where long-term dependencies are relevant. In addition, this modification may also need adjustments in the model parameters, such as increasing the hidden layer size, to handle the the additional information and added complexity.\n","\n","\n","---\n"],"metadata":{"id":"QI1u2oI1QT70"}}]}