{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBDRH-HDAT9500/Solutions-week01-05/blob/main/Week02-Exercise01-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s6-S_45y4kM"
      },
      "source": [
        "# Week 2: Linear Prediction\n",
        "\n",
        "# Exercise 1: Diabetes Hospitalisations - Data Preparation (data exploration and manipulation)\n",
        "\n",
        "\n",
        "# 1. Introduction: Machine Learning and Data Mining Work-Flow\n",
        "\n",
        "#### Step 1. Research question:\n",
        "\n",
        "We should always have in mind the final goal (what we call the 'question' or step 1 in our machine learning work-flow) of the machine learning problem that we are trying to solve. In real life, we are always interested not just in accurate predictions, but in the use of these predictions as part of a larger decision making process.\n",
        "\n",
        "In this set of exercises, <font color=green> <b> our final goal is to build a predictive algorithm to predict readmission to hospital 30 days after discharge. </b></font>\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=105SGqeyo8RgLhSO8mN7ZE5OsG0YiLPKt)\n",
        "\n",
        "####  Step 2. Data: Do we have the data to answer this research question?\n",
        "In this case, the data set that we have in our hands was directly given to us by a committee that represents the 130 hospitals. We are not 100% sure whether the data will be good enough to answer our question. The committee would like to test precisely that. Can we answer our research question using these data?\n",
        "\n",
        "####  Step 3. Data Gathering:\n",
        "\n",
        "The third step is data gathering. We assume that we have passed ethics commitees, spoken to the different IT teams of the 130 hospitals, and come up with a way to extract, transfer and store the data in a secure environment in order to carry out our research.\n",
        "\n",
        "In terms of variables, we have in mind a set of variables that might be relevant to predict readmission. But as stated above, the hospital only authorized to share with us a specific sets of variables. In other words, this will be the first attempt to solve this problem by testing algorithms and variables. Otherwise, we will have to keep testing to find the correct set of variables and the best algorithm to solve the problem.\n",
        "\n",
        "\n",
        "####  Step 4. Exploratory Data Analysis\n",
        "\n",
        "The fourth step is visualisation and exploration of our data. We did part of this analysis in Chapter 1.\n",
        "<b>In this exercise, we are going to explore with more detail our features and prepare our dataset for future analysis.</b>\n",
        "\n",
        "## 1.1. Data Exploration and Manipulation: Preprocessing\n",
        "\n",
        "We always need some type of data preprocessing. Why?\n",
        "\n",
        "Different algorithms make different assumptions about your data and may require different transformations.\n",
        "Some features might be completely irrelevant for our purpose.\n",
        "Other times, some features are not coded in the right way, or there are many missing values, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6i5lcxGy4kN"
      },
      "source": [
        "<font color=green><b> NB!!: During data cleaning process, the train and test sets are usually kept combined to maintain consistency between them. For example, when encoding a categorical variable, the same level of categories may not be present in both the sets. In such a case, they may be labelled differently if they are encoded separately.\n",
        "    \n",
        "<font color=green><b>    Having said that, we will have to distinguish when we have to carry out data manipulation with both sets combined, and when we have to split the dataset into training + test. For example, the last case will happen when we need to do some kind of imputation for missing variables. Let's say the feature age has some missing values. We might decide to impute these values with the median value of age. But watch out!! we will have to calculate the median age of the training set!!! We will impute these values in the training and test sets! Remember that the \"test\" set is \"the simulated future\". For that reason, no information from the test set should be leaked into the training set.\n",
        "    \n",
        "<font color=green><b>   In this exercise, all the cleaning will be carried out in the whole dataset. We can do that because the operations that we will do here will not leak any information from the \"future\" test set into the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-XQWLH4y4kO"
      },
      "source": [
        "## 1.2. Aims of the Exercise:\n",
        " 1. To manipulate our data to be in the right format for the predictive model that we selected.\n",
        " 2. To select input variables (feature selection, you can read more in Chapter 4 of Book 1).\n",
        " 3. To continue becoming familiar with the diabetes inpatient hospital dataset and the clinical terms contained in it.\n",
        "\n",
        " This exercise aligns with the next course learning outcomes:\n",
        "\n",
        "3. Apply machine learning workflow to health data problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh4zYjSsy4kO"
      },
      "source": [
        "## 1.3. Jupyter Notebook Intructions\n",
        "1. Read the content of each cell.\n",
        "2. Where necessary, follow the instructions that are written in each cell.\n",
        "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
        "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPBaHvZy4kO"
      },
      "source": [
        "## 1.4. Tips\n",
        "1. Run all the cells in sequence (one at a time), using the \"Run\" button.\n",
        "2. To edit this notebook, just double-click in each cell. Choose between \"Code\" cell or text \"Markdown\" cell in the combo-box above.\n",
        "3. If you want to save your notebook, please make sure you press \"the floppy disk\" icon button above.\n",
        "4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear\n",
        "\n",
        "Follow the instructions given and if you have any questions, please use the **Comments section** in **Open Learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7fpYLoqy4kP"
      },
      "source": [
        "# 2. Docstring:\n",
        "\n",
        "Create a docstring with the variables and constants that you will use in this exercise (data dictionary) and the purpose of your program. It is expected that you choose informative variable names and document your program (both docstrings and comments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mubLztrly4kP"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOK3etEqy4kP"
      },
      "source": [
        "# 3. Load the data and select only desired columns.\n",
        "\n",
        "For this chapter, we will be building a <b>logistic regression model to predict readmission</b>. This is a binary classification problem, meaning the target has only two outcomes: 'yes', and 'no'.\n",
        "\n",
        "Now, we will load the hospital dataset and manipulate the features that need some changes.\n",
        "        \n",
        "Some important points to have in mind:<p>\n",
        "    \n",
        " 1. The decision of which algorithm to  use will depend on several factors that we learn during the course. Here we tell you directly to use logistic regression because it is the classification algorithm that we learned in this lesson.<p>\n",
        "    \n",
        " 2. Time of prediction:\n",
        " Since we are predicting readmission at discharge (this is the time that the prediction is made), it is reasonable to assume that we can use all the data available for *all* features. <p>\n",
        " When some data is given to us, we are tempted to assume that all the data is known at all times, and we can use all features for our predictive algorithm. However, this is often not the case. Let me explain this. In our example, the data represents a hospital encounter or hospitalizations. We do not have time stamps of when a medication has been changed, or a lab test has been carried out. But what we know is that at discharge time, all this data is already in the electronic medical record. Therefore, we can use all this data if our time of prediction is discharge.\n",
        " But if our time of prediction had been half way through the hospitalization, we should have been more careful and all the time stamps would have been needed.<p>\n",
        "    \n",
        " As another example, if we wanted to predict length of stay (los), this is a value we would like to predict at the *beginning* of a patients stay. At the beginning of a patients stay, most of the lab tests wouldn't have been peformed, and we wouldn't know values such as number of procedures or surgeries performed, etc. Hence, we couldn't reasonably use all the features of the data set to predict length of stay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZklLpOply4kQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from plotnine import *\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# We do not need to run this cell if you are not running this notebook in Google Colab\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive # import drive from Gogle colab\n",
        "    root = '/content/drive'     # default location for the drive\n",
        "    # print(root)                 # print content of ROOT (Optional)\n",
        "    drive.mount(root)\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "metadata": {
        "id": "wPsHQJtB0Y46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running this notebook in Google Colab, you must define your project paths. In this case, define your `project_path`. Otherwise, all the data will be lost after you close the session."
      ],
      "metadata": {
        "id": "vo7O1sX01Kg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n",
        "    project_path = Path(root) / 'MyDrive' / 'HDAT9500' / 'week02'\n",
        "\n",
        "    # OPTIONAL - set working directory according to your google drive project path\n",
        "    # import os\n",
        "    # Change directory to the location defined in project_path\n",
        "    # os.chdir(project_path)\n",
        "else:\n",
        "    project_path = Path()"
      ],
      "metadata": {
        "id": "Zl77pMjI1MXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw6JF9fby4kQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle_data_path = Path(project_path) /'data'/'diabetes'/'hospital.pickle'\n",
        "with open(pickle_data_path, 'rb') as data:\n",
        "   hospital = pickle.load(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "sJNL51DDy4kR"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "print(hospital.columns)\n",
        "print(hospital.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9j_EuBSy4kS"
      },
      "source": [
        "We will now delete some of the columns that are not needed for our analysis. We could simply leave them in the data frame and not include them when making the models, but that is tedious and it is more convenient to remove them now.<p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQD5EmoBy4kT"
      },
      "source": [
        "# 4. Grouping the features  'discharge_disposition_id',  'admission_source_id' and 'admission_type_id'.\n",
        "\n",
        "As we could see in the data dictionary (data/diabetes/Data_Dictionary.pdf), the majority of discharges are 'Discharge home', followed by 'Discharged/Transferred to SNF' (SNF=Skilled Nursing Facility) and 'Discharge/Transferred to home with home health service'. One option could be to create a new variable, which pools some of these levels together.<p>\n",
        "    For this, we will create a new variable named \"discharge_disposition_grouped\". It will retain the levels 'Discharge home', 'Discharged/Transferred to SNF', and 'Discharge/Transferred to home with home health service'. We will also create a new level named 'Discharged to rehab, another type of inpatient care institution or short term hospital', which will include all observations from 'Discharge/Transferred to another rehab fac including rehab units of a hospital', 'Discharge/Tranferred to another type of inpatient care institution', and 'Discharge/transferred to another short term hospital'. The rest of the data will be grouped into a new level named 'Other'.\n",
        "\n",
        "**Method**<br/>\n",
        "First, each of the id features are duplicated and stored in the dataset under the same name but ending with *grouped* rather than *id*. Then, we create a dictionary object containing the desired mappings from numeric ids to word description. Finally, we use the *.map()* function to map the ids to their descriptions and convert to string data type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qSlrC-Cy4kT"
      },
      "source": [
        "**Duplicating the columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jgys0UFy4kT"
      },
      "outputs": [],
      "source": [
        "# Duplicating each of the desired columns\n",
        "hospital['discharge_disposition_grouped'] = hospital['discharge_disposition_id_cat']\n",
        "hospital['admission_source_grouped'] = hospital['admission_source_id_cat']\n",
        "hospital['admission_type_grouped'] = hospital['admission_type_id_cat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN8Mao87y4kU"
      },
      "source": [
        "********************************************************************************************************************************************************\n",
        "**admission_type_id\" (Check data dictionary - Appendix A: data/diabetes/Data_Dictionary.pdf)**\n",
        "\n",
        "**Counts and visualization for variable \"admission_type_id\" :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2UCUR2vy4kU"
      },
      "outputs": [],
      "source": [
        "print(hospital.admission_type_id_cat.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlH6ravxy4kU"
      },
      "outputs": [],
      "source": [
        "# Source: https://seaborn.pydata.org/generated/seaborn.countplot.html\n",
        "ax = sns.countplot(x=\"admission_type_id_cat\", hue=\"readmission\", data=hospital)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwZgVw6Uy4kU"
      },
      "source": [
        "**Using dictionary mapping for 'admission_type_grouped' (Source: data/diabetes/Data_Dictionary.pdf):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M6MV5Zzy4kU"
      },
      "outputs": [],
      "source": [
        "# Mapping admission_type_id to admission_type_grouped\n",
        "dict_map = ({1:'Emergency' ,\n",
        "             2:'Urgent',\n",
        "             3:'Elective',\n",
        "             4:'Not Available/Null',\n",
        "             5:'Not Available/Null',\n",
        "             6:'Trauma Centre',\n",
        "             7:'Not Available/Null'})\n",
        "hospital['admission_type_grouped'] = hospital['admission_type_grouped'].map(dict_map)\n",
        "hospital['admission_type_grouped'] = hospital['admission_type_grouped'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8GSouYYy4kV"
      },
      "outputs": [],
      "source": [
        "#Sanity Check\n",
        "hospital['admission_type_grouped'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBwSGrB9y4kV"
      },
      "source": [
        "********************************************************************************************************************************************************\n",
        "**discharge_disposition_id\" (Check data dictionary - Appendix B: data/diabetes/Data_Dictionary.pdf)**\n",
        "\n",
        "**Counts and visualization for variable \"discharge_disposition_id\":**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC1xBhv1y4kV"
      },
      "outputs": [],
      "source": [
        "print(hospital.discharge_disposition_id_cat.value_counts(sort=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jegVWzKPy4kV"
      },
      "outputs": [],
      "source": [
        "# Source: https://seaborn.pydata.org/generated/seaborn.countplot.html\n",
        "ax = sns.countplot(x=\"discharge_disposition_id_cat\", hue=\"readmission\", data=hospital)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6h4wWX3y4kV"
      },
      "source": [
        "**Using dictionary mapping for 'discharge_disposition_grouped' (Source: data/diabetes/Data_Dictionary.pdf):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IWHN7TZy4kW"
      },
      "outputs": [],
      "source": [
        "# Mapping admission_source_id to discharge_disposition_grouped\n",
        "dict_map = ({1: 'Discharged to home',\n",
        "             2: 'Short term hospital',\n",
        "             3: 'Transferred to SNF',\n",
        "             4: 'Transferred to ICF',\n",
        "             5: 'Short term hospital',\n",
        "             6: 'Home health service',\n",
        "             7: 'Left AMA',\n",
        "             8: 'Home health service',\n",
        "             9: 'Other',\n",
        "             10: 'Other',\n",
        "             12: 'Other',\n",
        "             15: 'Other',\n",
        "             16: 'Other',\n",
        "             17: 'Other',\n",
        "             18: 'Not available/Null',\n",
        "             22: 'Short term hospital',\n",
        "             23: 'Other',\n",
        "             24: 'Other',\n",
        "             25: 'Not available/Null',\n",
        "             27: 'Other',\n",
        "             28: 'Other'\n",
        "             })\n",
        "hospital['discharge_disposition_grouped'] = hospital['discharge_disposition_grouped'].map(dict_map)\n",
        "hospital['discharge_disposition_grouped'] = hospital['discharge_disposition_grouped'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqMin18wy4kW"
      },
      "outputs": [],
      "source": [
        "#Sanity Check\n",
        "hospital['discharge_disposition_grouped'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkUKzWMKy4kW"
      },
      "source": [
        "********************************************************************************************************************************************************\n",
        "**admission_source_id\" (Check data dictionary - Appendix C: data/diabetes/Data_Dictionary.pdf)**\n",
        "\n",
        "**Counts and visualization for variable \"admission_source_id\" (Check data dictionary: data/diabetes/Data_Dictionary.pdf):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0_Sg53dy4kW"
      },
      "outputs": [],
      "source": [
        "print(hospital.admission_source_id_cat.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3Ll1sOGy4kW"
      },
      "outputs": [],
      "source": [
        "# Source: https://seaborn.pydata.org/generated/seaborn.countplot.html\n",
        "ax = sns.countplot(x=\"admission_source_id_cat\", hue=\"readmission\", data=hospital)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUb2s1CSy4kW"
      },
      "source": [
        "**Using dictionary mapping for 'admission_source_grouped' (Source: data/diabetes/Data_Dictionary.pdf):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGjbZDizy4kW"
      },
      "outputs": [],
      "source": [
        "dict_map = ({1: 'Physician Referral',\n",
        "             2: 'Clinic Referral',\n",
        "             4: 'Transfer from another health care facility',\n",
        "             5: 'Transfer from SNF',\n",
        "             6: 'Transfer from another health care facility',\n",
        "             7: 'Emergency Room',\n",
        "             9: 'Not available/Null',\n",
        "             11: 'Not available/Null',\n",
        "             12: 'Not available/Null',\n",
        "             3: 'Other',\n",
        "             8: 'Other',\n",
        "             10: 'Other',\n",
        "             13: 'Other',\n",
        "             14: 'Other'})\n",
        "hospital['admission_source_grouped'] = hospital['admission_source_grouped'].map(dict_map)\n",
        "hospital['admission_source_grouped'] = hospital['admission_source_grouped'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbUncIgyy4kW"
      },
      "outputs": [],
      "source": [
        "#Sanity Check\n",
        "hospital['admission_source_grouped'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKlmpMEHy4kX"
      },
      "source": [
        "**Delete 'id' features for discharge disposition, admission source and admission type.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCFgZWToy4kX"
      },
      "outputs": [],
      "source": [
        "# Drop continuous variables for grouped variables\n",
        "hospital = hospital.drop(['discharge_disposition_id_cat','admission_source_id_cat','admission_type_id_cat'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF9I5iULy4kX"
      },
      "outputs": [],
      "source": [
        "# Check\n",
        "hospital.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LpQavovy4kX"
      },
      "source": [
        "# 5. Categorical data - Dummy variables\n",
        "\n",
        "Linear models, as with many learning methods covered in this course, require strictly numeric inputs in order to <b>use scikit-learn in Python</b>.  We will need to convert our categorical features into numerics. To do this, we will use the concept of dummy variables methods covered in chapter 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6erjg-iy4kX"
      },
      "source": [
        "## 5.1 Checking the levels of categorical variables with missing information.\n",
        "We should check that each categorical column contains only the expected values, and no mispellings have occured. This is an important step when dealing with categorical variables, as it can remove many unexpected errors and results during analysis.\n",
        "\n",
        "In Chapter 1, we were able to visualise sections of missing data ('?') in the variables 'weight', 'payer_code' and 'medical_specialty'. In addition, we identified missing values regarding 'sex'. Therefore, we will define whether we should keep or delete the missing values by analysing the number of records in these categories.\n",
        "\n",
        "**Lets look at the sex, weight, payer_code and medical_specialty variables.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84EATGK9y4kX"
      },
      "outputs": [],
      "source": [
        "print(hospital.sex.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YJAHo74Xy4kY"
      },
      "outputs": [],
      "source": [
        "print(hospital.payer_code.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2GxwjWlJy4kY"
      },
      "outputs": [],
      "source": [
        "print(hospital.medical_specialty.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xce_oX6Oy4kY"
      },
      "source": [
        "**Decisions**\n",
        "1. We notice that the Unknown/Invalid category from 'sex' variable has only 3 records. This is a very small sample size in comparison with the total number of records (~70,000). Thus, any pattern in readmission discovered based on the sex being Unknown/Invalid would most likely just be down to chance. Hence, we decide to delete these records.\n",
        "2. Most records have missing values for 'weight' ('?'=66,518). Therefore, we will remove this variable from our dataset.\n",
        "3. Most records have missing values for 'payer_code' ('?'=29,922). Therefore, we will remove this variable from our dataset.\n",
        "4. Most records have missing values for 'medical_specialty' ('?'=33,432). Therefore, we will remove this variable from our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOcI442gy4kY"
      },
      "outputs": [],
      "source": [
        "# Deleting category Unknown/Invalid from sex variable\n",
        "hospital = hospital[hospital.sex != 'Unknown/Invalid']\n",
        "#Sanity Check\n",
        "print(hospital.sex.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1htwBcFy4kY"
      },
      "outputs": [],
      "source": [
        "# Drop weight, payer_code and medical_specialty\n",
        "hospital = hospital.drop(['payer_code','medical_specialty'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRUps3lIy4kY"
      },
      "source": [
        "## 5.2 Creating the dummy variables\n",
        "We already transformed our continuous variables that were categorical in the correct format ('admission_type_grouped', 'discharge_disposition_grouped', and 'admission_source_grouped'). Now that we have the data in an appropriate form, we are ready to create our dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPuWnTCDy4kY"
      },
      "outputs": [],
      "source": [
        "# Checking the nature of the new grouped variables variables\n",
        "hospital.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n72NDTycy4kZ"
      },
      "source": [
        "We **do not** want to convert the response variable into a dummy variable. Hence, we will extract 'readmission', perform the get_dummies transformation and save the dummy variables in hospital data frame, and then add readmission back into the hospital data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSIvs-UXy4kZ"
      },
      "outputs": [],
      "source": [
        "#  We save our output (y)\n",
        "readmission = hospital['readmission']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab4WTVz4y4kZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em-KuV8by4kZ"
      },
      "source": [
        "### <font color='blue'>Question 1: Write below the code to create dummy variables </font>\n",
        "<p><font color='green'>Tip: Remember, we do not want to convert the response, 'readmission', to a dummy variable. You may want to use pandas.DataFrame.drop('column name', axis = 1), to choose all but the specified columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXglfCSSy4kZ"
      },
      "outputs": [],
      "source": [
        "#Solution\n",
        "hospital = pd.get_dummies(hospital.drop('readmission', axis = 1)) # Dont want the response to be \"dummified\".\n",
        "hospital['readmission'] = readmission # Add readmission back into hospital"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVebcbTyy4kZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi7iPuT0y4kZ"
      },
      "outputs": [],
      "source": [
        "print(\"Features after get_dummies:\\n\", list(hospital.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfGEmNvly4kZ"
      },
      "outputs": [],
      "source": [
        "hospital.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "32rdutwny4ka"
      },
      "outputs": [],
      "source": [
        "hospital.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_uuNatKy4ka"
      },
      "source": [
        "## 5.3 Saving our new cleaned dataset with dummy variables using 'pickle'.\n",
        "\n",
        "For this we will use 'pickle'. Pickle is used to store python objects (lists, dictionaries, dataframes) in a file that we can call or load after. In our case, we will store our dataset in pickle and load it in the following exercises of this an other Chapters.\n",
        "\n",
        "First, we will open a file that we will call 'hospital_data.pickle'. Then, we will use pickle.dump() to put the dataset into the opened file, then close. More information: https://docs.python.org/3/library/pickle.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWQBCYtry4ka"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('hospital_data.pickle', 'wb') as output:\n",
        "    pickle.dump(hospital, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGXqbXj-y4ka"
      },
      "source": [
        "**Note:** If we want to load our dataset back, we can use the following script. We will use this code in the following exercise (no need to use it in this exercise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9ThHvHFy4kb"
      },
      "outputs": [],
      "source": [
        "#import pickle\n",
        "\n",
        "#with open('hospital_data.pickle', 'rb') as data:\n",
        "#    hospital = pickle.load(data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}