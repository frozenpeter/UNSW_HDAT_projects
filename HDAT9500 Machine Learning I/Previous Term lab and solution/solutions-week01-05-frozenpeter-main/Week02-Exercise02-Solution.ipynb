{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBDRH-HDAT9500/Solutions-week01-05/blob/main/Week02-Exercise02-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zon1vOee5snE"
      },
      "source": [
        "![alt text](https://github.com/CBDRH-HDAT9500/Solutions-week01-05/blob/main/images/HDAT9500Banner.PNG?raw=1)\n",
        "<br>\n",
        "Â© Copyright The University of New South Wales - CRICOS 00098G\n",
        "\n",
        "\n",
        "# Week 2: Linear Prediction\n",
        "# Exercise: Diabetes Hospitalisations - Logistic Regression with L1-norm and L2-norm Regularization (Lasso and Ridge)\n",
        "\n",
        "\n",
        "# 1. Introduction\n",
        "Following the visualisation and manipulation stages (data mining and machine learning work-flow), we are now going to fit/build a predictive model.\n",
        "http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
        "\n",
        "<b>Goal/Research question:</b> <font color=green> <b> our final goal is to build a predictive algorithm to predict readmission to hospital 30 days after discharge. </b></font>\n",
        "\n",
        "\n",
        "## 1.1. Regression with L1-norm Regularization (Lasso) and L2-norm Regularization (Ridge)\n",
        "\n",
        "In this exercise, we are going to fit a logistic regression to our data set, by using two techniques that constrains (or regularizes) the coefficient estimates with a L1-norm (Lasso regularization) and a L2-norm regularization (Ridge regularization).\n",
        "\n",
        "1. The Lasso regularization will shrink the coefficient estimates towards zero or <b>directly to zero</b>. For this last reason, Lasso regularization <font color=green><b>could also be</b></font> a technique for <font color=green><b>feature selection </b></font>(those features that are not zero in the model).\n",
        "\n",
        "2. The Ridge regularization will shrink the coefficient estimates **towards zero**.\n",
        "\n",
        "The advantage of regularization versus plain least squares has to do with the bias-variance trade off. For more information, read Book 2, pages 217 and 218 \"An Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n",
        "\n",
        "\n",
        "## 1.2. Aims:\n",
        " 1. To build a predictive model <b>using logistic regression with L1-norm and L2-norm regularization of the coefficient estimates</b> (Lasso and Ridge regularization).\n",
        " 2. To choose input variables.\n",
        " 3. To manipulate features: standardization.\n",
        " 4. To use training and test sets.\n",
        " 5. To continue becoming familiar with the diabetes inpatient hospital dataset and the clinical terms contained in it.\n",
        "\n",
        "It aligns with all the learning outcomes of our course:\n",
        "\n",
        "\n",
        "1. Distinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
        "2. Design machine learning tasks for Health Data Science scenarios.\n",
        "3. Apply machine learning workflow to health data problems.\n",
        "4. Generate knowledge via the application of machine learning techniques to health data.\n",
        "\n",
        "\n",
        "## 1.3. Jupyter Notebook Intructions\n",
        "1. Read the content of each cell.\n",
        "2. Where necessary, follow the instructions that are written in each cell.\n",
        "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
        "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
        "\n",
        "## 1.4. Tips\n",
        "1. Run all the cells in sequence (one at a time), using the \"Run\" button.\n",
        "2. To edit this notebook, just double-click in each cell. Choose between \"Code\" cell or text \"Markdown\" cell in the combo-box above.\n",
        "3. If you want to save your notebook, please make sure you press \"the floppy disk\" icon button above.\n",
        "4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear\n",
        "\n",
        "Follow the instructions given and if you have any questions, please use the **Comments section** in **Open Learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTc9dHNf5snH"
      },
      "source": [
        "# 2. Docstring:\n",
        "\n",
        "Create a docstring with the variables and constants that you will use in this exercise (data dictionary) and the purpose of your program. It is expected that you choose informative variable names and document your program (both docstrings and comments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-PrX7lP5snI"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBfGLC-Q5snI"
      },
      "source": [
        "# 3. Load the dataset stored in pickle (from Exercise 1)\n",
        "\n",
        "**Let's load the dataset that we prepared in the previous exercise of this Chapter.** Please note that we stored the dataset using pickle. Now, we will load this dataset, using 'pickle' (more information: https://docs.python.org/3/library/pickle.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyRL6bgs5snI"
      },
      "outputs": [],
      "source": [
        "# check required libraries are installed if not calling system to install\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "required = {'numpy', 'pandas', 'plotnine', 'matplotlib', 'seaborn',\n",
        "            'grid', 'lime', 'shap', 'scikit-learn'}\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    print('Installing: ', missing)\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "# delete unwanted variables\n",
        "del required\n",
        "del installed\n",
        "del missing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# We do not need to run this cell if you are not running this notebook in Google Colab\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive # import drive from Gogle colab\n",
        "    root = '/content/drive'     # default location for the drive\n",
        "    # print(root)                 # print content of ROOT (Optional)\n",
        "    drive.mount(root)\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "metadata": {
        "id": "8h4Pgntn6H-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running this notebook in Google Colab, you must define your project paths. In this case, define your `project_path`. Otherwise, all the data will be lost after you close the session."
      ],
      "metadata": {
        "id": "deYorW7v6RQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n",
        "    project_path = Path(root) / 'MyDrive' / 'HDAT9500' / 'week02'\n",
        "\n",
        "    # OPTIONAL - set working directory according to your google drive project path\n",
        "    # import os\n",
        "    # Change directory to the location defined in project_path\n",
        "    # os.chdir(project_path)\n",
        "else:\n",
        "    project_path = Path()"
      ],
      "metadata": {
        "id": "1MYZxGFw6SvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z1h8e435snJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle_path = Path(project_path) /'hospital_data.pickle'\n",
        "\n",
        "# Load dataset stored in pickle in Exercise 1\n",
        "with open(pickle_path, 'rb') as data:\n",
        "    hospital = pickle.load(data)\n",
        "\n",
        "hospital.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhvC07-e5snK"
      },
      "outputs": [],
      "source": [
        "hospital.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4M5KtcD5snK"
      },
      "outputs": [],
      "source": [
        "test=hospital"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mjQIFYZ5snK"
      },
      "source": [
        "# 4. Feature scaling: standardizing the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYZ2HqPd5snK"
      },
      "source": [
        "For some of the methods covered in this chapter, specifically ridge and lasso regression, it is best to standardize the features (page 217, Book 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQIXzD6B5snK"
      },
      "source": [
        " **To standardize a feature**, subtract the sample mean, $\\bar x$, and then divide by its estimated standard deviation, $s$. This process results in the standardized features all having a standard deviation of 1 and a mean of 0. This makes each feature to have the same shape and spread, so that no single feature dominates the regression based purely on its scaling and distribution.<p>\n",
        "\n",
        "If you want to read a bit more about this issue, I leave some links here:\n",
        "\n",
        "1. Read page 217, Book 2 (An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani). PDF Free to download from: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf\n",
        "\n",
        "    This the mathematical formula for standardizing features as per Book 2:\n",
        "    \n",
        " ![alt text](https://github.com/CBDRH-HDAT9500/Solutions-week01-05/blob/main/images/Standardization.PNG?raw=1)  \n",
        "    \n",
        "    \n",
        "2. Towards Data Science: https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
        "3. One of the forums in which the ML community is very active: https://stats.stackexchange.com/questions/290958/logistic-regression-and-scaling-of-features\n",
        "\n",
        "        \n",
        "Let's standardize our features:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixa1jaAJ5snL"
      },
      "source": [
        "**Interpretation of beta coefficients**:\n",
        "\n",
        "Keep in mind that the interpretation of regression coefficients becomes more complicated after the features are standardized. For instance, consider multiple linear regression. Prior to standardization, a unit change in $X_j$ corresponds to a change of $\\hat \\beta_j$ to the predicted response. Simple. However, after standardization of $X_j$, the interpretation of the coefficients becomes: for every increase of <b>one standard deviation</b> in $X_j$, the predicted response changes by $\\hat \\beta_j$ standard deviations. Still manageable, but not as straight forward.<p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlB6Wj3f5snL"
      },
      "source": [
        "First, we split the hospital data set into two DataFrames: the features, stored in X, and the target, stored in y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbp5U-ks5snL"
      },
      "outputs": [],
      "source": [
        "X = hospital.drop(['readmission'], axis = 1)\n",
        "y = hospital[['readmission']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1PJlXmv5snL"
      },
      "source": [
        "Now let's split the data into a training and test set. We will include the optional argument 'stratify = y' to preserve the ratio between readmission = NO to readmission = YES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtcpnhUZ5snM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPRz1Zan5snM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.25,random_state=0)\n",
        "# stratify = y means we wish to preserve the ratio of y in the test and training sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-f1IYM85snM"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "\n",
        "# Check that proportions are maintained:\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "y_test_df.columns = ['readmission']\n",
        "y_train_df = pd.DataFrame(y_train)\n",
        "y_train_df.columns = ['readmission']\n",
        "\n",
        "print('*********************************************************************************')\n",
        "print(f\"\"\"Number of records in original data frame (hospital): {len(y)},\n",
        "Proportion of readmission in original data frame (hospital):\\n {hospital.readmission.value_counts('NO')}\\n\n",
        "********************************************************************************* \\n\n",
        "Number of records in test set: {len(y_test)},\n",
        "Proportion of readmission in test set:\\n {y_test_df.readmission.value_counts('NO')}\\n\n",
        "********************************************************************************* \\n\n",
        "Number of records in training set: {len(y_train)},\n",
        "Proportion of readmission in train set:\\n {y_train_df.readmission.value_counts('NO')}\n",
        "\"\"\")\n",
        "print('*********************************************************************************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ApnZ7tE5snN"
      },
      "source": [
        "We can see that the training and test set have maintained the proportion of NO:YES response from the original data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOQZapga5snN"
      },
      "source": [
        "**Returning to the process of standardization. Now, we need to make a choice as to whether to standardize the dummy variables, or not.**<p>  \n",
        "    According to Robert Tibrishani, co-author of \"The Elements of Statistical Learning\" (Book 5 of the recommended books in the outline, free to download), you should standardize the dummies: \"*The lasso* (and ridge) *method requires initial standardization of the regressors, so that the penalization scheme is fair to all regressors. For categorical regressors, one codes the regressor with dummy variables and then <b> standardizes the dummy variables </b>*\".<p>\n",
        "        \n",
        "If you want to read a bit more about why we should standarize our variables, I leave you here a couple of links:\n",
        "\n",
        "1. Paper written by Robert Tibshirani of Stanford University, one of the creators of Ridge and Lasso (click here and look for the paragraph I wrote above: http://statweb.stanford.edu/~tibs/lasso/fulltext.pdf)\n",
        "2. A forum discussion: https://stats.stackexchange.com/questions/69568/whether-to-rescale-indicator-binary-dummy-predictors-for-lasso/120600    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttaQ6QKO5snN"
      },
      "source": [
        "Now let's define the scaler we will use, which we decide is *StandardScaler*. *StandardScaler* performs the standardization described at the beginning of this section. We can also easily replace one preprocessing algorithm with another by simply changing the scaler definition at this step. For instance, instead of using *StandardScaler*, we could have used *MinMaxScaler*, or some other alternative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIXBJvf55snN"
      },
      "source": [
        "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2m4XBRo5snO"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ZqeMAh5snO"
      },
      "source": [
        "### <font color='green'> <b>Very very very very important!:</b></font>\n",
        "\n",
        "<font color='green'>Now we fit the scaler to the training data. It is important that we fit the scaler to **only the training set** and then apply that same scaler to the test set (remember that the test set is never seen until the very end, once our model has been fully trained). The purpose of having a test set is to mimic the situation where your model is making prediction decisions in the real world, when you do not have access to the true response. Therefore, we cannot use the test set for *anything* except comparing to predicted values. This means we cannot use the test set to fit the scaler.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POi4r1Zb5snO"
      },
      "source": [
        "Standardize features means to subtract the mean and divide by the standard deviation.\n",
        "In this step, we calculate the actual means and variances for each feature in  <font color='red'> THE TRAIN SET.</font>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEsM1VLR5snO"
      },
      "outputs": [],
      "source": [
        "# Standardize features by removing the mean and scaling to unit variance.\n",
        "# In this step, we calculate the actual means and variances for each feature in  THE TRAIN SET.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "scaler.fit(X_train)\n",
        "# mu-training-set = mean of the age in the training set\n",
        "# sigma-training-set = standard deviation of the age in the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezL4dNS5snP"
      },
      "source": [
        "To actually scale the data, use the transform method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkeMNLsw5snP"
      },
      "outputs": [],
      "source": [
        "# rescale the training data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "# (X_train - mu-training-set )/sigma-training-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r--8UWL5snP"
      },
      "outputs": [],
      "source": [
        "# Check\n",
        "X_train_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGjWnvJm5snP"
      },
      "source": [
        "To apply predictive models to the scaled data, we also need to transform the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu0GuyRg5snP"
      },
      "outputs": [],
      "source": [
        "# scale the test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# (X_test - mu-training-set )/sigma-training-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f1J_ug45snQ"
      },
      "outputs": [],
      "source": [
        "# Check\n",
        "X_test_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOYfKeEk5snQ"
      },
      "source": [
        "# 5. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-2Uzqnb5snR"
      },
      "source": [
        "Logistic regression is used for *binary classification*, which means the response, $y$,  is a categorical variable with 2 levels. This is precisely the situation for modelling <b>readmission</b>, which has levels YES and NO. Now, rather than modelling the response variable directly, logistic regression involves modelling the probability that the response variable belongs to a particular category. Logistic regression estimates the probability by the **logistic function**:<p>\n",
        "\n",
        "\\begin{align*}\n",
        " P(Y=1 | X) \\ &= \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1+e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}} \\\\\n",
        "            \\ &= \\frac{1}{1+e^{-[\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p]}}\n",
        "\\end{align*}\n",
        "\n",
        "    \n",
        "\n",
        "Now, the reason we model the probability via the logistic equation is to 'squash' the output of the model to the range (0,1). This is required as probabilities must be between 0 and 1 (there cannot be a negative chance of occurrence and there cannot be a greater than certain chance of occurrence).<p>\n",
        "   \n",
        "## Logit Function and Log Odds\n",
        "    \n",
        "        \n",
        "We can also rearrange the logistic function into what is known as the **logit function**:<p>\n",
        "    \\begin{equation*}\n",
        "log\\left(\\frac{P(y=YES)}{1-P(y=YES)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n",
        "\\end{equation*}<p>\n",
        "    Now, the quantity $\\frac{P(y=YES)}{1-P(y=YES)}$ has a special interpretation. It is the *odds ratio* of an event occuring, and can take on any value between $0$ and $\\infty$. Notice that $1-P(y=YES)$ is the probability that y is *not* YES, so the odds ratio is the probability that $y = YES$ over the probability that $y\\ne YES$. This shows that logistic regression essentially involves modelling the *log odds* of an event via a linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoynViXt5snR"
      },
      "source": [
        "## Likelihood Function\n",
        "\n",
        "Remember that we encode YES as 1, and NO as 0. <p>\n",
        "    Now we will go a little bit further. The way in which the model is fitted is more complicated than linear regression. For logistic regression, we choose the beta coefficients by the method of **maximum likelihood**. Intuitively, we try to find values for the beta coefficients such that the probability is close to 1 when the true value is YES, and close to 0 when the true value is NO. We can express this mathematically as trying to *maximise* the **likelihood function**:<p>\n",
        "      \n",
        "\n",
        "    \n",
        "\\begin{align*}\n",
        " L(\\beta|X) \\ &= \\prod_{i: y_i = 1}{P(y_i = 1)}\\prod_{i': y_i' = 0}{P(y_i=0)} \\\\\n",
        "            \\ &= \\prod_{i: y_i = 1}{P(y_i = 1)}\\prod_{i': y_i' = 0}{[1 - P(y_i = 1)]}\n",
        "\\end{align*}\n",
        "\n",
        "    \n",
        "Don't be intimidated by this. Let's unpack: the $\\prod$ sign just means 'multiply everything together', similar to how the familiar $\\sum$ operates. Below the first $\\prod$, we see '$i: y_i = 1$', which means 'do the multiplying for every i such that $y_i = 1$'. Similarly, below the second $\\prod$ we see '$i: y_i = 0$', which means 'do the multiplying for every i such that $y_i = 0$'. Finally, $P(y_i = 1)$ means 'the probability that $y_i = YES$.<p>\n",
        "        \n",
        "The details are not so important, so don't worry too much if you are not familiar with this notation. To maximise the likelihood, we use the convenient property that maximising the likelihood is equivalent to maximising the logarithm of the likelihood, which converts the above products into sums. This is helpful as sums are easier to maximise via differentiation methods. In addition, it is convenient to code the two-class via a 0/1 response $y_i$.\n",
        "The **log-likelihood** can be written:<p>\n",
        "\n",
        "\\begin{align*}\n",
        " \\textit {l} (\\beta|X)) \\ &= log(L(\\beta|X)) \\\\\n",
        " \\ &= \\sum_{i: y_i = 1}{log(P(y_i = 1))} + \\sum_{i: y_i = 0}{log(1-P(y_i=1))} \\\\\n",
        "            \\ &= \\sum_{i}^{N} {y_i log(P(y_i = 1))} + \\sum_{i}^N{(1-y_i)log(1-P(y_i=1))} \\\\\n",
        "            \\ &= \\sum_{i}^{N} {\\left[y_i log(P(y_i = 1))+ (1-y_i)log(1-P(y_i=1))\\right]} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Recall that $P(y=1) = \\frac{1}{1+e^{-[\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p]}}$. Then the log-likelihood function becomes:<p>\n",
        "\n",
        "\\begin{align*}\n",
        "\\textit {l} (\\beta|X)) \\ &=  \\sum_{i}^{N} {\\left[y_i log(\\frac{1}{1+e^{-[\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p]}})+ (1-y_i)log(1-\\frac{1}{1+e^{-[\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p]}})\\right]} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Now, we simply maximize this log-likehood expression to find the betas. How do we do that? Very simple, set the first derivative equal to zero. We will leave the full explanation for a more advanced machine learning course, but it is good to know how this *maximum likelihood principle* works because many machine learning algorithms use this principle.<p>\n",
        "    So, in summary, regular logistic regression chooses beta coefficients by maximising the log-likelihood function, $l(\\beta)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIG2jEe5snS"
      },
      "source": [
        "## L2 Regularization: Ridge (page 215, Book 2)\n",
        "\n",
        "Now, L2 regularization, also known as Ridge Regression, shrinks the beta coefficients by imposing a penalty on their size. This is achieved by adding a penalty term to the likelihood function maximised by regular logistic regression. The strength of the penalty is commonly denoted as $\\lambda$.\n",
        "\n",
        " ![alt text](https://github.com/CBDRH-HDAT9500/Solutions-week01-05/blob/main/images/Ridge.PNG?raw=1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG3FX6_g5snS"
      },
      "source": [
        "## L1 Regularization: Lasso (page 219, Book 2)\n",
        "Similarly to L2 regularization, lasso regression shrinks the beta coefficients by adding a penalty term to the likelihood function optimised by regular logistic regression. The difference between the two regularizations is the form of this penalty term. For Ridge, we have penalty term as the beta coefficients *squared*. For Lasso, we have the *absolute value*. Again, the strength of the penalty is denoted as $\\lambda$.\n",
        "\n",
        " ![alt text](https://github.com/CBDRH-HDAT9500/Solutions-week01-05/blob/main/images/Lasso.PNG?raw=1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkO4b_l35snT"
      },
      "source": [
        "## 5.1 Training a logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fhk-1E55snT"
      },
      "source": [
        "Now we can train our logistic regression model using ridge regression. The default of logistic regression in <b>scikit-learn</b>  is to include a ridge penalty term in the linear component. The parameter controlling the regularization strength, <b>$alpha$</b>, or $C$ in our Python library. This parameter is also known as $\\lambda$, and its relationship with $alpha$ is $(alpha =  C = \\frac {1}{\\lambda})$. <p>\n",
        "    Small values of $C$ indicate strong regularization, whilst larger values of $C$ indicate weaker regularization. In fact, if we set $C$ to be an extremely large number, then $\\lambda$ is effectively zero and the regression reduces to <b> regular logistic regression</b> ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egu3tOPf5snT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soNGfWbC5snT"
      },
      "source": [
        "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVl2hbIx5snU"
      },
      "source": [
        "![alt text](https://github.com/CBDRH-HDAT9500/Solutions-week01-05/blob/main/images/LRsklearn.PNG?raw=1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A_ol7yX5snU"
      },
      "source": [
        "Let's use the 'liblinear' solver that is valid for L1 and L2 regularizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3PvWT_d5snV"
      },
      "source": [
        "### Training a logistic regression model using L1-norm regularization (Lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15DFuKLf5snV"
      },
      "outputs": [],
      "source": [
        "# C=0.01 as a first step\n",
        "Log_Reg_L1 = LogisticRegression(C = 0.01 , penalty = 'l1', solver='liblinear').fit(X_train_scaled, y_train.ravel())\n",
        "# ravel() used to convert y from column vector to 1d array, as required by the method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xDQ2w15snW"
      },
      "source": [
        "### Training a logistic regression model using L2-norm regularization (Ridge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTSwjt2v5snW"
      },
      "outputs": [],
      "source": [
        "# C=0.01 as a first step\n",
        "Log_Reg_L2 = LogisticRegression(C = 0.01 , penalty = 'l2',solver='liblinear').fit(X_train_scaled, y_train.ravel())\n",
        "# ravel() used to convert y from column vector to 1d array, as required by the method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OolYBpG05snW"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 2**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fqx_eKj5snW"
      },
      "source": [
        "### <font color='blue'> Question 1: Print the beta coefficients of both logistic regression models and comment on your first impression. Change the C hyperparameter several times and print the beta coefficients again to see how they change.</font>\n",
        "\n",
        "<p><font color='green'> Tip:  You can use the following structure: \"ModelName.coef_\" to print the beta coefficients only.</font></p>\n",
        "<p><font color='green'> You can use the following structure: \"coefficients = pd.concat([pd.DataFrame(X.columns,columns=['Features']), pd.DataFrame(np.transpose(ModelName.coef_),columns=['Coefficients'])], axis = 1)\" to print the beta coefficients of each variable from the diabetes dataset.</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97c5_OTT5snW"
      },
      "source": [
        "# L1 Norm: Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "veMquWoZ5snW"
      },
      "outputs": [],
      "source": [
        "# Write Python code here:\n",
        "\n",
        "#Log_Reg_L1.coef_                  #or\n",
        "\n",
        "coefficients_Log_Reg_L1 = pd.concat([pd.DataFrame(X.columns,columns=['Features']),\n",
        "                                     pd.DataFrame(np.transpose(Log_Reg_L1.coef_),columns=['Coefficients'])], axis = 1)\n",
        "\n",
        "# Sorting new DataFrame by Coefficients (Sort Descending)\n",
        "coefficients_Log_Reg_L1 = coefficients_Log_Reg_L1.sort_values(by='Coefficients', ascending=False)\n",
        "coefficients_Log_Reg_L1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsczxqQw5snW"
      },
      "source": [
        "Let's check how many coefficients are different from zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeJ4_xPI5snX"
      },
      "outputs": [],
      "source": [
        "sum(coefficients_Log_Reg_L1.Coefficients!=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhk1GgJn5snX"
      },
      "source": [
        "Let's check how many coefficients are exactly zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBc0WJKq5snX"
      },
      "outputs": [],
      "source": [
        "sum(coefficients_Log_Reg_L1.Coefficients==0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex57djgC5snX"
      },
      "source": [
        "Which ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo_xL5LU5snX"
      },
      "outputs": [],
      "source": [
        "coefficients_Log_Reg_L1_eq0=coefficients_Log_Reg_L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4W6Xmel5snX"
      },
      "outputs": [],
      "source": [
        "coefficients_Log_Reg_L1_eq0.loc[coefficients_Log_Reg_L1_eq0['Coefficients']==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAQus0685snX"
      },
      "source": [
        "# L2 Norm: Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aMzttkCd5snX"
      },
      "outputs": [],
      "source": [
        "# Write Python code here:\n",
        "\n",
        "#Log_Reg_L2.coef_           #or\n",
        "\n",
        "coefficients_Log_Reg_L2 = pd.concat([pd.DataFrame(X.columns,columns=['Features']),\n",
        "                                     pd.DataFrame(np.transpose(Log_Reg_L2.coef_),columns=['Coefficients'])], axis = 1)\n",
        "\n",
        "# Sorting new DataFrame by Coefficients (Sort Descending)\n",
        "coefficients_Log_Reg_L2 = coefficients_Log_Reg_L2.sort_values(by='Coefficients', ascending=False)\n",
        "coefficients_Log_Reg_L2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCYHtXJR5snY"
      },
      "source": [
        "Let's check how many coefficients are different from zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zymO8A365snY"
      },
      "outputs": [],
      "source": [
        "sum(coefficients_Log_Reg_L2.Coefficients!=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnjOz3wC5snY"
      },
      "source": [
        "Let's check how many coefficients are exactly zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5BX0J0X5snY"
      },
      "outputs": [],
      "source": [
        "sum(coefficients_Log_Reg_L2.Coefficients==0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F84F_JY75snY"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more.\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUahBwKG5snY"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 2**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSCQjFhs5snY"
      },
      "source": [
        "**This highlights the important difference between Ridge and Lasso. In Ridge, the coefficients are reduced, but not completely to zero. In Lasso, we have many of the coefficients becoming zero. This is a form of feature selection.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu61k3Au5snY"
      },
      "source": [
        "## 5.2 Making predictions and using 'accuracy' to evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acdPUHno5snY"
      },
      "source": [
        "We now have two model that can predict readmission based on the feature variables. Let's predict the first record in the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGMxCyeJ5snZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 3**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JwfDcJY5snZ"
      },
      "source": [
        "### <font color='blue'> Question 2: Write the Python code to make predictions using the test set with the model that we fitted previously. Make sure you save the predictions as 'y_pred', as this variable will be used later on.</font>\n",
        "\n",
        "<p><font color='green'> Tip:  You can use the following structure: \"y_pred_Model1 = ModelName.predict(X_test_scaled)\"</font></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVJ7Qrws5snZ"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"LASSO\"\n",
        "\n",
        "y_pred_L1= Log_Reg_L1.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvcRA9uY5snZ"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"RIDGE\"\n",
        "\n",
        "y_pred_L2= Log_Reg_L2.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMYNZSh_5snZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 3**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jlJuZOT5snZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 4**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzTtZM1q5snZ"
      },
      "source": [
        "### <font color='blue'> Question 3: Compute the accuracy of both models 'Log_Reg_L1' and 'Log_Reg_L2', and give your opinion about it</font>\n",
        "\n",
        "<p><font color='green'> Tip:  You can use the following structure: \"round(ModelName.score(X_test_scaled,y_test),3)\"</font></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAeKyNTt5snZ"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"LASSO\"\n",
        "# Use score method to get accuracy of model\n",
        "\n",
        "score = round(Log_Reg_L1.score(X_test_scaled, y_test),3)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmjEQPG_5snZ"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"RIDGE\"\n",
        "# Use score method to get accuracy of model\n",
        "\n",
        "score = round(Log_Reg_L2.score(X_test_scaled, y_test),3)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SERbMmCo5sna"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPoBB1Xy5sna"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 4**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM1iIYQ15sna"
      },
      "source": [
        "Does the accuracy look like a good resut? But be careful..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20D--_TM5sna"
      },
      "outputs": [],
      "source": [
        "print(hospital.readmission.value_counts('no'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5uWmD_e5sna"
      },
      "source": [
        "We know that readmission = NO accounts for approximately 83% of the records in the data set. This means that if we set a model to always predict NO, we will automatically obtain 83% model accuracy! This highlights the flaw in using accuracy as the only performance metric, particularly for response variables that are imbalanced (many more of one level than the other). Let's look at the confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCD9rFEc5sna"
      },
      "source": [
        "### 5.2.1 Confusion Matrix (Chapter 5, Book 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKk0zVyU5sna"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXNwcQbl5sna"
      },
      "source": [
        "![alt text](https://github.com/CBDRH-HDAT9500/Solutions-week01-05/blob/main/images/metrics.PNG?raw=1)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVKWSBhR5sna"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix for model using 'LASSO'\n",
        "\n",
        "# Confusion Matrix\n",
        "confusion_L1 = metrics.confusion_matrix(y_test, y_pred_L1)\n",
        "\n",
        "# Visualising the confusion matrix of our KNN model\n",
        "labels = {'No', 'Yes'}\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_L1, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkbjFC-w5snb"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix for model using 'RIDGE'\n",
        "\n",
        "confusion_L2 = metrics.confusion_matrix(y_test, y_pred_L2)\n",
        "\n",
        "# Visualising the confusion matrix of our KNN model\n",
        "labels = {'No', 'Yes'}\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_L2, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZVKr_Oz5snb"
      },
      "source": [
        "The confusion matrix shows that only very few records were classified as YES, and almost all of the records were classified as NO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3OOoqbD5snb"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 5**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG0HuukP5snb"
      },
      "source": [
        "### <font color='blue'> Question 4: Explain if you see any differences between both confusion matrix in your own words</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY8O560b5snb"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUTrMfdL5snb"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 5**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO53PzAo5snc"
      },
      "source": [
        "## 5.3 Evaluating the model using F1 Score - L1-norm (LASSO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4sWMoy5snc"
      },
      "source": [
        "In order to assess our model with a more suitable measure, we use the F1 score.\n",
        "\n",
        "This is a very nice reading about evaluation metrics or performance measures:\n",
        "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n",
        "\n",
        "F1 scores need 'YES' to be 1 and 'NO' to be 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5k3PSg5snc"
      },
      "source": [
        "We build a new list in which 'YES' will be 1 and 'NO' will be 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seqCt-Ws5snc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "round(f1_score(y_test, y_pred_L1, pos_label='yes', average='binary'),3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxeGXXAP5snc"
      },
      "source": [
        "The F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. As we can see, our predictive algorithm is not bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjzBx1bT5snc"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 6**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx6fxmfB5snd"
      },
      "source": [
        "### <font color='blue'> Question 5: Repeat the process of evaluating the model using F1 Score, but for our logistic regression model using L2-norm regularization (RIDGE) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdxQ-meX5snd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "round(f1_score(y_test, y_pred_L2, pos_label='yes', average='binary'),3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WfPnnn25snd"
      },
      "source": [
        "### <font color='blue'> Question 6: Use the 'classification_report' to list precision, recall, f1 score and support for each class (page 284 and 285 of Book 1). Repeat the process for both models.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWF_lAK05snd"
      },
      "outputs": [],
      "source": [
        "# Write Python code here. TIP: Use y_pred_binary_L1 for LASSO\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred_L1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GwXwMoG5snd"
      },
      "outputs": [],
      "source": [
        "# Write Python code here. TIP: Use y_pred_binary_L2 for RIDGE\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred_L2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Who8oXLm5sne"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 6**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wAzJOak5sne"
      },
      "source": [
        "## 5.4 Receiver Operating Characteristic (ROC): TPR and FPR\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "\n",
        "If you haven't seen this video that explains ROC and AUC, I recommend it to you since this activity will be much clearer: https://www.youtube.com/watch?v=xugjARegisk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iANq4QbL5sne"
      },
      "source": [
        "### 5.4.1 Probability associated with each prediction\n",
        "We need to determine the probability of each record in the test set being a 'YES', or equivalently a 1 as we have converted the response into a binary variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoGpD2Oy5sne"
      },
      "outputs": [],
      "source": [
        "# Probabilities of the test set being 0 and 1: LASSO\n",
        "y_pred_proba_L1 = Log_Reg_L1.predict_proba(X_test_scaled)[:,1]\n",
        "y_pred_L1\n",
        "\n",
        "print(y_pred_proba_L1[:5])\n",
        "print(y_pred_L1[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p704ZYS35sne"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "Log_Reg_L1.predict_proba(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZkNBXWB5sne"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "Log_Reg_L1.predict_proba(X_test_scaled)[11:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-2xPKli5sne"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "solution=np.argmax(Log_Reg_L1.predict_proba(X_test_scaled), axis=1)\n",
        "solution[11:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjgPUmeX5snf"
      },
      "outputs": [],
      "source": [
        "# Probabilities of the test set being 0 and 1: RIDGE\n",
        "y_pred_proba_L2 = Log_Reg_L2.predict_proba(X_test_scaled)[:,1]\n",
        "y_pred_L2\n",
        "\n",
        "print(y_pred_proba_L2[:5])\n",
        "print(y_pred_L2[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F99tcva65snf"
      },
      "source": [
        "### 5.4.2 Determining the fpr and tpr at each threshold value for logistic regression using Lasso (L1 Regularization)\n",
        "Now that we have the probabilities associated with each prediction, we know exactly which records are predicted YES and NO for each choice of decision threshold. Hence, we can determine the false positive rate (fpr) and true positive rate (tpr) for threshold value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frZojOKy5snf"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "y_test_binary = [0 if x=='no' else 1 for x in y_test]\n",
        "fpr, tpr,thresholds= metrics.roc_curve(y_true = y_test_binary, y_score = y_pred_proba_L1)\n",
        "\n",
        "# Defining dataframe with fpr and tpr at each threshold value for logistic regression using Lasso\n",
        "df = pd.DataFrame()\n",
        "df['fpr'] = fpr\n",
        "df['tpr'] = tpr\n",
        "\n",
        "# Check\n",
        "print(thresholds[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTkcWWRo5snf"
      },
      "source": [
        "### 5.4.3 Plotting the ROC curve (Chapter 5, Book 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCcdi3yx5snf"
      },
      "outputs": [],
      "source": [
        "from plotnine import *\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "p = ggplot(mapping = aes(x = fpr, y = tpr), data = df)\n",
        "p += geom_line(color = 'red')\n",
        "p += geom_abline(aes(intercept=0, slope=1), linetype = 'dashed', colour = 'blue')\n",
        "p += labs(title = 'ROC Curve', x = 'fpr', y = 'tpr')\n",
        "p += theme_bw()\n",
        "\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkJWNJIe5snf"
      },
      "source": [
        "### 5.4.4 Area Under the ROC curve (AUC)\n",
        "Note that AUC = 0.5 corresponds to random assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLWVS0zf5snf"
      },
      "outputs": [],
      "source": [
        "print(round(metrics.roc_auc_score(y_true = y_test_binary, y_score = y_pred_proba_L1),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9fe1tMp5sng"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 7**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z9y-kmZ5sng"
      },
      "source": [
        "### <font color='blue'> Question 7: Repeat this process for the logistic regression model using Ridge (L2-norm regularization). Explain AUC metric in your own words (Chapter 5, Book 1) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lBSdkeg5sng"
      },
      "outputs": [],
      "source": [
        "# Write Python code here. TIP: In this case we need to use 'y_pred_proba_L2'.\n",
        "fpr, tpr,thresholds= metrics.roc_curve(y_true = y_test_binary, y_score = y_pred_proba_L2)\n",
        "\n",
        "# Defining dataframe with fpr and tpr at each threshold value for logistic regression using Lasso\n",
        "df = pd.DataFrame()\n",
        "df['fpr'] = fpr\n",
        "df['tpr'] = tpr\n",
        "\n",
        "# Check\n",
        "print(thresholds[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KQ7JggX5sng"
      },
      "outputs": [],
      "source": [
        "# Write Python code here:\n",
        "\n",
        "from plotnine import *\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "p = ggplot(mapping = aes(x = fpr, y = tpr), data = df)\n",
        "p += geom_line(color = 'red')\n",
        "p += geom_abline(aes(intercept=0, slope=1), linetype = 'dashed', colour = 'blue')\n",
        "p += labs(title = 'ROC Curve', x = 'fpr', y = 'tpr')\n",
        "p += theme_bw()\n",
        "\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv0caapv5sng"
      },
      "outputs": [],
      "source": [
        "print(round(metrics.roc_auc_score(y_true = y_test_binary, y_score = y_pred_proba_L2),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMd9P40L5sng"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKWVpPOF5sng"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 7**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJB8vZw65sng"
      },
      "source": [
        "## 5.5 Explanation of how our models make predictions: Lime and Shap\n",
        "\n",
        "### **Letâs first use Lime to interpret the predictions from the logistic regression model with L1-norm (Lasso)**\n",
        "\n",
        "We will analyse how our logistic regression model with Lasso predicts 'readmission' with one patient. For this, we will assume that the first patient from the 'X_test' dataset is 'new' and that the model has not seen it in the training sample and has not been used in the test sample. Let's analyse this particular prediction using Lime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prSFVdvX5sng"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "import shap\n",
        "import time\n",
        "\n",
        "# Adding the explainer for Lime algorithm. More information: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled,\n",
        "                                                   feature_names=X_train.columns.values.tolist(),\n",
        "                                                   class_names=np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aav_Vt3u5snh"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for our logistic regression model using Lasso\n",
        "exp = explainer.explain_instance(X_test_scaled[0], Log_Reg_L1.predict_proba, num_features=10)\n",
        "exp.show_in_notebook(show_all=False, show_table=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQs-lU_u5snh"
      },
      "source": [
        "### **Letâs use Lime to interpret the predictions from the logistic regression model with L2-norm (Ridge).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4GQYA2J5snh"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for our logistic regression model using Ridge\n",
        "exp = explainer.explain_instance(X_test_scaled[0], Log_Reg_L2.predict_proba, num_features=10)\n",
        "exp.show_in_notebook(show_all=False, show_table=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0V5swpl5snh"
      },
      "source": [
        "**Let's use SHAP explainer to compare results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtdSUgad5snh"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 8**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p2fyX2j5snh"
      },
      "source": [
        "### <font color='blue'> Question 8: Could you use SHAP summary plot to visualise both models in a global sense in order to summarise the effect of all the features in the 'X_train_scaled' dataset?. </font>\n",
        "\n",
        "<p><font color='green'> Tip:  Please consider that we need to consider X_train_scaled for both models.</font></p>\n",
        "\n",
        "<p><font color='green'> Explainer for Logistic Regression: shap.LinearExplainer(ModelName, X_train_scaled, feature_dependence='independent').\n",
        "\n",
        "\n",
        "<p><font color='green'> NB: It could take 20-30 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFF0HUWS5snh"
      },
      "outputs": [],
      "source": [
        "# Write your Python code here (LASSO):\n",
        "\n",
        "# Using SHAP to explain predictions\n",
        "explainer_Model1 = shap.LinearExplainer(Log_Reg_L1, X_train_scaled, feature_dependence=\"independent\")\n",
        "shap_values_Model1_train = explainer_Model1.shap_values(X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7flTG0Z5snh"
      },
      "outputs": [],
      "source": [
        "# Summary plot - SHAP\n",
        "shap.summary_plot(shap_values_Model1_train, X_train_scaled, feature_names= X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9PJtW755sni"
      },
      "outputs": [],
      "source": [
        "# Write your Python code here (RIDGE):\n",
        "\n",
        "# Using SHAP to explain predictions\n",
        "explainer_Model2 = shap.LinearExplainer(Log_Reg_L2, X_train_scaled, feature_dependence=\"independent\")\n",
        "shap_values_Model2_train = explainer_Model2.shap_values(X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GGFR2BP5sni"
      },
      "outputs": [],
      "source": [
        "# Summary plot - SHAP\n",
        "shap.summary_plot(shap_values_Model2_train, X_train_scaled, feature_names= X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhD7unXl5sni"
      },
      "source": [
        "### <font color='blue'> Question 9: Please briefly explain the results of the SHAP summary plot of Model 2 (Logistic Regression using Ridge). </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMOsRNoN5sni"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbqss94f5sni"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 8**</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}