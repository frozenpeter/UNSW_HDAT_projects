{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBDRH-HDAT9500/week05/blob/master/Week-05-Exercise02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDL7xEHbLpKc"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1DXUVHxd4t15mfuqMgMCLnsP4jWVI5EWz)\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "© Copyright The University of New South Wales - CRICOS 00098G\n",
        "\n",
        "**Author**: Oscar Perez-Concha: o.perezconcha@unsw.edu.au\n",
        "\n",
        "**Contributors/Co-authors**: Marta Fredes-Torres and Zhisheng (Sandy) Sa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Week 7: Artificial Neural Networks / Deep Learning\n",
        "# Exercise 02: Add regularization to a neural network by using Dropout\n",
        "\n",
        "\n",
        "# 1. Introduction\n",
        "\n",
        "In this exercise, we will regularize the deep neural network we built in the previous exercise.\n",
        "\n",
        "\"Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables in the data sample or activations from a previous layer.\"\n",
        "\n",
        "Read: [How to reduce overfitting with dropout regularizationn in keras](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)\n",
        "\n",
        "In particular, pay attention to the section: MLP Dropout Regularization.\n",
        "\n",
        "\n",
        "## 1.1. Aims of the Exercise:\n",
        "\n",
        "1. This is an introduction to Artificial Neural Networks / Deep Learning with regularisation.\n",
        "2. We will use Keras.\n",
        "\n",
        "\n",
        "It aligns with all of our course learning outcomes:\n",
        "\n",
        "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
        "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
        "\n",
        "\n",
        "## 1.2. Jupyter Notebook Instructions\n",
        "1. Read the content of each cell.\n",
        "2. Where necessary, follow the instructions that are written in each cell.\n",
        "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
        "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
        "\n",
        "## 1.3. Tips\n",
        "1. Run all the cells in sequence (one at a time), using the \"Run\" button.\n",
        "2. To edit this notebook, just double-click in each cell. Choose between \"Code\" cell or text \"Markdown\" cell in the combo-box above.\n",
        "3. If you want to save your notebook, please go File->Save a copy on Drive/GitHub.\n",
        "4. To clean the content of all cells and re-start Notebook, please go to Edit->Clear all outputs then Runtime->Restart runtime\n",
        "\n",
        "Follow the instructions given and if you have any questions, please use the **Comments section** in **Open Learning**."
      ],
      "metadata": {
        "id": "upiAS75QLqpb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIWDjs2TLpKn"
      },
      "source": [
        "# 2. Docstring:\n",
        "\n",
        "Create a docstring with the variables and constants that you will use in this exercise (data dictionary) and the purpose of your program. It is expected that you choose informative variable names and document your program (both docstrings and comments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmwN4IdMLpKo"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "\n",
        "#####################################################################################################################\n",
        "\n",
        "(double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEVFkM70LpKq"
      },
      "source": [
        "# 3. Load the Malaria Cell Images Data set\n",
        "\n",
        "For more information about this dataset, please see [Tensorflow Malaria](https://www.tensorflow.org/datasets/catalog/malaria) and [NLM - Malaria Data](https://lhncbc.nlm.nih.gov/LHC-research/LHC-projects/image-processing/malaria-datasheet.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "egAQioyHLpKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d93c58-7b4a-40d8-cdc0-c0c93263c5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "# FYI: This notebook was created with Python version 3.6.5\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8RkVmzL-LpKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a4607c-a828-4d61-949b-81dc2c0b1718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ADoY68FXLpKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d8a955-710f-482c-f957-fd9ee7ae590c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "# We do not need to run this cell if you are not running this notebook in Google Colab\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive # import drive from Gogle colab\n",
        "    root = '/content/drive'     # default location for the drive\n",
        "    # print(root)                 # print content of ROOT (Optional)\n",
        "    drive.mount(root)\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEWdW81vLpKw"
      },
      "source": [
        "If you are running this notebook in Google Colab, you must define your project paths. In this case, define your `project_path`. Otherwise, the model output will be lost after you close the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ev9AOXg4LpKx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n",
        "    # T\n",
        "    project_path = Path(root) / 'MyDrive' / 'HDAT9500' / 'week07'\n",
        "\n",
        "    # OPTIONAL - set working directory according to your google drive project path\n",
        "    # import os\n",
        "    # Change directory to the location defined in project_path\n",
        "    # os.chdir(project_path)\n",
        "else:\n",
        "    project_path = Path()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UAIo3lrLpKz"
      },
      "source": [
        "Let's retrieve the arrays \"data\" and \"labels\" that we stored in exercise 1 using binary files in NumPy .npy format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9q7zPmO0LpK0"
      },
      "outputs": [],
      "source": [
        "data_path = Path(project_path) / 'data' / 'malaria_img.npz'\n",
        "with np.load(data_path) as img:\n",
        "    data = img['data']\n",
        "    labels = img['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TI8Qurw9LpK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7de8059-f074-4793-bce2-12fd1684826d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cells : (27558, 12288) | labels : (27558,)\n"
          ]
        }
      ],
      "source": [
        "# Sanity Check\n",
        "print('Cells : {} | labels : {}'.format(data.shape , labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dNGjVLoLpK2"
      },
      "source": [
        "**Splitting the dataset into the Training set and Test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QbNreuGtLpK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c0fbc4-fe96-4211-f077-300724295e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAPE OF TRAINING IMAGE DATA : (22046, 12288)\n",
            "SHAPE OF TESTING IMAGE DATA : (5512, 12288)\n",
            "SHAPE OF TRAINING LABELS : (22046,)\n",
            "SHAPE OF TESTING LABELS : (5512,)\n"
          ]
        }
      ],
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 0, stratify = labels)\n",
        "\n",
        "# Sanity Check\n",
        "print(f'SHAPE OF TRAINING IMAGE DATA : {X_train.shape}')\n",
        "print(f'SHAPE OF TESTING IMAGE DATA : {X_test.shape}')\n",
        "print(f'SHAPE OF TRAINING LABELS : {y_train.shape}')\n",
        "print(f'SHAPE OF TESTING LABELS : {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chAuNh-6LpK3"
      },
      "source": [
        "# 4. Our first ANN using Keras + Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zKlxKNntLpK4"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Flatten\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr9PPS36LpK4"
      },
      "source": [
        "Please note that the dropout regularisation is a technique to stop over-fitting of the data. The dropout regularisation probabilistically drops a set of nodes during each training epoch. When a node is not available during training, the remaining nodes have to \"learn\" how to deal with the data being processed. This reduces the ability of the nodes to specialise, and hence prevents over-fitting. The final model does not drop any node; the corresponding weights reflect that learning process of having dropped nodes every now and then during the training stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d6fwaILpK5"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksliQQd9LpK5"
      },
      "source": [
        "### <font color='brown'> Question 1: We create a function `create_model` that will create the skeleton of the neural network. Add dropout regularisation to the first hidden layer.  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M-v1u0g7LpK6"
      },
      "outputs": [],
      "source": [
        "def create_model(DropoutL1):\n",
        "    # Initiate model\n",
        "    model = keras.Sequential()\n",
        "    # Add first hidden layer and input\n",
        "    model.add(Dense(16, activation='relu', input_dim = X_train.shape[1], kernel_initializer='uniform'))\n",
        "\n",
        "    # Apply dropout to outputs of hidden layer1\n",
        "    model.add(Dropout(DropoutL1))\n",
        "\n",
        "    # Add second hidden layer\n",
        "    model.add(Dense(16, activation = 'relu', kernel_initializer='uniform'))\n",
        "    # Add output layer\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform'))\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRHpbEcvLpK6"
      },
      "source": [
        "We need a package that acts as  a wrapper between keras and sklearn. [More information](https://medium.com/@am.benatmane/keras-hyperparameter-tuning-using-sklearn-pipelines-grid-search-with-cross-validation-ccfc74b0ce9f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ooGEvuq6LpK7"
      },
      "outputs": [],
      "source": [
        "# Additional packages\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUjGhEmPLpK7"
      },
      "source": [
        "We build our model and we call it \"ann\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bHfDt5ztLpK8"
      },
      "outputs": [],
      "source": [
        "ann = KerasClassifier(build_fn=create_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMxSrm7lLpK8"
      },
      "source": [
        "Pipeline and parameter grid for GridSearchCV:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOs3FtI4LpK8"
      },
      "source": [
        "1. Scale the data.\n",
        "\n",
        "2. GridsearchCV for the correct value of Dropout: The correct solution will be to use a pipeline that scales the data inside GridSearchCV. We know that when we use GridsearchCV, the only option for scaling the data is a pipeline. This is because we carry out cross-validation within the traninig set. That is, we divide the data into training and validation k times (as many as k-CV) in order to tune the hyperparameters. Therefore, we scale the data in different folds every time. A pipeline does it automatically for us.\n",
        "\n",
        "3. The GridSearchCV will take a long time. Feel free to choose less hyper-parameters, except for the dropout (0.2 and 0.4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rhytkto7LpK9"
      },
      "outputs": [],
      "source": [
        "# Pipeline\n",
        "from sklearn.pipeline  import *\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "Scaler = StandardScaler()\n",
        "# Scaler2 = MinMaxScaler() # Creates values between zero and 1 vs mean of 0 and SD of 1, does not retain scaled variance\n",
        "\n",
        "steps=[('Transform', Scaler), ('ANN', ann)]\n",
        "\n",
        "ann_pipeln = Pipeline(steps)\n",
        "\n",
        "# Create grid for testing density options. It will take a long time to run. Feel free to use less hyper-parameters.\n",
        "# param_grid = {'ANN__epochs':[100, 200],\n",
        "#               'ANN__batch_size': [200, 500],\n",
        "#               'ANN__DropoutL1':[0.2, 0.4]}\n",
        "\n",
        "param_grid = {'ANN__epochs':[100],\n",
        "              'ANN__batch_size': [200],\n",
        "              'ANN__DropoutL1':[0.2, 0.4]}\n",
        "\n",
        "# Establish GridSearchCV\n",
        "grid_search = GridSearchCV(estimator = ann_pipeln, param_grid = param_grid, cv=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "id": "hfhkBJ5TLpK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "297cad33-0fe5-4c1e-acce-4d03374cd5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                196624    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 196,913\n",
            "Trainable params: 196,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 3s 24ms/step - loss: 0.6318 - acc: 0.6517\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 3s 56ms/step - loss: 0.5894 - acc: 0.6907\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.5664 - acc: 0.7082\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.5444 - acc: 0.7258\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5282 - acc: 0.7378\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5156 - acc: 0.7411\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.5040 - acc: 0.7501\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.4886 - acc: 0.7599\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.4777 - acc: 0.7669\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.4563 - acc: 0.7831\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.4461 - acc: 0.7838\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4354 - acc: 0.7915\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4345 - acc: 0.7904\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4133 - acc: 0.8070\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4046 - acc: 0.8052\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3951 - acc: 0.8177\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3750 - acc: 0.8298\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3677 - acc: 0.8324\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3504 - acc: 0.8418\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.3421 - acc: 0.8442\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.3334 - acc: 0.8488\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 2s 44ms/step - loss: 0.3245 - acc: 0.8571\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 2s 31ms/step - loss: 0.3046 - acc: 0.8672\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3130 - acc: 0.8630\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2965 - acc: 0.8699\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2880 - acc: 0.8725\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2750 - acc: 0.8814\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2697 - acc: 0.8831\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2593 - acc: 0.8920\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2481 - acc: 0.8965\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.2539 - acc: 0.8910\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.2354 - acc: 0.9016\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2352 - acc: 0.9015\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2296 - acc: 0.9031\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2419 - acc: 0.8983\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2106 - acc: 0.9147\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2075 - acc: 0.9159\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2228 - acc: 0.9052\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2028 - acc: 0.9187\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2126 - acc: 0.9169\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2049 - acc: 0.9155\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2023 - acc: 0.9174\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.1975 - acc: 0.9196\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.1896 - acc: 0.9223\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1687 - acc: 0.9318\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1936 - acc: 0.9245\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1756 - acc: 0.9312\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1759 - acc: 0.9321\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1666 - acc: 0.9314\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1631 - acc: 0.9343\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1614 - acc: 0.9390\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1718 - acc: 0.9324\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1571 - acc: 0.9418\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1987 - acc: 0.9206\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1705 - acc: 0.9339\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.1488 - acc: 0.9449\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 2s 34ms/step - loss: 0.1373 - acc: 0.9462\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1496 - acc: 0.9418\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 2s 31ms/step - loss: 0.1408 - acc: 0.9450\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 3s 50ms/step - loss: 0.1468 - acc: 0.9422\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1392 - acc: 0.9483\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1364 - acc: 0.9446\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1250 - acc: 0.9527\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.1262 - acc: 0.9525\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1421 - acc: 0.9454\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.1311 - acc: 0.9495\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1232 - acc: 0.9542\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1394 - acc: 0.9456\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1251 - acc: 0.9519\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1293 - acc: 0.9502\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1268 - acc: 0.9497\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1357 - acc: 0.9488\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1284 - acc: 0.9507\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1278 - acc: 0.9535\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1331 - acc: 0.9487\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1210 - acc: 0.9546\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1182 - acc: 0.9553\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.1138 - acc: 0.9546\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1381 - acc: 0.9491\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1238 - acc: 0.9527\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1076 - acc: 0.9594\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1054 - acc: 0.9572\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1101 - acc: 0.9585\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1203 - acc: 0.9544\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1086 - acc: 0.9594\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1020 - acc: 0.9615\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1053 - acc: 0.9581\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.0974 - acc: 0.9619\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1020 - acc: 0.9621\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.1198 - acc: 0.9538\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1080 - acc: 0.9590\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1086 - acc: 0.9583\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.0986 - acc: 0.9638\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1043 - acc: 0.9598\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1009 - acc: 0.9614\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0938 - acc: 0.9643\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.0958 - acc: 0.9645\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1082 - acc: 0.9582\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1004 - acc: 0.9619\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.0938 - acc: 0.9658\n",
            "56/56 [==============================] - 1s 9ms/step - loss: 1.2132 - acc: 0.7483\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 16)                196624    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 196,913\n",
            "Trainable params: 196,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 3s 20ms/step - loss: 0.6309 - acc: 0.6488\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.5916 - acc: 0.6882\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5734 - acc: 0.6945\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.5611 - acc: 0.7142\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.5456 - acc: 0.7172\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.5305 - acc: 0.7302\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5217 - acc: 0.7381\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.5025 - acc: 0.7502\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.4859 - acc: 0.7615\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.4665 - acc: 0.7724\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.4530 - acc: 0.7801\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4492 - acc: 0.7829\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4288 - acc: 0.7942\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4066 - acc: 0.8109\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3937 - acc: 0.8152\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3777 - acc: 0.8257\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3681 - acc: 0.8324\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3545 - acc: 0.8392\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3377 - acc: 0.8496\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3306 - acc: 0.8504\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3174 - acc: 0.8580\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.3054 - acc: 0.8644\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2961 - acc: 0.8702\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2856 - acc: 0.8744\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2963 - acc: 0.8669\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2661 - acc: 0.8840\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2507 - acc: 0.8911\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2532 - acc: 0.8902\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2445 - acc: 0.8961\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2331 - acc: 0.9012\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2516 - acc: 0.8937\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2204 - acc: 0.9078\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2193 - acc: 0.9079\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2053 - acc: 0.9169\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2034 - acc: 0.9154\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2116 - acc: 0.9124\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1947 - acc: 0.9221\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1915 - acc: 0.9240\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1928 - acc: 0.9233\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1889 - acc: 0.9233\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2059 - acc: 0.9160\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1627 - acc: 0.9379\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1699 - acc: 0.9352\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1693 - acc: 0.9301\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1708 - acc: 0.9326\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1693 - acc: 0.9323\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.1478 - acc: 0.9437\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1535 - acc: 0.9414\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.1419 - acc: 0.9466\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1559 - acc: 0.9411\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1445 - acc: 0.9454\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1328 - acc: 0.9496\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1385 - acc: 0.9467\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1461 - acc: 0.9431\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1302 - acc: 0.9509\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1365 - acc: 0.9484\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1322 - acc: 0.9488\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1292 - acc: 0.9514\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.1204 - acc: 0.9563\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.1298 - acc: 0.9484\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.1615 - acc: 0.9366\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1250 - acc: 0.9554\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1144 - acc: 0.9586\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1135 - acc: 0.9565\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1216 - acc: 0.9560\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1054 - acc: 0.9623\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1039 - acc: 0.9629\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1143 - acc: 0.9582\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1075 - acc: 0.9605\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1013 - acc: 0.9655\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1284 - acc: 0.9520\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.1065 - acc: 0.9631\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.0989 - acc: 0.9682\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1286 - acc: 0.9542\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1156 - acc: 0.9584\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0954 - acc: 0.9658\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0942 - acc: 0.9663\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1074 - acc: 0.9631\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0905 - acc: 0.9696\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1042 - acc: 0.9611\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1020 - acc: 0.9647\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.0853 - acc: 0.9708\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1282 - acc: 0.9545\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1089 - acc: 0.9614\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.0888 - acc: 0.9678\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.0931 - acc: 0.9646\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.0993 - acc: 0.9665\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1001 - acc: 0.9645\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.0870 - acc: 0.9682\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0915 - acc: 0.9698\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0918 - acc: 0.9682\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0817 - acc: 0.9721\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0899 - acc: 0.9685\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0735 - acc: 0.9753\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0738 - acc: 0.9747\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0909 - acc: 0.9682\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.0786 - acc: 0.9745\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.0838 - acc: 0.9703\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.0855 - acc: 0.9701\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.0774 - acc: 0.9720\n",
            "56/56 [==============================] - 1s 10ms/step - loss: 1.2172 - acc: 0.7556\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 16)                196624    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 196,913\n",
            "Trainable params: 196,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 2s 22ms/step - loss: 0.6339 - acc: 0.6451\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5956 - acc: 0.6877\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5771 - acc: 0.6998\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5649 - acc: 0.7110\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5607 - acc: 0.7137\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5487 - acc: 0.7209\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5410 - acc: 0.7250\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.5327 - acc: 0.7300\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 2s 32ms/step - loss: 0.5202 - acc: 0.7433\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.5158 - acc: 0.7414\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.5105 - acc: 0.7446\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5038 - acc: 0.7492\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4909 - acc: 0.7597\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4824 - acc: 0.7639\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4712 - acc: 0.7727\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4685 - acc: 0.7722\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4713 - acc: 0.7666\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4551 - acc: 0.7807\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4574 - acc: 0.7782\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.4405 - acc: 0.7897\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.4306 - acc: 0.7963\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.4335 - acc: 0.7963\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.4263 - acc: 0.7960\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4121 - acc: 0.8104\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4103 - acc: 0.8056\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4017 - acc: 0.8145\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4013 - acc: 0.8116\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3892 - acc: 0.8202\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3860 - acc: 0.8219\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3863 - acc: 0.8189\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3734 - acc: 0.8255\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3693 - acc: 0.8317\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.3589 - acc: 0.8339\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.3619 - acc: 0.8345\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.3656 - acc: 0.8321\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3494 - acc: 0.8424\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3440 - acc: 0.8447\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3393 - acc: 0.8437\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3356 - acc: 0.8499\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3396 - acc: 0.8440\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3304 - acc: 0.8486\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3210 - acc: 0.8541\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3253 - acc: 0.8548\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3187 - acc: 0.8543\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.3163 - acc: 0.8567\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.3064 - acc: 0.8612\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.3235 - acc: 0.8518\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3043 - acc: 0.8621\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3000 - acc: 0.8613\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2958 - acc: 0.8645\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2918 - acc: 0.8703\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2966 - acc: 0.8663\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2838 - acc: 0.8715\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2762 - acc: 0.8793\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2824 - acc: 0.8741\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2749 - acc: 0.8773\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2734 - acc: 0.8784\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2649 - acc: 0.8828\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2654 - acc: 0.8825\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.2780 - acc: 0.8751\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2722 - acc: 0.8797\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2615 - acc: 0.8807\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2596 - acc: 0.8848\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2485 - acc: 0.8871\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2641 - acc: 0.8833\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2532 - acc: 0.8882\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2526 - acc: 0.8893\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2495 - acc: 0.8902\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2402 - acc: 0.8927\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.2403 - acc: 0.8929\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2407 - acc: 0.8957\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.2459 - acc: 0.8882\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2371 - acc: 0.8947\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2381 - acc: 0.8945\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2273 - acc: 0.8995\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2366 - acc: 0.8953\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2316 - acc: 0.8981\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2293 - acc: 0.8986\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2380 - acc: 0.8948\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2268 - acc: 0.8997\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2298 - acc: 0.8969\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2200 - acc: 0.9023\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2202 - acc: 0.9047\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2192 - acc: 0.9010\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2216 - acc: 0.9027\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2146 - acc: 0.9055\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2297 - acc: 0.9008\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2047 - acc: 0.9106\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2058 - acc: 0.9095\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2066 - acc: 0.9098\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1956 - acc: 0.9140\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2036 - acc: 0.9108\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2080 - acc: 0.9091\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2054 - acc: 0.9129\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2079 - acc: 0.9094\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1987 - acc: 0.9149\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1971 - acc: 0.9142\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2048 - acc: 0.9111\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1924 - acc: 0.9169\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.1942 - acc: 0.9195\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.7585 - acc: 0.7307\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 16)                196624    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 196,913\n",
            "Trainable params: 196,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 2s 21ms/step - loss: 0.6368 - acc: 0.6500\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.5978 - acc: 0.6857\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5822 - acc: 0.6959\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5728 - acc: 0.7005\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5610 - acc: 0.7081\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5549 - acc: 0.7101\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5464 - acc: 0.7199\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5433 - acc: 0.7178\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.5355 - acc: 0.7286\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.5229 - acc: 0.7389\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.5201 - acc: 0.7385\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.5123 - acc: 0.7405\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5023 - acc: 0.7476\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.5018 - acc: 0.7483\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4913 - acc: 0.7599\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4833 - acc: 0.7580\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4765 - acc: 0.7632\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4620 - acc: 0.7751\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4551 - acc: 0.7773\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4436 - acc: 0.7846\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.4453 - acc: 0.7854\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.4401 - acc: 0.7874\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.4282 - acc: 0.7957\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.4186 - acc: 0.7970\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4144 - acc: 0.8003\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4070 - acc: 0.8056\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.4138 - acc: 0.8008\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3888 - acc: 0.8182\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3930 - acc: 0.8149\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3890 - acc: 0.8159\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3740 - acc: 0.8287\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3687 - acc: 0.8298\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3651 - acc: 0.8288\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.3571 - acc: 0.8340\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.3537 - acc: 0.8382\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.3518 - acc: 0.8375\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3414 - acc: 0.8417\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3398 - acc: 0.8439\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3378 - acc: 0.8426\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3314 - acc: 0.8482\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3258 - acc: 0.8511\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3232 - acc: 0.8539\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3190 - acc: 0.8541\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3091 - acc: 0.8593\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3152 - acc: 0.8546\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.3136 - acc: 0.8563\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2995 - acc: 0.8638\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2888 - acc: 0.8665\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.2880 - acc: 0.8694\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2801 - acc: 0.8739\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2887 - acc: 0.8693\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2887 - acc: 0.8691\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2752 - acc: 0.8774\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2706 - acc: 0.8776\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2793 - acc: 0.8756\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2818 - acc: 0.8766\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2799 - acc: 0.8747\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.2699 - acc: 0.8760\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2567 - acc: 0.8861\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.2490 - acc: 0.8869\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 2s 31ms/step - loss: 0.2438 - acc: 0.8893\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2503 - acc: 0.8885\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2550 - acc: 0.8842\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2527 - acc: 0.8868\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2502 - acc: 0.8863\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2536 - acc: 0.8827\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2528 - acc: 0.8896\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2478 - acc: 0.8890\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2276 - acc: 0.8977\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2345 - acc: 0.8942\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2161 - acc: 0.9065\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.2186 - acc: 0.9021\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2182 - acc: 0.9008\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2393 - acc: 0.8943\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2246 - acc: 0.9006\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2116 - acc: 0.9067\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2159 - acc: 0.9053\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2148 - acc: 0.9059\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2155 - acc: 0.9047\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2227 - acc: 0.9008\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2196 - acc: 0.9027\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.2207 - acc: 0.9026\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2210 - acc: 0.9016\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.2089 - acc: 0.9056\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2069 - acc: 0.9102\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2141 - acc: 0.9047\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2129 - acc: 0.9074\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1959 - acc: 0.9131\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2215 - acc: 0.9037\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1941 - acc: 0.9148\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2149 - acc: 0.9057\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1959 - acc: 0.9115\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1950 - acc: 0.9134\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.2020 - acc: 0.9095\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.1928 - acc: 0.9135\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.1928 - acc: 0.9145\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.1866 - acc: 0.9188\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2189 - acc: 0.9074\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1965 - acc: 0.9123\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2026 - acc: 0.9128\n",
            "56/56 [==============================] - 1s 10ms/step - loss: 0.6905 - acc: 0.7357\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 16)                196624    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 196,913\n",
            "Trainable params: 196,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "111/111 [==============================] - 5s 31ms/step - loss: 0.6037 - acc: 0.6765\n",
            "Epoch 2/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.5738 - acc: 0.7006\n",
            "Epoch 3/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.5530 - acc: 0.7135\n",
            "Epoch 4/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.5392 - acc: 0.7252\n",
            "Epoch 5/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.5258 - acc: 0.7316\n",
            "Epoch 6/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.5165 - acc: 0.7402\n",
            "Epoch 7/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.5005 - acc: 0.7519\n",
            "Epoch 8/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.4903 - acc: 0.7581\n",
            "Epoch 9/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.4797 - acc: 0.7650\n",
            "Epoch 10/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.4682 - acc: 0.7744\n",
            "Epoch 11/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.4548 - acc: 0.7802\n",
            "Epoch 12/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.4413 - acc: 0.7899\n",
            "Epoch 13/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.4308 - acc: 0.7945\n",
            "Epoch 14/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.4182 - acc: 0.8035\n",
            "Epoch 15/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.4081 - acc: 0.8120\n",
            "Epoch 16/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.3927 - acc: 0.8181\n",
            "Epoch 17/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.3883 - acc: 0.8217\n",
            "Epoch 18/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.3777 - acc: 0.8294\n",
            "Epoch 19/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.3693 - acc: 0.8364\n",
            "Epoch 20/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.3524 - acc: 0.8443\n",
            "Epoch 21/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.3480 - acc: 0.8481\n",
            "Epoch 22/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.3354 - acc: 0.8532\n",
            "Epoch 23/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.3338 - acc: 0.8515\n",
            "Epoch 24/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.3221 - acc: 0.8602\n",
            "Epoch 25/100\n",
            "111/111 [==============================] - 3s 31ms/step - loss: 0.3136 - acc: 0.8628\n",
            "Epoch 26/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.3050 - acc: 0.8691\n",
            "Epoch 27/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.3026 - acc: 0.8695\n",
            "Epoch 28/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2960 - acc: 0.8750\n",
            "Epoch 29/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2899 - acc: 0.8788\n",
            "Epoch 30/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.2815 - acc: 0.8818\n",
            "Epoch 31/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.2707 - acc: 0.8865\n",
            "Epoch 32/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2627 - acc: 0.8895\n",
            "Epoch 33/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2643 - acc: 0.8903\n",
            "Epoch 34/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2629 - acc: 0.8884\n",
            "Epoch 35/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.2489 - acc: 0.8976\n",
            "Epoch 36/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.2567 - acc: 0.8934\n",
            "Epoch 37/100\n",
            "111/111 [==============================] - 4s 37ms/step - loss: 0.2498 - acc: 0.8983\n",
            "Epoch 38/100\n",
            "111/111 [==============================] - 4s 35ms/step - loss: 0.2396 - acc: 0.9027\n",
            "Epoch 39/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2383 - acc: 0.9023\n",
            "Epoch 40/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2322 - acc: 0.9042\n",
            "Epoch 41/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.2241 - acc: 0.9072\n",
            "Epoch 42/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.2220 - acc: 0.9108\n",
            "Epoch 43/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2214 - acc: 0.9086\n",
            "Epoch 44/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2160 - acc: 0.9113\n",
            "Epoch 45/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2249 - acc: 0.9109\n",
            "Epoch 46/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2117 - acc: 0.9124\n",
            "Epoch 47/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.2086 - acc: 0.9201\n",
            "Epoch 48/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.2042 - acc: 0.9213\n",
            "Epoch 49/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1987 - acc: 0.9206\n",
            "Epoch 50/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1966 - acc: 0.9212\n",
            "Epoch 51/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.2015 - acc: 0.9203\n",
            "Epoch 52/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.2019 - acc: 0.9203\n",
            "Epoch 53/100\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 0.1916 - acc: 0.9238\n",
            "Epoch 54/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.1854 - acc: 0.9275\n",
            "Epoch 55/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1863 - acc: 0.9259\n",
            "Epoch 56/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1841 - acc: 0.9275\n",
            "Epoch 57/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1865 - acc: 0.9251\n",
            "Epoch 58/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1833 - acc: 0.9289\n",
            "Epoch 59/100\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 0.1763 - acc: 0.9304\n",
            "Epoch 60/100\n",
            "111/111 [==============================] - 3s 31ms/step - loss: 0.1780 - acc: 0.9312\n",
            "Epoch 61/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1688 - acc: 0.9349\n",
            "Epoch 62/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1866 - acc: 0.9279\n",
            "Epoch 63/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1706 - acc: 0.9330\n",
            "Epoch 64/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1612 - acc: 0.9373\n",
            "Epoch 65/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.1609 - acc: 0.9380\n",
            "Epoch 66/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.1671 - acc: 0.9365\n",
            "Epoch 67/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1675 - acc: 0.9364\n",
            "Epoch 68/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1568 - acc: 0.9394\n",
            "Epoch 69/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1707 - acc: 0.9316\n",
            "Epoch 70/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1599 - acc: 0.9379\n",
            "Epoch 71/100\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 0.1602 - acc: 0.9378\n",
            "Epoch 72/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.1531 - acc: 0.9392\n",
            "Epoch 73/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1518 - acc: 0.9409\n",
            "Epoch 74/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1597 - acc: 0.9384\n",
            "Epoch 75/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1509 - acc: 0.9422\n",
            "Epoch 76/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1566 - acc: 0.9399\n",
            "Epoch 77/100\n",
            "111/111 [==============================] - 3s 27ms/step - loss: 0.1490 - acc: 0.9435\n",
            "Epoch 78/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.1470 - acc: 0.9434\n",
            "Epoch 79/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1489 - acc: 0.9442\n",
            "Epoch 80/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1455 - acc: 0.9436\n",
            "Epoch 81/100\n",
            "111/111 [==============================] - 2s 22ms/step - loss: 0.1497 - acc: 0.9413\n",
            "Epoch 82/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1456 - acc: 0.9426\n",
            "Epoch 83/100\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 0.1429 - acc: 0.9468\n",
            "Epoch 84/100\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 0.1471 - acc: 0.9438\n",
            "Epoch 85/100\n",
            "111/111 [==============================] - 3s 22ms/step - loss: 0.1417 - acc: 0.9441\n",
            "Epoch 86/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1411 - acc: 0.9450\n",
            "Epoch 87/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1406 - acc: 0.9459\n",
            "Epoch 88/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1423 - acc: 0.9448\n",
            "Epoch 89/100\n",
            "111/111 [==============================] - 3s 27ms/step - loss: 0.1385 - acc: 0.9461\n",
            "Epoch 90/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.1304 - acc: 0.9492\n",
            "Epoch 91/100\n",
            "111/111 [==============================] - 3s 23ms/step - loss: 0.1326 - acc: 0.9480\n",
            "Epoch 92/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1274 - acc: 0.9502\n",
            "Epoch 93/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1263 - acc: 0.9518\n",
            "Epoch 94/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1338 - acc: 0.9485\n",
            "Epoch 95/100\n",
            "111/111 [==============================] - 3s 26ms/step - loss: 0.1307 - acc: 0.9493\n",
            "Epoch 96/100\n",
            "111/111 [==============================] - 3s 30ms/step - loss: 0.1334 - acc: 0.9507\n",
            "Epoch 97/100\n",
            "111/111 [==============================] - 3s 23ms/step - loss: 0.1327 - acc: 0.9480\n",
            "Epoch 98/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1254 - acc: 0.9534\n",
            "Epoch 99/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1292 - acc: 0.9516\n",
            "Epoch 100/100\n",
            "111/111 [==============================] - 2s 21ms/step - loss: 0.1243 - acc: 0.9546\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=2,\n",
              "             estimator=Pipeline(steps=[('Transform', StandardScaler()),\n",
              "                                       ('ANN',\n",
              "                                        <keras.wrappers.scikit_learn.KerasClassifier object at 0x78bf6e82fcd0>)]),\n",
              "             param_grid={'ANN__DropoutL1': [0.2, 0.4], 'ANN__batch_size': [200],\n",
              "                         'ANN__epochs': [100]})"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
              "             estimator=Pipeline(steps=[(&#x27;Transform&#x27;, StandardScaler()),\n",
              "                                       (&#x27;ANN&#x27;,\n",
              "                                        &lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x78bf6e82fcd0&gt;)]),\n",
              "             param_grid={&#x27;ANN__DropoutL1&#x27;: [0.2, 0.4], &#x27;ANN__batch_size&#x27;: [200],\n",
              "                         &#x27;ANN__epochs&#x27;: [100]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n",
              "             estimator=Pipeline(steps=[(&#x27;Transform&#x27;, StandardScaler()),\n",
              "                                       (&#x27;ANN&#x27;,\n",
              "                                        &lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x78bf6e82fcd0&gt;)]),\n",
              "             param_grid={&#x27;ANN__DropoutL1&#x27;: [0.2, 0.4], &#x27;ANN__batch_size&#x27;: [200],\n",
              "                         &#x27;ANN__epochs&#x27;: [100]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Transform&#x27;, StandardScaler()),\n",
              "                (&#x27;ANN&#x27;,\n",
              "                 &lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x78bf6e82fcd0&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x78bf6e82fcd0&gt;</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Apply Gridsearch, fit the model with the best params identified in the GridSearch\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teDZx4XSLpK9"
      },
      "source": [
        "### <font color='brown'> Question 2: Calculate accuracy, confusion matrix and all the metrics included in classification_report function. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ka97f54sLpK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b77b3b-6f82-4b38-8725-23c2f7c866b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ANN__DropoutL1': 0.2, 'ANN__batch_size': 200, 'ANN__epochs': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Write Python code here\n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions based on the grid search model\n",
        "y_pred = grid_search.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwlqPctECDI8",
        "outputId": "30ddb7c0-18c6-4b45-bb25-2356bbf22ad3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173/173 [==============================] - 1s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Determine metrics\n",
        "# Confusion Matrix\n",
        "labels = {'Uninfected', 'Infected'}\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualising the confusion matrix\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.xaxis.set_ticklabels(['Uninfected', 'Infected']); ax.yaxis.set_ticklabels(['Uninfected', 'Infected'])\n",
        "\n",
        "# Accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print()\n",
        "print(\"Accuracy score of grid_search: {:.2f}\".format(accuracy))\n",
        "print()\n",
        "print(\"Additional Measures for grid_search:\\n{}\".format(cr))\n",
        "print()\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "phmK6p0oCFXf",
        "outputId": "a88c18a0-db13-4caa-c918-0e1ad89e9cb4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy score of grid_search: 0.77\n",
            "\n",
            "Additional Measures for grid_search:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.76      2756\n",
            "           1       0.76      0.80      0.78      2756\n",
            "\n",
            "    accuracy                           0.77      5512\n",
            "   macro avg       0.77      0.77      0.77      5512\n",
            "weighted avg       0.77      0.77      0.77      5512\n",
            "\n",
            "\n",
            "Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhyR_zSmLpK-"
      },
      "source": [
        "### <font color='brown'> Question 3: Write your conclusions about the performance and potential use of this classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lznHIOtJLpK-"
      },
      "source": [
        "<b> Write answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_LIwCKvLpK_"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OIMflvILpK_"
      },
      "source": [
        "© 2022 Copyright The University of New South Wales - CRICOS 00098G"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}