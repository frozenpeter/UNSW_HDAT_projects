{"cells":[{"cell_type":"markdown","metadata":{"id":"s-QbAPHwEN5C"},"source":["![alt text](https://drive.google.com/uc?export=view&id=1DXUVHxd4t15mfuqMgMCLnsP4jWVI5EWz)\n","<br>\n","Â© Copyright The University of New South Wales - CRICOS 00098G"]},{"cell_type":"markdown","metadata":{"id":"sJa_c6KpEtEl"},"source":["# Laboratory 2, Part B:\n","# Diabetes Hospitalisations - Linear Prediction"]},{"cell_type":"markdown","metadata":{"id":"k2zB-0eOFkJz"},"source":["![alt text](https://drive.google.com/uc?export=view&id=105SGqeyo8RgLhSO8mN7ZE5OsG0YiLPKt)"]},{"cell_type":"markdown","metadata":{"id":"pI9jpPt8Frcl"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LJ3_amskFo7T"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yT89oWdZFm-o"},"source":["# 1. Introduction\n","Following the visualisation and manipulation steps (data mining and machine learning work-flow), we are now moving to steps 5-7. Check this tutorial for more information:\n","http://scikit-learn.org/stable/tutorial/basic/tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"sKjBhcubGSmK"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xo1eoECyGQ2L"},"source":["<b>Goal/Research question:</b> <font color=green> <b> our final goal is to build a predictive algorithm to predict readmission to hospital 30 days after discharge. </b></font>\n"]},{"cell_type":"markdown","metadata":{"id":"RLoGw98PGThJ"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G-45YKUPGcop"},"source":["## 1.1. Regression with L1-norm Regularization (Lasso) and L2-norm Regularization (Ridge)\n","\n","In this exercise, we are going to fit a logistic regression to our data set, by using two techniques that constrains (or regularizes) the coefficient estimates with a L1-norm (Lasso regularization) and a L2-norm regularization (Ridge regularization).\n","\n","1. The Lasso regularization will shrink the coefficient estimates towards zero or <b>directly to zero</b>. For this last reason, Lasso regularization <font color=green><b>could also be</b></font> a technique for <font color=green><b>feature selection </b></font>(those features that are not zero in the model).\n","\n","2. The Ridge regularization will shrink the coefficient estimates **towards zero**.\n"]},{"cell_type":"markdown","metadata":{"id":"sFCOThuAGdtq"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kYWLkpCLGmHj"},"source":["## 1.2. Aims:\n"," 1. To build a predictive model <b>using logistic regression with L1-norm and L2-norm regularization of the coefficient estimates</b> (Lasso and Ridge regularization).\n"," 2. **To  choose evaluation metrics.**\n"," 3. To manipulate features: standardization.\n"," 4. To use training and test sets.\n"," 5. To continue becoming familiar with the diabetes inpatient hospital dataset and the clinical terms contained in it.\n","\n","It aligns with all the learning outcomes of our course:\n","\n","\n","1. Distinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n","2. Design machine learning tasks for Health Data Science scenarios.\n","3. Apply machine learning workflow to health data problems.\n","4. Generate knowledge via the application of machine learning techniques to health data.\n"]},{"cell_type":"markdown","metadata":{"id":"GHaMpr5VGnPr"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2oQ2PFZvG_OC"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zTc9dHNf5snH"},"source":["# 2. Initial Docstring:\n","\n","All programs should have an initial docstring comment. It must include at least the following elements:\n","\n","* Purpose: what is the aim of your code?\n","* Date created\n","* Author\n","* Date modified\n","* Author of the modification\n","* Method: how did you go about solving the problem?\n","* Data dictionary: The data dictionary should contain all the important variables and constants defined, their datatype (float, string, int) and a short description of what they are.\n","* List and defintions of functions: similar to the data dictionary, but with functions.\n","* List of libraries: libraries used in the program and their functionality.\n","\n","Is there anything else you think we should include in the docstring? Please comment in the comments section of this week's laboratory.\n","\n","Please read these two documents:\n","1. pandas docstring guide: https://pandas.pydata.org/pandas-docs/version/0.23/contributing_docstring.html\n","2. Style guide: https://www.cse.unsw.edu.au/~en1811/resources/style.html\n"]},{"cell_type":"markdown","metadata":{"id":"u-PrX7lP5snI"},"source":["<b> Docstring:</b>\n","#####################################################################################################################\n","\n","(double-click here)\n","\n","\n","#####################################################################################################################"]},{"cell_type":"markdown","metadata":{"id":"lJj6jC7yGz5E"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"onFGDkg7G7CM"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PBfGLC-Q5snI"},"source":["# 3. Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbklIAHGxmNO"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","# We can also plot directly using pandas:\n","# https://pandas.pydata.org/docs/reference/plotting.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17298,"status":"ok","timestamp":1717469137484,"user":{"displayName":"HDAT9500 ML1","userId":"11526535201803481114"},"user_tz":-600},"id":"8h4Pgntn6H-z","outputId":"6fd822df-6842-4ad1-df7d-dcde613c5d3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Insert your comments and explanations\n","\n","# Mount Google Drive\n","# We do not need to run this cell if you are not running this notebook in Google Colab\n","\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive # import drive from Gogle colab\n","    root = '/content/drive'     # default location for the drive\n","    # print(root)                 # print content of ROOT (Optional)\n","    drive.mount(root)\n","else:\n","    print('Not running on CoLab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MYZxGFw6SvG"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","from pathlib import Path\n","\n","if 'google.colab' in str(get_ipython()):\n","    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n","    # You may need to change 'MyDrive' to 'My Drive'.\n","    project_path = Path(root) / 'MyDrive' / 'Colab Notebooks'\n","\n","    # OPTIONAL - set working directory according to your google drive project path\n","    # import os\n","    # Change directory to the location defined in project_path\n","    # os.chdir(project_path)\n","else:\n","    project_path = Path()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Z1h8e435snJ"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# import pickle\n","pickle_data_path = Path(project_path) /'hospital_final.pickle'\n","hospital = pd.read_pickle(pickle_data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1717469171421,"user":{"displayName":"HDAT9500 ML1","userId":"11526535201803481114"},"user_tz":-600},"id":"MeA7A-LsI0GJ","outputId":"8e08d4b1-a6c6-4a3c-ba6f-1217b30a8649"},"outputs":[{"data":{"text/plain":["0         True\n","1        False\n","2         True\n","3         True\n","4        False\n","         ...  \n","69265     True\n","69266     True\n","69267    False\n","69268    False\n","69269    False\n","Name: sex_Female, Length: 69267, dtype: bool"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["hospital.sex_Female"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhvC07-e5snK"},"outputs":[],"source":["hospital.columns"]},{"cell_type":"markdown","metadata":{"id":"Q8HfgxRrL6I-"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kMLjTzMwL689"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4mjQIFYZ5snK"},"source":["# 4. Feature scaling"]},{"cell_type":"markdown","metadata":{"id":"YYZ2HqPd5snK"},"source":["For some of the methods covered in this chapter, specifically ridge and lasso regression, it is advisable to scale the features.\n","\n","We can use several methods to that, for example:\n","\n","1.  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n","\n","2. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n","\n","In this example, we are going to use the `StandardScaler()`."]},{"cell_type":"markdown","metadata":{"id":"Ixa1jaAJ5snL"},"source":["**Interpretation of beta coefficients**:\n","\n","Keep in mind that the interpretation of regression coefficients becomes more complicated after the features are standardized. For instance, consider multiple linear regression. Prior to standardization, a unit change in $X_j$ corresponds to a change of $\\hat \\beta_j$ to the predicted response. Simple. However, after standardization of $X_j$, the interpretation of the coefficients becomes: for every increase of <b>one standard deviation</b> in $X_j$, the predicted response changes by $\\hat \\beta_j$ standard deviations. Still manageable, but not as straight forward.<p>"]},{"cell_type":"markdown","metadata":{"id":"BlB6Wj3f5snL"},"source":["First, we split the hospital data set into two DataFrames: the features, stored in X, and the target, stored in y."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtcpnhUZ5snM"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hbp5U-ks5snL"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","X = hospital.drop(['readmission'], axis = 1)\n","y = hospital[['readmission']].values"]},{"cell_type":"markdown","metadata":{"id":"1gjMJgOnL-ae"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kDlwP3ytQk4P"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q1PJlXmv5snL"},"source":["### Use of `Stratify = y`\n","\n","Now let's split the data into a training and test set. We will include the optional argument `stratify = y` to preserve the ratio between readmission=NO to readmission=YES."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPRz1Zan5snM"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.25,random_state=0)\n","# stratify = y means we wish to preserve the ratio of y in the test and training sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-f1IYM85snM"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Sanity Check:\n","\n","# Check that proportions are maintained:\n","y_test_df = pd.DataFrame(y_test)\n","y_test_df.columns = ['readmission']\n","y_train_df = pd.DataFrame(y_train)\n","y_train_df.columns = ['readmission']\n","\n","print('*********************************************************************************')\n","print(f\"\"\"Number of records in original data frame (hospital): {len(y)},\n","Proportion of readmission in original data frame (hospital):\\n {hospital.readmission.value_counts('NO')}\\n\n","********************************************************************************* \\n\n","Number of records in test set: {len(y_test)},\n","Proportion of readmission in test set:\\n {y_test_df.readmission.value_counts('NO')}\\n\n","********************************************************************************* \\n\n","Number of records in training set: {len(y_train)},\n","Proportion of readmission in train set:\\n {y_train_df.readmission.value_counts('NO')}\n","\"\"\")\n","print('*********************************************************************************')"]},{"cell_type":"markdown","metadata":{"id":"9ApnZ7tE5snN"},"source":["We can see that the training and test set have maintained the proportion of NO:YES response from the original data."]},{"cell_type":"markdown","metadata":{"id":"1yvu6hEFMABY"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c7ogWB17MChG"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ttaQ6QKO5snN"},"source":["### `StandardScaler()`\n","\n","Now let's define the scaler we will use, which we decide is `StandardScaler()`. We can also easily replace one preprocessing algorithm with another by simply changing the scaler definition at this step. For instance, instead of using `StandardScaler()`, we could have used `MinMaxScaler`, or some other alternative."]},{"cell_type":"markdown","metadata":{"id":"DIXBJvf55snN"},"source":["http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2m4XBRo5snO"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","scaler = StandardScaler()"]},{"cell_type":"markdown","metadata":{"id":"l7ZqeMAh5snO"},"source":["### Feature scaling: How to apply scaling to our data.\n","\n","Now we fit the scaler to the training data. It is important that we fit the scaler to **only the training set** and then apply that same scaler to the test set (remember that the test set is never seen until the very end, once our model has been fully trained). The purpose of having a test set is to mimic the situation where your model is making prediction decisions in the real world, when you do not have access to the true response. Therefore, we cannot use the test set for *anything* except comparing to predicted values. This means we cannot use the test set to fit the scaler."]},{"cell_type":"markdown","metadata":{"id":"POi4r1Zb5snO"},"source":["Standardize features means to subtract the mean and divide by the standard deviation.\n","In this step, we calculate the actual means and variances for each feature in  <font color='green'> THE TRAIN SET.</font>\n","https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEsM1VLR5snO"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Standardize features by removing the mean and scaling to unit variance.\n","# In this step, we calculate the actual means and variances for each feature in  THE TRAIN SET.\n","# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n","scaler.fit(X_train)\n","# mu-training-set = mean of the age in the training set\n","# sigma-training-set = standard deviation of the age in the training set"]},{"cell_type":"markdown","metadata":{"id":"iezL4dNS5snP"},"source":["To actually scale the data, use the transform method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkeMNLsw5snP"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# rescale the training data\n","X_train_scaled = scaler.transform(X_train)\n","# (X_train - mu-training-set )/sigma-training-set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r--8UWL5snP"},"outputs":[],"source":["# Check\n","X_train_scaled"]},{"cell_type":"markdown","metadata":{"id":"sGjWnvJm5snP"},"source":["To apply predictive models to the scaled data, we also need to transform the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zu0GuyRg5snP"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# scale the test data\n","X_test_scaled = scaler.transform(X_test)\n","# (X_test - mu-training-set )/sigma-training-set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f1J_ug45snQ"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Sanity check\n","X_test_scaled"]},{"cell_type":"markdown","metadata":{"id":"1Y3O47vFMKmZ"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GTZ6BEtKMMSZ"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DOYfKeEk5snQ"},"source":["# 5. Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"MkO4b_l35snT"},"source":["## 5.1 Training a logistic regression model"]},{"cell_type":"markdown","metadata":{"id":"5Fhk-1E55snT"},"source":["Now we can train our logistic regression model using ridge regression. The default of logistic regression in <b>scikit-learn</b>  is to include a ridge penalty term in the linear component. The parameter controlling the regularization strength, <b>$alpha$</b>, or $C$ in our Python library. This parameter is also known as $\\lambda$, and its relationship with $alpha$ is $(alpha =  C = \\frac {1}{\\lambda})$. <p>\n","    Small values of $C$ indicate strong regularization, whilst larger values of $C$ indicate weaker regularization. In fact, if we set $C$ to be an extremely large number, then $\\lambda$ is effectively zero and the regression reduces to <b> regular logistic regression</b> ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Egu3tOPf5snT"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","from sklearn.linear_model import LogisticRegression"]},{"cell_type":"markdown","metadata":{"id":"soNGfWbC5snT"},"source":["http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"]},{"cell_type":"markdown","metadata":{"id":"5A_ol7yX5snU"},"source":["Let's use the 'liblinear' solver that is valid for L1 and L2 regularizations."]},{"cell_type":"markdown","metadata":{"id":"A3PvWT_d5snV"},"source":["### Training a logistic regression model using L1-norm regularization (Lasso)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15DFuKLf5snV"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# C=0.01 as a first step. We will see how to adjust this hyper-paremters next week\n","Log_Reg_L1 = LogisticRegression(C = 0.01 , penalty = 'l1', solver='liblinear').fit(X_train_scaled, y_train.ravel())\n","# ravel() used to convert y from column vector to 1d array, as required by the method"]},{"cell_type":"markdown","metadata":{"id":"i6xDQ2w15snW"},"source":["### Training a logistic regression model using L2-norm regularization (Ridge)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTSwjt2v5snW"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# C=0.01 as a first step. We will see how to adjust this hyper-paremters next week\n","Log_Reg_L2 = LogisticRegression(C = 0.01 , penalty = 'l2',solver='liblinear').fit(X_train_scaled, y_train.ravel())\n","# ravel() used to convert y from column vector to 1d array, as required by the method"]},{"cell_type":"markdown","metadata":{"id":"12to30hMMfEa"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AA1yf-cfMcLT"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1fqx_eKj5snW"},"source":["### <font color='blue'> Question 1: Print the beta coefficients of both logistic regression models and comment on your first impression. Change the C hyperparameter several times and print the beta coefficients again to see how they change.</font>\n","\n","<p><font color='green'> Tip:  You can use the following structure: \"ModelName.coef_\" to print the beta coefficients only.</font></p>\n","<p><font color='green'> You can use the following structure: \"coefficients = pd.concat([pd.DataFrame(X.columns,columns=['Features']), pd.DataFrame(np.transpose(ModelName.coef_),columns=['Coefficients'])], axis = 1)\" to print the beta coefficients of each variable from the diabetes dataset.</font></p>"]},{"cell_type":"markdown","metadata":{"id":"97c5_OTT5snW"},"source":["# L1 Norm: Lasso"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veMquWoZ5snW","scrolled":true},"outputs":[],"source":["# Write Python code here:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eAQus0685snX"},"source":["# L2 Norm: Ridge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMzttkCd5snX","scrolled":true},"outputs":[],"source":["# Write Python code here:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F84F_JY75snY"},"source":["<b> Write the answer here:</b>\n","#####################################################################################################################\n","\n","We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more.\n","\n","#####################################################################################################################"]},{"cell_type":"markdown","metadata":{"id":"exVnmokHMmyC"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2i5KgIitMl2X"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zu61k3Au5snY"},"source":["## 5.2 Making predictions and using 'accuracy' to evaluate the model"]},{"cell_type":"markdown","metadata":{"id":"acdPUHno5snY"},"source":["We now have two model that can predict readmission based on the feature variables."]},{"cell_type":"markdown","metadata":{"id":"4JwfDcJY5snZ"},"source":["### <font color='blue'> Question 2: Write the Python code to make predictions using the test set with the model that we fitted previously. Make sure you save the predictions as `y_pred`, as this variable will be used later on. </font>\n","\n","<p><font color='green'> Tip:  You can use the following structure: \"y_pred_Model1 = ModelName.predict(X_test_scaled)\"</font></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVJ7Qrws5snZ"},"outputs":[],"source":["# Write Python code here for \"LASSO\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvcRA9uY5snZ"},"outputs":[],"source":["# Write Python code here for \"RIDGE\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0LXESzJYOqKj"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m-8z6_ApPbhb"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FzTtZM1q5snZ"},"source":["### <font color='blue'> Question 3: Compute the accuracy of both models 'Log_Reg_L1' and 'Log_Reg_L2', and give your opinion about it.\n","<p><font color='green'> Tip:  You can use the following structure: \"round(ModelName.score(X_test_scaled,y_test),3)\"</font></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAeKyNTt5snZ"},"outputs":[],"source":["# Write Python code here for \"LASSO\"\n","# Use score method to get accuracy of model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmjEQPG_5snZ"},"outputs":[],"source":["# Write Python code here for \"RIDGE\"\n","# Use score method to get accuracy of model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SERbMmCo5sna"},"source":["<b> Write the answer here:</b>\n","#####################################################################################################################\n","\n","(Double-click here)\n","\n","\n","#####################################################################################################################"]},{"cell_type":"markdown","metadata":{"id":"RIZJGKumOtHS"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TGLpg0X2Ow4c"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AM1iIYQ15sna"},"source":["### Important note regarding accuracy:\n","\n","Does the accuracy look like a good resut? But be careful..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20D--_TM5sna"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","print(hospital.readmission.value_counts('no'))"]},{"cell_type":"markdown","metadata":{"id":"-5uWmD_e5sna"},"source":["We know that readmission = NO accounts for approximately 83% of the records in the data set. This means that if we set a model to always predict NO, we will automatically obtain 83% model accuracy! This highlights the flaw in using accuracy as the only performance metric, particularly for response variables that are imbalanced (many more of one level than the other). Let's look at the confusion matrix."]},{"cell_type":"markdown","metadata":{"id":"NyW-6LVGOygK"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rmB_XXRBO0YF"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nCD9rFEc5sna"},"source":["### 5.2.1 Confusion Matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKk0zVyU5sna"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","from sklearn import metrics\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVKWSBhR5sna"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Confusion matrix for model using 'LASSO'\n","\n","# Confusion Matrix\n","confusion_L1 = metrics.confusion_matrix(y_test, y_pred_L1)\n","\n","# Visualising the confusion matrix of our KNN model\n","labels = {'No', 'Yes'}\n","ax= plt.subplot()\n","sns.heatmap(confusion_L1, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix');\n","ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkbjFC-w5snb"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Confusion matrix for model using 'RIDGE'\n","\n","confusion_L2 = metrics.confusion_matrix(y_test, y_pred_L2)\n","\n","# Visualising the confusion matrix of our KNN model\n","labels = {'No', 'Yes'}\n","ax= plt.subplot()\n","sns.heatmap(confusion_L2, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix');\n","ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n"]},{"cell_type":"markdown","metadata":{"id":"IZVKr_Oz5snb"},"source":["The confusion matrix shows that only very few records were classified as YES, and almost all of the records were classified as NO."]},{"cell_type":"markdown","metadata":{"id":"irH990iiP1A6"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Fv5kz6paP177"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eG0HuukP5snb"},"source":["### <font color='blue'> Question 4: Explain if you see any differences between both confusion matrix in your own words</font>"]},{"cell_type":"markdown","metadata":{"id":"iY8O560b5snb"},"source":["<b> Write the answer here:</b>\n","#####################################################################################################################\n","\n","(Double-click here)\n","\n","\n","#####################################################################################################################"]},{"cell_type":"markdown","metadata":{"id":"LtgmVckaP5Ki"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BuRaGI7vP6bC"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gO53PzAo5snc"},"source":["## 5.3 Evaluating the model using F1 Score - L1-norm (LASSO)"]},{"cell_type":"markdown","metadata":{"id":"LS4sWMoy5snc"},"source":["In order to assess our model with a more suitable measure, we use the F1 score.\n","\n","This is a very nice reading about evaluation metrics or performance measures:\n","https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n","\n","F1 scores need 'YES' to be 1 and 'NO' to be 0."]},{"cell_type":"markdown","metadata":{"id":"Eq5k3PSg5snc"},"source":["We build a new list in which 'YES' will be 1 and 'NO' will be 0:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"seqCt-Ws5snc"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","from sklearn.metrics import f1_score\n","round(f1_score(y_test, y_pred_L1, pos_label='yes', average='binary'),3)"]},{"cell_type":"markdown","metadata":{"id":"PxeGXXAP5snc"},"source":["The F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. As we can see, our predictive algorithm is not bad."]},{"cell_type":"markdown","metadata":{"id":"OmtApHPuQALU"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OASYaIR4QsMG"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Rx6fxmfB5snd"},"source":["### <font color='blue'> Question 5: Repeat the process of evaluating the model using F1 Score, but for our logistic regression model using L2-norm regularization (RIDGE) </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdxQ-meX5snd"},"outputs":[],"source":["# Insert your comments and explanations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EOyCrL2IQBYi"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-fgVjuzhQtxM"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4WfPnnn25snd"},"source":["### <font color='blue'> Question 6: Use the 'classification_report' to list precision, recall, f1 score and support for each class (page 284 and 285 of Book 1). Repeat the process for both models.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWF_lAK05snd"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Write Python code here. TIP: Use y_pred_binary_L1 for LASSO\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GwXwMoG5snd"},"outputs":[],"source":["# Insert your comments and explanations\n","\n","# Write Python code here. TIP: Use y_pred_binary_L2 for RIDGE\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4buMR2LbQDsb"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","Log_Reg_L1 = LogisticRegression(C = 0.01 , penalty = 'l1', solver='liblinear').fit(X_train_scaled, y_train.ravel())\n","Log_Reg_L2 = LogisticRegression(C = 0.01 , penalty = 'l2',solver='liblinear').fit(X_train_scaled, y_train.ravel())\n","\n","#Log_Reg_L1.coef_                  #or\n","\n","coefficients_Log_Reg_L1 = pd.concat([pd.DataFrame(X.columns,columns=['Features']),\n","                                     pd.DataFrame(np.transpose(Log_Reg_L1.coef_),columns=['Coefficients'])], axis = 1)\n","\n","# Sorting new DataFrame by Coefficients (Sort Descending)\n","coefficients_Log_Reg_L1 = coefficients_Log_Reg_L1.sort_values(by='Coefficients', ascending=False)\n","coefficients_Log_Reg_L1\n","\n","# check how many coefficients are different from zero\n","sum(coefficients_Log_Reg_L1.Coefficients!=0)\n","\n","# check how many coefficients are exactly zero\n","sum(coefficients_Log_Reg_L1.Coefficients==0)\n","\n","#Log_Reg_L2.coef_           #or\n","\n","coefficients_Log_Reg_L2 = pd.concat([pd.DataFrame(X.columns,columns=['Features']),\n","                                     pd.DataFrame(np.transpose(Log_Reg_L2.coef_),columns=['Coefficients'])], axis = 1)\n","\n","# Sorting new DataFrame by Coefficients (Sort Descending)\n","coefficients_Log_Reg_L2 = coefficients_Log_Reg_L2.sort_values(by='Coefficients', ascending=False)\n","coefficients_Log_Reg_L2\n","\n","# check how many coefficients are different from zero\n","sum(coefficients_Log_Reg_L2.Coefficients!=0)\n","\n","# check how many coefficients are exactly zero\n","sum(coefficients_Log_Reg_L2.Coefficients==0)\n","\n","# Making predictions and using 'accuracy' to evaluate the model\n","y_pred_train_L1  = Log_Reg_L1.predict(X_train_scaled)\n","y_pred_test_L1   = Log_Reg_L1.predict(X_test_scaled)\n","# Sanity Check\n","print(y_pred_train_L1)\n","print(y_pred_test_L1)\n","\n","y_pred_train_L2 = Log_Reg_L2.predict(X_train_scaled)\n","y_pred_test_L2  = Log_Reg_L2.predict(X_test_scaled)\n","# Sanity Check\n","print(y_pred_train_L2)\n","print(y_pred_test_L2)\n","\n","from sklearn.metrics import accuracy_score\n","# sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n","\n","accuracy_train_L1_alt_sol = accuracy_score(y_train, y_pred_train_L1)\n","print(\"The accuracy in the training set for L1 (Lasso) is =\", accuracy_train_L1_alt_sol)\n","accuracy_test_L1_alt_sol = accuracy_score(y_test, y_pred_test_L1)\n","print(\"The accuracy in the test set  for L1 (Lasso) is =\", accuracy_test_L1_alt_sol)\n","\n","\n","accuracy_train_L2_alt_sol = accuracy_score(y_train, y_pred_train_L2)\n","print(\"The accuracy in the training set for L2 (Ridge) is =\", accuracy_train_L2_alt_sol)\n","accuracy_test_L2_alt_sol = accuracy_score(y_test, y_pred_test_L2)\n","print(\"The accuracy in the test set  for L2 (Ridge) is =\", accuracy_test_L2_alt_sol)\n","\n","print(hospital.readmission.value_counts('no'))\n","# We know that readmission = NO accounts for approximately 83% of the records in the data set. \n","# This means that if we set a model to always predict NO, we will automatically obtain 83% model accuracy! \n","# This highlights the flaw in using accuracy as the only performance metric, particularly for response variables that are imbalanced (many more of one level than the other). \n","\n","# Let's look at the confusion matrix.\n","from sklearn import metrics\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Confusion matrix for model using 'LASSO' using the training set\n","confusion_L1_train = metrics.confusion_matrix(y_train, y_pred_train_L1)\n","\n","# Visualising the confusion matrix of our KNN model\n","labels = {'No', 'Yes'}\n","ax= plt.subplot()\n","sns.heatmap(confusion_L1_train, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix using the training set for L1');\n","ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n","\n","# Confusion matrix for model using 'LASSO' using the test set\n","confusion_L1_test = metrics.confusion_matrix(y_test, y_pred_test_L1)\n","\n","# Visualising the confusion matrix of our KNN model\n","labels = {'No', 'Yes'}\n","ax= plt.subplot()\n","sns.heatmap(confusion_L1_test, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix using the test set for L1');\n","ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n","\n","\n","# Confusion matrix for model using 'RIDGE' using the training set\n","confusion_L2_train = metrics.confusion_matrix(y_train, y_pred_train_L2)\n","\n","# Visualising the confusion matrix of our KNN model\n","labels = {'No', 'Yes'}\n","ax= plt.subplot()\n","sns.heatmap(confusion_L2_train, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix using the training set for L2');\n","ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n","\n","\n","# Confusion matrix for model using 'RIDGE' using the test set\n","confusion_L2_test = metrics.confusion_matrix(y_test, y_pred_test_L2)\n","\n","# Visualising the confusion matrix of our KNN model\n","labels = {'No', 'Yes'}\n","ax= plt.subplot()\n","sns.heatmap(confusion_L2_test, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n","ax.set_title('Confusion Matrix using the test set for L2');\n","ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n","\n","\n","# Evaluating the model using F1 Score\n","from sklearn.metrics import f1_score\n","# F1 for the training and test sets L1\n","f1_train_L1 = f1_score(y_train, y_pred_train_L1, pos_label='yes', average='binary')\n","print(\"F1 for the training set for L1\",f1_train_L1)\n","\n","f1_test_L1 = f1_score(y_test, y_pred_test_L1, pos_label='yes', average='binary')\n","print(\"F1 for the test  set for L1\",f1_test_L1)\n","\n","# F1 for the training and test sets L2\n","f1_train_L2 = f1_score(y_train, y_pred_train_L2, pos_label='yes', average='binary')\n","print(\"F1 for the training set for L2\",f1_train_L2)\n","\n","f1_test_L2 =f1_score(y_test,y_pred_test_L2 , pos_label='yes', average='binary')\n","print(\"F1 for the test  set for L2\",f1_test_L2)\n","\n","# Use the `classification_report` to list precision, recall, f1 score and support for each class (check the classification report method in the scikit-learn API).\n","from sklearn.metrics import classification_report\n","\n","print(\"Classification report for the training set for L1\")\n","print(classification_report(y_train, y_pred_train_L1))\n","\n","print(\"Classification Report for the test set for L1\")\n","print(classification_report(y_test, y_pred_test_L1))\n","\n","print(\"Classification report for the training set for L2\")\n","print(classification_report(y_train, y_pred_train_L2))\n","\n","print(\"Classification report for the test set for L2\")\n","print(classification_report(y_test, y_pred_test_L2))"]},{"cell_type":"markdown","metadata":{"id":"WfakjbKQQvC8"},"source":["\n","\n","---\n","\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
