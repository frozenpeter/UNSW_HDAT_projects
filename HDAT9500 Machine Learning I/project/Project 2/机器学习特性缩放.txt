要获得正确的预测和结果，就需要特征缩放。 如果某一列的值与其他列相比非常高，则具有更高值的列的影响将比其他低值列的影响高得多。 高强度的特征比低强度的特征重得多，即使它们在确定输出中更为关键。 因此，预测可能无法给出预期的结果，并且可能无法满足业务用例。

机器学习算法也可能对范围较小的列不敏感，并可能导致不一致

总而言之，功能缩放是必需的，因为：

回归系数直接受特征范围的影响
具有较高比例的功能比具有较低比例的功能更重要
如果我们具有缩放值，则可以轻松实现渐变下降
如果按比例缩放，某些算法将减少执行时间。
一些算法基于欧几里得距离，欧几里得距离对特征尺度非常敏感。
不同的特征缩放技术
我们可以使用不同的缩放技术来缩放输入数据集。 我们可以应用以下任一方法：

Normalization 或 Standardization

什么是归一化（Normalization ）？
归一化是在0到1之间缩放要素值归一化。这称为最小-最大缩放。

X' = (X - Xmin) / (Xmax - Xmin)
在上式中：
    Xmax和Xmin是功能列的最大值和最小值
    X的值始终在最小值和最大值之间
使用Scikit Learn进行数据归一化

以下是使用Scikit Learn进行归一化的简单实现。
import pandas
import numpy as np

dataset = pandas.read_csv("./data/FirstDataset.csv")

X = dataset.iloc[:, :].values

from sklearn.preprocessing import MinMaxScaler
norm = MinMaxScaler()
X[:, 1:3] = norm.fit_transform(X[:, 1:3])
print(X)

什么是机器学习的标准化？

标准化基于标准偏差。 它衡量功能中价值的传播。 这是最常用的之一。

Z-score = (X - mean(x)) / std(X)

当对特征值应用标准偏差时，特征集中值的99.7％介于-3 SD（标准偏差）至3 SD（标准偏差）之间。因此减小了数据列中值的范围
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/deephub/article/details/108374865